"Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation","Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper [1] by Kosa et al. In fact this submission is a totally new paper. It  re-used the following parts of the original paper [1]:  -  The introductory paragraph in Section 3 explaining the essence of OntoElect  as the methodology for ontology refinement  -  The introductory part of Section 4 to present the research questions forming  the agenda of the reported project – broadly – to explain the context and, fur- ther, the focus of the reported accomplishments in this paper  The rest has been written to report about the new results which have been obtained  after ICTERI 2017.      Cross-Evaluation of Term Extraction Tools   by Measuring Terminological Saturation  Victoria Kosa1, David Chaves-Fraga2, Dmitriy Naumenko3, Eugene Yuschenko3,  Carlos Badenes-Olmedo2, Vadim Ermolayev1, and Aliaksandr Birukou4  1 Department of Computer Science, Zaporizhzhia National University,   Zhukovskogo st. 66, 69600, Zaporizhzhia, Ukraine  victoriya1402.kosa@gmail.com, vadim@ermolayev.com  2 Ontology Engineering Group, Universidad Politécnica de Madrid, Madrid, Spain  {dchaves, cbadenes}@fi.upm.es   3 BWT Group, Mayakovskogo st. 11, 69035, Zaporizhzhia, Ukraine   admin@groupbwt.com   4 Springer-Verlag GmbH, Tiergartenstrasse 17, 69121, Heidelberg, Germany  Aliaksandr.Birukou@springer.com  Abstract. This paper reports on cross-evaluating the two freely available soft- ware tools for automated term extraction (ATE) from English texts: NaCTeM  TerMine and UPM Term Extractor. The objective was to find the most fitting  software for extracting the bags of terms to be the part of our instrumental pipe- line for exploring terminological saturation in text document collections in a  domain of interest. The choice of these particular tools from the bunch of the  other available is explained in our review of the related work in ATE. The ap- proach to measure terminological saturation is based on the use of the  THD algorithm developed in frame of our OntoElect methodology for ontology  refinement. The paper presents the suite of instrumental software modules, ex- perimental workflow, 2 synthetic and 3 real document collections, generated  datasets, and set-up of our experiments. Next, the results of the cross-evaluation  experiments are presented, analyzed, and discussed. Finally the paper offers  some conclusions and recommendations on the use of ATE software for meas- uring terminological saturation in retrospective text document collections.  Keywords. Automated Term Extraction, Software Tool, Experimental Cross- Evaluation, Terminological Saturation  1 Introduction  Automated term extraction (ATE, also known as recognition – ATR) from textual  documents is an established sub-field in text mining. Its results are further used for  different important purposes, for example as inputs in ontology learning. Many re- search activities are undertaken currently to improve the quality of extraction results.  These activities focus on different aspects, including: new or improved extraction  algorithms; combining linguistic and statistical approaches to extraction; developing  new or refined metrics which allow higher quality extraction; developing new extrac- tion tools which yield better results and scale to fit current dataset size requirements.  The mainstream criteria used to assess the quality of extracted results are adopted  from information retrieval and based on recall and precision metrics. However, to the  best of our knowledge, there were no reports on approaches to assess the complete- ness of the document collection from which extraction is performed. Recall measures  just inform about how completely the set of terms was extracted from the available  data but does not hint if the data itself was complete to contain all significant terms  characterizing the domain. In other words, there is no way so far to check if the col- lection of documents chosen for term extraction is representative. Therefore the ap- proaches to measure the representativeness of document collections are timely. In this  context, it is also important to know what would be a minimal representative subset of  documents.   The research presented in this paper1 develops the methodological and instrumen- tal components for measuring the representativeness of high-quality collections of  textual documents. It is assumed that the documents in a collection cover a single and  well circumscribed domain and have a timestamp associated with them – so can be  ordered by publication time. A typical example of such a collection is the set of the  full text papers of a professional journal or conference proceedings series. The main  hypothesis, put forward in this work, is that a sub-collection can be considered as  representative to describe the domain, in terms of its terminological footprint, if any  additions of extra documents from the entire collection to this sub-collection do not  noticeably change this footprint. Such a sub-collection is further considered as com- plete and could be used e.g. for learning an ontology from it. In fact, this approach to  assess the representativeness does so by evaluating terminological saturation in a  document collection.                                                              In this approach we are concerned about automated term extraction, as doing so  manually is not feasible for any realistic document collection pretending to cover a  professional domain. Therefore, it is important to know if terminological saturation  depends on a term extraction method, implemented in a software tool. For finding this  out, the presented research project cross-evaluated the two software tools. The choice  of these particular tools from the bunch of the other available is explained in our re- view of the related work in Section 2.    The approach to measure terminological saturation is based on the use of the  THD algorithm developed in frame of our OntoElect methodology for ontology re- finement [2]. This part of OntoElect is outlined in Section 3.   Sections 4 to 6 present our contributions.   We focused our experiments on a single but important factor that may influence  terminological saturation – the choice of an ATE software tool. Further, we presented  our generic workflow to support different series of experiments answering different  research questions in our project [1]. We also developed the suite of instrumental    1  This research is performed as the PhD project by the first author. Its exposé has been pre- sented in [1].   software modules to support our experimental workflow. We provided a more de- tailed experimental set-up, based on the generic workflow, for studying the influence  of the choice of the term extraction software. This contribution is presented in Sec- tion 4.   For evaluating the aspect of the choice of a term extraction software, we cross- evaluated the two selected software tools, UPM Term Extractor2 versus NaCTeM  TerMine3, on two synthetic and three real document collections of full-text papers  from different domains. Section 5 presents the document collections and datasets, and  further elaborates on the details of the experimental set-up. The results of our cross- evaluation experiments are presented and discussed in Section 6.   Finally, we summarize our results in Section 7, which concludes the paper.   2 Motivation and Related Work   Extracting terminology from texts is a complicated and laborious process which re- quires a substantial part of highly qualified human effort. Despite that it is more and  more often used in many important applications, e.g. for engineering ontologies [2],  [3]. So, knowing the smallest possible representative document collection for a do- main is very important to efficiently develop ontologies with satisfactory domain  coverage. Therefore, laying out a method to determine a terminologically saturated  subset of documents of the minimal size within a collection is topical. It is also impor- tant to make this method as efficient and automated as possible to lower the overhead  on the core knowledge engineering workflow.  As the focus of this paper is to find out which relevant term extraction software  yields the best (smallest) saturated sub-sets of documents, we review the related work  along the following aspects. We look at the comparison of existing ATE approaches  in terms of the quality of their results. We also consider as relevant those methods  (ATE algorithms plus metrics) which are domain-independent, unsupervised, and  allow assessing the significance of extracted terms. Further we check if the selected  methods are implemented as software tools which are publicly available for our ex- periments. We also pay attention to whether the tools return data for term significance  evaluations that are essential for our further saturation measurements.  2.1 Methods for Automated Term Extraction   Despite being important for practice, ATE is still far from being reliable. New ap- proaches to ATE are being proposed and still demonstrate their precision at the level  below 80 percent [4].  So, these can hardly be used in industry. Several reviews have                                                              2  UPM Term Extractor could be downloaded from https://github.com/ontologylearning- oeg/epnoi-legacy. It has to be further installed locally for use.   3  The batch service of NaCTeM TerMine is available at http://www.nactem.ac.uk/batch.php.  Access needs to be requested.    been performed to compare and cross-evaluate ATE methods, e.g. [5].  Perhaps, [4]  and [20] are the most recent work on that.   In the majority of approaches to ATE, e.g. [6] or [7], processing is done in two  consecutive phases: Linguistic Processing and Statistical Processing. Linguistic proc- essors, like POS taggers or phrase chunkers, filter out stop words and restrict candi- date terms to n-gram sequences: nouns or noun phrases, adjective-noun and noun- preposition-noun combinations. Statistical processing is then applied to measure the  ranks of the candidate terms. These measures are [5] either the measures of ‘uni- thood’, which focus on the collocation strength of units that comprise a single term; or  the measures of ‘termhood’ which point to the association strength of a term to do- main concepts.  For ‘unithood’, the metrics are used such as mutual information [8], log likelihood  [9], t-test [6], [7], the notion of ‘modifiability’ and its variants [10], [7]. The metrics  for ‘termhood’ are either term frequency-based (unsupervised approaches) or refer- ence corpora-based (semi-supervised approaches). The most used frequency-based  metrics are TF/IDF (e.g. in [4], [11]), weirdness [12] which compares the frequency  of a term in the evaluated corpus with that in the reference corpus, domain pertinence  [14]. More recently, hybrid approaches were proposed, that combine ‘unithood’ and  ‘termhood’ measurements in a single value. A representative metric is c/nc-value  [13]. C/nc-value-based approaches to ATE have received their further evolution in  many works, e.g. [6], [14], [15] to mention a few.  Linguistic Processing is organized and implemented in a very similar fashion in all  the ATE methods, except some of them that also include filtering out stop words.  Stop words (terms) could be filtered out also at a cut-off step after statistical process- ing. So, in our review and selection we further look at the second phase of Statistical  Processing only. Statistical Processing is sometimes further split in two consecutive  sub-phases of term candidate scoring, and ranking. For term candidates scoring, re- flecting its likelihood of being a term, known methods could be distinguished by be- ing based on (c.f. [4]) measuring occurrences frequencies (including word associa- tion), assessing occurrences contexts, using reference corpora, e.g. Wikipedia [16],  topic modeling [17].   A cut-off procedure, takes the top candidates, based on scores, and thus distin- guishes significant terms from insignificant (or non-) terms. Many cut-off methods  rely upon the scores, coming from one scoring algorithm, and establish a threshold in  one or another way. Some others that collect the scores from several scoring algo- rithms use (weighted) linear combinations [18], voting [5], [2], or (semi-)supervised  learning [19]. In our set-up, we do cut-offs after term extraction based on voting, as  explained in Section 3. So, the ATE algorithms / solutions which perform cut-offs  together with scoring are not relevant for our experimental setting.     Based on the evaluations in [5], [4], [20] the most widely used ATE algorithms, for  which their performance assessments are published, are listed in Table 1. The table  also provides the assessments on the aspects that we use for selection.   Table 1: The comparison of the most widely used ATE metrics and algorithms  Method  [Source]  Domain- indepen- dence   (+/-)  Super- vizion  (U/SS)  Metrics  Term  Signi- ficance Cut- off  (+/-) Precision  (GENIA;  average)  Run  Time  (%/c- value)      TTF [21]  +  U  Term (Total)  Frequency  +  -  0.70; 0.35  0.34  0.71; 0.33  0.37  ATF [20]  +  U  Average Term  Frequency  +  -  0.75; 0.32  0.35      TTF-IDF  [22]  +  U  TTF+Inverse  Document Fre- quency  +  -  0.82; 0.51  0.35  0.71; 0.32  0.53  RIDF  [23]  +  U  Residual IDF  -    0.80; 0.49  0.37  0.73; 0.53  1.00  C-value  [13]  +  U  c-value,   nc-value  +  -  0.77; 0.56  1.00  0.77; 0.47  0.41  Weird- ness [12]  +/-  SS  Weirdness  -    0.82; 0.48  1.67      GlossEx  [18]  +  SS  Lexical (Term)  Cohesion, Do- main Specificity  -    0.70; 0.41  0.42      TermEx  [14]  +  SS  Domain Perti- nence, Domain  Consensus, Lexi- cal Cohesion,  Structural Rele- vance  -  +  0.87; 0.46  0.52  0.78; 0.57  809.21  PU-ATR  [16]  -  SS  nc-value,  Domain  Specificity  -  +      Comments:   Domain Independence: “+” stands for a domain-independent method; “-“ marks that the  method is either claimed to be domain-specific by its authors, or is evaluated only on one par- ticular domain. We are looking for a domain-independent method.   Supervision: “U” – unsupervised; “SS” – semi-supervised. We are looking for an unsupervised  method.   Term Significance: “+” – the method returns a value for each retained term which could fur- ther be used as a measure of its significance compared to the other terms. “-“ – marks that such  a measure is not returned or the method does the cut-off itself. We are looking for receiving a  measure to do cut-offs later.   Cut-off: “+” – the method does cut-offs itself and returns only significant terms; “-” – the  method does not do cut-offs. We are looking for “-”.  Precision and Run Time: The values are based on the comparison of the two cross-evaluation  experiments reported in [4] / [20]. Empty cells in the table mean that there was no data for this  particular method in this particular experiment. [4] used ATR4S – open-source software written  in Scala. It evaluated 13 different methods, implemented in ATR4S, on 5 different datasets,  including GENIA.  [20] used JATE 2.0, free software written in Java. It evaluated 9 different  methods, implemented in JATE, on 2 different datasets, including GENIA. So, the results on  GENIA are the baseline for comparing the Precision. Two values are given for each reference  experiment: precision on GENIA; average precision. Both [4] and [20] experimented with c- value method which was the slowest on average for [20]. So, the execution times for c-value  were used as a baseline to normalize the rest in the Run Time column.     After analyzing the findings listed in Table 1, we support the conclusion of [20]  stating that “c-value is the most reliable method as it obtains consistently good results,  in terms of precision”, evenly on the two different mixes of datasets – [20] and [4].  We also note that c-value is one of the slowest in the group of unsupervised and do- main-independent methods, though its performance is comparable with the fastest  ones. Still, c-value outperforms the domain-specific methods, sometimes signifi- cantly, as it is in the case with PU-ATR. Hence, we have chosen c-value as the  method for our cross-evaluation experiments. We were therefore looking further at  the tools which implemented c-value and were publicly freely available.   2.2 Available Software Implementations  For choosing the software tools that implement the c-value method for ATE we  looked at the descriptions of term extraction tools at several web resources like at  http://inmyownterms.com/terminology-extraction-tools/ or https://en.wikipedia.org/  wiki/Terminology_extraction. In addition to the reference implementations mentioned  before, ATR4S [4] and JATE 2.0 [20], we have identified the following freely avail- able ATE software tools as outlined in Table 2.   For the final selection of the tools for our cross-evaluation we:   Decided not to consider ATR4S and JATE 2.0, at list at this stage, because it was  not fully clear how to extract the c-value method implementation from these suites   Selected the tools that use the c-value method – which are NaCTeM TerMine and  UPM Term Extractor  3 OntoElect Saturation Metric and Measurement Pipeline  OntoElect methodology [2] seeks for maximizing the fitness of the developed ontol- ogy to what the domain knowledge stakeholders think about the domain. Fitness is  measured as the stakeholders’ “votes” which allows assessing stakeholders’ commit- ment to the ontology under development, reflecting how well their sentiment about  the requirements is met. The more votes are collected – the higher the commitment is  expected to be. If a critical mass of votes is acquired (say 50%+1, which is a simple  majority vote), the ontology is considered to satisfactorily meet the requirements.   Table 2: Free ATE Software Tools (Listed Alphabetically)  Name /  Owner  Website  Short description  Algo- rithm /  Metric  Domain  Con- straints  BioTex /  LIRMM  http://tubo.lir mm.fr/biotex/   Extracts biomedical  terms from free text    Bio- medical  Domain- specific  FiveFilters  / Medialab- Prado  http://fivefilter s.org/term- extraction/   Extracts terms through a  web service; relies on a  PHP port of Topia's  Term Extraction; a sim- ple alternative to Yahoo  Term Extraction service  Occurrence  (TTF) and  word count  in a term  independ- ent  Web ser- vice, size of  text con- strained   TaaS (TaaS  EU Project)  https://term.til de.com/   Identifies term candi- dates in documents and  extracts them automati- cally. Uses CollTerm  (linguistic) or Kilgray  (statistical) services  Frequency- based  independ- ent  Does not  provide term  significance  scores  TerMine /  NaCTeM  http://www.na ctem.ac.uk/sof tware/termine/   Extracts terms from plain  English texts, provides  the Batch mode (access  to be requested for non- UK academic users)  c-value  independ- ent  The service  requests to  avoid heavy  bulk proc- essing  Term- Finder /  Trans- lated.net  https://labs.tra ns- lated.net/termi nology- extraction/   A Web application that  extracts terms from the  inserted text. Compares  the frequency of words  in a given document with  their frequency in the  language (generic cor- pus).   Poisson  statistics,  Maximum  Likelihood  Estimation  and IDF  requires  language  corpus  Returns the  score of a  term as a  numeric  value (%)  TBXTools  [24] / Uni- versitat  Oberta de  Catalunya  https://sourcef orge.net/pro- jects/tbxtools/   A Python toolset using  NLTK (Natural Lan- guage Toolkit)  TTF  Independ- ent, multi- lingual,  requires  language  corpus  Deletes n- grams woth  stop words  UPM  Term Ex- tractor  [25] / Dr  Inventor  EU project   https://github. com/ontologyl earning- oeg/epnoi- legacy   A Java software for  extracting terms and  relations from scientific  papers.   c-value  Independ- ent  Takes text  input data of  at most 15  Mb     It is well known that direct acquisition of requirements from domain experts is not  very realistic as they are expensive and not really willing to do the work falling out of  their core activity. So, in OntoElect, we are focused on the indirect collection of the  stakeholders’ votes by extracting these from high quality and reasonably high impact  documents authored by the stakeholders.   An important feature to be ensured for knowledge extraction from text collections  is that a dataset needs to be statistically representative to cover the opinions of the  domain knowledge stakeholders satisfactorily fully. OntoElect suggests a method to  measure the terminological completeness of a document collection by analyzing the  saturation of terminological footprints of the incremental slices of the collection, as  e.g. reported in [26]. The full texts of the documents from the retrospective collection  are grouped in datasets in the increasing order of their timestamps. As pictured in  Fig. 1a, the first dataset D1 contains the first portion (inc) of documents. The second  dataset D2 contains the first dataset D1 plus the second incremental slice (inc) of  documents. Finally, the last dataset Dn contains all the documents from the collection.               (a)          (b)  Fig. 1: (a) Incrementally enlarged datasets in OntoElect; (b) an example of a bag of terms ex- tracted by TerMine.  At the next step of the OntoElect workflow the bags of terms B1, B2, …, Bn are  extracted from the datasets D1, D2, …, Dn, using TerMine software, together with  their significance (c-value) scores. Please see an example of a bag of terms extracted  by TerMine in Fig. 1b.    At the subsequent step, every extracted bag of terms Bi, i = 1, …, n is processed as  follows:   Normalized  scores  are  computed  for  each  individual  term:   n-score = c-value / max(c-value)   Individual term significance threshold (eps) is computed to retain those terms that  are within the majority vote. The sum of n-scores having values above eps form  the majority vote if this sum is higher that ½ of the sum of all n-scores.    The cut-off at n-score < eps is done     The result is saved in Ti – the bags of retained terms  After this step only significant terms, whose n-scores represent the majority vote,  are retained in the bags of terms. Ti are then evaluated for saturation by measuring  pair-wise terminological difference between the subsequent bags Ti and Ti+1,   i = 0, …, n-1. It is done by applying the THD algorithm [2]. We provide it also here in  Fig. 2 for reader convenience.   In fact, THD accumulates, in the thd value for the bag Ti+1, the n-score differences  if there were linguistically the same terms in Ti and Ti+1. If there was no the same  term in Ti, it adds the n-score of the orphan to the thd value of Ti+1. After thd has  been computed, the relative terminological difference thdr receives its value as thd  divided by the sum of n-scores in Ti+1.                              Algorithm THD. Compute Terminological Difference between Bags of Terms  Input:  1 ,  i i T T   Output:  ) , ( 1  i i T T thd   sum := 0  for  1 ,1   iT k       sum :=  sum +  1  i k ns        . . : F ident        for  iT m ,1          if    ) , ( 1 i k i m t t then do  1 :     i k i m ns ns thd thd ;  . . : T ident   end do    end for    if   . . : F ident   then  1 :    i k ns thd thd   end for  thdr := thd  / sum   Pick up one  Look for linguistically similar in the previous   Found: check the n-scores   Not found: add the n-score     Fig. 2: THD algorithm [2] for comparing a pair of bags of retained terms. It has been modified,  compared to [2], for computing the thdr value.    Absolute (thd) and relative (thdr) terminological differences are computed for fur- ther assessing if Ti+1 differs from Ti more than by the individual term significance  threshold eps. If not, it implies that adding an increment of documents to Di for pro- ducing Di+1 did not contribute any noticeable amount of new terminology. So, the  subset Di+1 of the overall document collection may have become terminologically  saturated. However, to obtain more confidence about the saturation, OntoElect sug- gests that some more subsequent pairs of Ti and Ti+1 are evaluated. If stable satura- tion is observed, then the process of looking for a minimal saturated sub-collection  could be stopped. Sometimes, however, a terminological peak may occur after satura- tion has been observed in the previous pairs of T. Normally this peak indicates that a  highly innovative document with a substantial number of new terms has been added  in the increment.   To finalize this brief presentation of the OntoElect approach, it is worth noting that  it is domain independent and unsupervised – due to the use of TerMine for term ex- traction. The shortcomings of this reliance on TerMine are revealed in our experimen- tal study (Section 6).   One of the tasks for our research, on which we focus in this paper, is trying Onto- Elect pipeline with the alternative term extraction tool – UPM Term Extractor – and  cross-evaluate the results versus those obtained using NaCTeM TerMine.    4 Research Questions, Workflow, and Software Tools  The objective of the presented experimental research project is to check if the Onto- Elect approach to assess the representativeness of a sub-collection within a document  collection, based on measuring terminological saturation, is valid. The setting of the  experiments should consider several aspects which may influence the measurements.  These aspects are taken into account while answering the following research ques- tions:  Q1: Which of the term extraction software tools yield better saturated sets of  terms?  Q2: Which would be the proper direction in forming the datasets to check satura- tion: chronological, reverse-chronological, bi-directional, random selection? Which  direction is the most appropriate to cope with potential terminological drift in time?  Q3: Would the size of a dataset increment influence saturation measurements?   Is there an optimal size of an increment for the purpose?   Q4: Would frequently cited documents form a minimal representative subset of  documents? Do these documents indeed provide the biggest terminological contribu- tion to the document collection?  Q5: Is the method for assessing completeness based on saturation measurements  valid? Does it indeed provide a correct indication of statistical representativeness?   The answers to the outlined research questions Q1 – Q4 are sought based on con- ducting experiments using different document collections coming from different do- mains and communities.   One possible way to answer Q5 is to do a cross-evaluation with another method for  ontology learning, e.g. [27]. An alternative could be to select a much smaller subset of  a document collection, e.g. only the papers with high terminological impact. The set  of terms extracted from this “decisive minority vote” subset could be manually  checked by human experts.  4.1 Generic Experimental Workflow and Instrumental Software  Our generic experimental workflow, outlined in Fig. 3, is based on the OntoElect  processing pipeline (Section 3). In particular, this workflow will be applied (using  Configure Experiment step) to perform all the cross-evaluation experiments described  below (Section 6).   The workflow covers the preparatory phase, experiment configuration, the genera- tion of the datasets, term extraction, saturation measurement, and the analysis and  comparison of the results. Some of the steps in these phases can only be performed  manually, like Configure Experiment, Analyze Saturation, and Compare Results.  These steps are not too laborious, however, and the effort does not noticeably grow  with the number of documents. To support the rest of the steps, the instrumental soft- ware has been developed and offered for public use – as described in [28].   The preparatory phase includes:   The generation of the catalogue for the chosen document collection using the  information available at the publisher’s web site. This catalogue includes all the  metadata for the documents, including their abstracts, and also the numbers of their  citations acquired from Google Scholar4. This step is supported by the Catalogue  Generator module.                                                              4  http://scholar.google.com/    The download of the full texts of the papers, usually in PDF format, based on the  information in the catalogue. This step may require the permission granted by the  owner of the collection to bulk-download their full texts. This step is supported by  the Full Text Downloader module.      Fig. 3: Experimental workflow   The conversion of the full texts of the downloaded documents to the plain text  format for further term extraction is supported by the PDF to Plain Text Conver- tor module   The configuration phase is the choice of the experimental setting and the parame- ters of the datasets to be generated. The experimental setting is defined by the series –  i.e. by the research question we wish to answer. The parameters are hence defined by  the objective of the series. These parameters are: the order of adding documents to a  dataset, the size of an increment, the software tool used for term extraction.   The datasets generation phase takes these parameters and the document collection  in the plain text format. The datasets are then generated (Section 3), to be further  taken by term extraction. The texts are added to the increments in the order chosen as  the parameter of the experiment. This phase is supported by the Dataset Generator  module.   The phase of term extraction applies the chosen software tool to the generated  datasets: D1, D2, … In result, it outputs the bags of extracted terms B1, B2, … In the  context of reported experiments, this phase is supported by the use of the two soft- ware tools: UPM Term Extractor and NaCTeM TerMine. UPM Term Extractor  has been developed in the Dr Inventor project. The tool takes a collection of docu- ments (PDF or plain text) in English or a dataset generated from this collection (plain  text) and returns the bag of extracted terms as a CSV file. Each term is provided in a  separate line with its c-value. NaCTeM TerMine is a publicly available service which  is used in a batch mode5. It takes an English plain text (ANSI) document (dataset) as  a file to upload and returns the bag of extracted terms as a CSV output.  Each term is  provided in a separate line and accompanied with its numeric c-value and frequency  (TTF).  The saturation measurement phase applies the THD algorithm to the bags of  terms as explained in Section 3. It outputs the results in the tabular form (see [28] for  more details). This phase is supported by the THD modules, the Convertor module,  and StopTermRemover module. The THD modules implement the THD algorithm  for the input bags of terms in UPM Term Extractor and NaCTeM TerMine formats.  The Convertor takes a bag of terms in TerMine format and saves it in the UPM Ex- tractor format. The StopTermRemover takes the list of the manually selected stop  terms and deletes all these terms from the bags of terms.  The analysis and comparison are done manually using any appropriate software  tool. We use MS Excel in our experiments.    Hence, our experimental workflow is fully covered by the developed and used in- strumental software.    4.2 Planned Series of Experiments  Different series of experiments, using this workflow, are planned to be conducted in  the presented project [1].   The first series are planned for experimental cross-evaluation of the selected ATE  software tools. Based on the datasets with the increments of reasonable size, term  extraction is done separately using the UPM Term Extractor and NaCTeM TerMine.  The results are compared in terms of saturation measures. This may allow answering  Q1. In this paper we do not report about the experimental series aimed at answering  Q2-Q4, but focus on answering Q1 only.   For this we set-up the first series of experiments to cross-evaluate UPM Term Ex- tractor versus NaCTeM TerMine. In this sub-section we present the configuration of  these experimental series and the measurements in more detail.    We plan to perform this cross-evaluation by applying the experimental workflow  to the three selected real document collections coming from different domains. Before  applying the tools to the real document collections we check if they perform ade- quately on the two specifically crafted synthetic collections representing the boundary  cases – for immediate saturation and no saturation. All the document collections are  presented in more detail in Section 5.    To cross-evaluate term extraction tools we look at:   How quickly the bags of terms, extracted from the incrementally growing datasets,  saturate terminologically in terms of thd versus eps. We also measure thdr. The re- sults are measured for all the document collections, independently for each tool,  and then compared.                                                               5  Batch mode for TerMine is freely accessible at http://www.nactem.ac.uk/batch.php for aca- demic purposes, provided that the permission by NaCTeM is granted for non-UK users.    If the tools extract the similar bags of terms from each of the document collections  in which saturation has been observed. The similarity between the extracted bags  of terms is also measured using thd versus eps approach by applying the THD  module to the pairs (B1, B1m), (B2, B2m), …, (Bn, Bnm), where Bi is the bag of  terms extracted by the first chosen tool (UPM Term Extractor) and Bim is the bag  of terms extracted by the second chosen tool (NaCTeM TerMine).  5 Document Collections and Datasets  In this section we describe the data used in our experiments. These data come from  two synthetic and three real document collections6.   5.1 Synthetic Document Collections  Our synthetic collections have been prepared to evaluate the boundary cases: one in  which terminological saturation should happen immediately; and the other one in  which terminological saturation should not happen. These cases help us evaluate if  saturation metric is adequate at these two extremes. If so, there is more confidence  that it is also adequate for real document collections.  1DOC is the document collection containing just one paper. As this paper, we used  the source of [24]. It has been converted to plain ANSI text format manually. From  the plain text, the datasets D1, D2, …, D20 have been generated, as described in Sec- tion 3, and the increment for each subsequent dataset was the text of this one paper.  So, D1 contained one copy of this paper text, D2 – two copies of the same text, …,  D20 – 20 copies of the same text. It is straightforward that, if the OntoElect approach  to measuring saturation is correct, the saturation in this case should be observed quite  quickly with thd close to 0,  as all the increments are identical.   The intuition behind crafting the RAW collection is opposite to the previous case.  To avoid saturation, we need a collection in which all the increments are substantially  terminologically different. To have that, we need to put together the documents deal- ing with different topics, coming from different fields, and therefore using very dif- ferent terminology. For constructing RAW we have randomly selected 80 articles  from English Wikipedia such that no two of them are about a similar topic and the  size of an article is not too small. The articles have been downloaded in 1-column  PDF format. Further, we converted these PDF files to plain ASCII texts using our  PDF to Plain Text Convertor. The texts have not been cleaned to keep the possibility  for checking how does the noise injected by Wikipedia into the PDF printouts influ- ences saturation. Based on the plain texts, we generated 20 datasets, D1, D2, …, D20,  with increments comprising 4 randomly taken documents from this collection.                                                                6  All the five collections in plain text and the datasets generated of these texts are publicly  available at: https://www.dropbox.com/sh/64pbodb2dmpndcy/AACoDO0iBKP6Lm4400ux  JQ6Ca?dl=0   5.2 Real Document Collections  Our real document collections are all composed of the papers published at the peer- reviewed international venues in three different domains: the TIME collection con- tains the full text papers of the proceedings of the TIME Symposia series7; the  DMKD collection contains the subset of full text articles from the Springer journal on  Data Mining and Knowledge Discovery8; the DAC collection contains the subset of  full text papers of the Design Automation Conference9.  The domain of the TIME collection is Time Representation and Reasoning. The  publisher of these papers is IEEE. This collection has been acquired in our previous  research [24]. It contains all the papers published in the TIME symposia proceedings  between 1994 and 2013, which are 437 full text documents.  These papers have been  processed manually, including their conversion to plain texts and cleaning of these  texts. So, the resulting datasets were not very noisy. We have chosen the increment  for generating the datasets to be 20 papers. So, based on the available texts, we have  generated 22 incrementally enlarged datasets D1, D2, …, D22.  The domain of DMKD collection is Data Mining and Knowledge Discovery,  which falls into our broader target domain of Knowledge Management as its essential  part. It was provided by Springer based on their policy on full text provision for data  mining purposes10.  To the DMKD document collection, we have included 300 papers  published in the Journal of Data Mining and Knowledge Discovery between 1997 and  2010. All the papers in their full texts were automatically processed using our instru- mental pipeline. In difference to the TIME collection, no manual cleaning of docu- ment texts was applied. For generating the datasets, the increment has been chosen to  be 20 papers. So, based on the available documents we have generated 15 incremen- tally enlarged datasets D1, D2, …, D15.  The domain of the DAC collection is Engineering Design Automation. The pub- lisher of these papers is IEEE. For this collection, we have chosen 506 papers pub- lished between 2004 and 2010. The papers of DAC have been automatically con- verted to plain text using our instrumental software. We deliberately skipped manual  cleaning of the plain texts to be able to compare the results between very noisy (DAC)  and not very noisy (TIME) datasets generated from the papers having the same pub- lisher and, therefore, the same source layout (IEEE). Similarly to TIME, we have  chosen the increment for generating the datasets to be 20 papers. So, based on the  available texts, we have generated 26 incrementally enlarged datasets D1, D2, …,  D26.                                                              7  http://time.di.unimi.it/TIME_Home.html  8  https://link.springer.com/journal/10618  9  http://dac.com/  10 https://www.springer.com/gp/rights-permissions/springer-s-text-and-data-mining-policy/  29056  5.3 Summary of Data Features  The characteristics of all the five document collections and datasets are summarized  in Table 3.   Table 3: The features of the used document collections and datasets  Collec- tion  Type  Paper Type  and Layout  No  Doc  Noise  Processing  Inc  No   Datasets  1DOC  synthetic  journal, ACM  1-column  1  manually  cleaned  manual  1 paper  20  RAW  synthetic  Wikipedia   1-column  80  not cleaned,  moderately  noisy  automated  4 papers  20  TIME  real  conference,  IEEE   2-column  437 manually  cleaned  manual conver- sion to plain text,  automated data- set generation  20 papers  22  DMKD real  journal,  Springer 1- column  300 not cleaned,  moderately  noisy  automated  20 papers  15  DAC  real  conference,  IEEE 2-column 506 not cleaned,  quite noisy  automated  20 papers  26    For all real collections, the documents have been added to the datasets in their  chronological order of publication. For the RAW collection the documents have been  added in random order.  6 Experiments and Discussion  In this section we report and discuss the results of our experiments on the datasets  generated from the five data collections presented in Section 5, particularly on the  results of the phases of term extraction, saturation measurement, analysis and com- parison.   In the experiment with each collection we: (i) extracted the bags of terms from the  prepared datasets using NaCTeM TerMine and UPM Term Extractor; (ii) measured  saturation for both sets of the bags of terms using the corresponding THD modules;  (iii) measured comparative saturation for the pairs of the bags of terms (B1, B1m),  (B2, B2m), …, (Bn, Bnm) – as described in Section 4.2; (iv) built the diagrams and  analyzed the results  In addition to the above activities, for the RAW collection we also looked at the ef- fect of removing stop terms after doing term extraction. By removing these stop  terms, which represented the injection of noise by Wikipedia and also the text frag- ments from the figures, we de-noised the output. The lists of the stop terms were pre- pared manually based on the extractions from the last dataset D20. These stop terms  were further automatically removed from all the datasets using our Stop Term Re- mover module. So, for the RAW collection we also compared noisy and cleaned bags  of terms.   6.1 Terminological Saturation in Synthetic Collections   Due to collections design (Section 5), the results on 1DOC are expected to demon- strate quick and steady saturation and the results on RAW have to be far from being  saturated.   For the bags of terms extracted from 1DOC, the results of measuring saturation  look as follows.  We first processed the bags of terms extracted by TerMine. The results of measur- ing individual term significance thresholds (eps) and terminological differences (thd,  thdr) are visualized in Fig. 4(a)11. We then measured terminological differences be- tween the bags of terms extracted by UPM Extractor. The results of measuring indi- vidual term significance thresholds (eps) and terminological differences (thd, thdr) are  pictured in Fig. 4(b).                 (a) Bags of terms extracted by TerMine  (b) Bags of terms extracted by UPM Extractor  Fig. 4: Visualization of saturation measurements on the 1DOC datasets  The dashed vertical line in Fig. 4(a) points to the bag of terms (extracted from D3)  in which saturation indicator has been observed for the first time as thd went below  eps. In fact, and as expected, we further observe steady saturation with the same num- ber of extracted terms and increasing individual term significance threshold eps. The  values of thd and thdr drop down to become statistically equal to zero starting from  T2-T3. The dashed vertical line in Fig. 4(b) points to the bag of terms (extracted from  D4) in which saturation indicator has been observed for the first time as thd went  below eps. Very similarly to the case of TerMine, and as expected, we further ob- served very stable saturation with the same number of extracted terms and increasing  individual term significance threshold eps. The values of thd and thdr drop down to  become statistically equal to zero starting from T3-T4.                                                               11 The values measured in the reported experiments, though sometimes mentioned in the text,  are not presented for saving space. All these detailed experimental data is provided in the  supporting technical report [28] which is publicly available online.  The differences in saturation measurements for the bags of terms extracted by Ter- Mine and UPM Extractor are as follows: (i) UPM Extractor generated bigger bags of  terms with c-value > 1: 3 019 terms versus 1 208 in the TerMine case; (ii) idividual  term significance thresholds (eps) were about 2.5 times higher for UPM Extractor;  (iii) the number of retained terms with c-value > eps was ~2 times bigger in the UPM  Extractor case; (iv) the values of thd and thdr were significantly lower (~10 000  times) for TerMine.  Overall, TerMine results showed a slightly quicker convergence to saturation,  compared to UPM Extractor results. From the other hand: (i) the number of retained  terms from the saturated sub-collection; and (ii) the cut-off point at the individual  term significance threshold were higher in the UPM Extractor results. Based on ob- serving these differences, we can conclude that, linguistically, TerMine was ~3 times  more selective regarding extracting term candidates. So, the pre-processing in Ter- Mine is more sophisticated and, probably, more accurate. From the other hand, the  cut-offs in UPM Extractor outputs happened for approximately two times more sig- nificant terms. Hence, the statistical processing part in UPM Extractor circumscribes  more compact, yet significant sets of terms. This points out that, due to the statistical  processing phase, UPM Extractor is a more precise instrument.      Fig. 5: Comparison of the retained sets of terms extracted from the 1DOC collection by UPM  Term Extractor and NaCTeM TerMine  We further checked if both tools extracted statistically similar sets of terms from  the 1DOC collection. The measurements are visualized in Fig. 5. The figure shows  that both tools extracted statistically identical bags of terms despite the fact that the  numbers of retained terms differed significantly in the individual cases (reported  above). The terminological difference became statistically negligible at the second  measurement point, where the thd value (2.291409) went significantly below eps  (9.509775). This situation was stable, since the thd values oscillated around 2.1 and  the eps values steadily went up to 95.  For the bags of terms extracted from RAW the results of measuring saturation look  as follows.  We first processed the bags of terms extracted by TerMine. The results of measur- ing individual term significance thresholds (eps) and terminological differences (thd,  thdr) are visualized in Fig. 6(a).  We then analyzed B20, extracted by TerMine, going from the top of the list down  to the terms having c-values greater than 40. Based on this scan, we extracted the list  of ~200 stop terms. These stop terms have been removed from the bags of terms B1,  …, B20 and saturation analysis has been repeated. The results of measuring individual  term significance thresholds (eps) and terminological differences (thd, thdr) for so de- noised bags of terms are visualized in Fig. 6(b).     When looking at Fig. 6(a) and, especially, at 6(b), we observe that, as it was ex- pected, the RAW collection is not terminologically saturated.  Further, looking at the  differences between Fig. 6 (a) and (b), we observe some nice indicators of the pres- ence of noise in the textual documents of the collection. Indeed, the thdr values in  Fig. 6(a) are much higher than the corresponding thd values. Though the thd values  hint that the bags of terms might be close to saturation, the values of thdr are far be- yond eps. Very interestingly, the values of thd measured after removing stop terms  become similar to that of thdr. At the same time the thd and thdr curves in Fig 6(b)  very much resemble the thdr curve in Fig. 6(a).  So, substantial differences between  thd and thdr values signal about a possible need to clean the bags of terms, or the  source texts, by removing the stop terms which have no relevance to the domain of  the collection.          (a) Saturation measurements before removing stop terms       (b) Saturation measurements after removing stop terms  Fig. 6: Visualization of saturation measurements on the RAW bags of terms extracted   by NaCTeM TerMine. The diagram to the right represents a more granular look into the  rounded rectangle in the diagram to the left.  We then repeated the same experiment for the bags of terms extracted by the UPM  Term Extractor. The results of measuring saturation look as follows.  The values of individual term significance thresholds (eps) and terminological dif- ferences (thd, thdr) are visualized in Fig. 7(a). We then analyzed B20, extracted by  UPM Extractor, going from the top of the list down to the terms having c-values  greater than 40. Based on this scan, we extracted the list of ~220 stop terms. These  stop terms have been removed from the bags of terms B1, …, B20 and saturation  analysis has been repeated. The values of individual term significance thresholds  (eps) and terminological differences (thd, thdr) for so de-noised bags of terms are  pictured in Fig. 7(b).          (a) Saturation measurements before removing stop terms.        (b) Saturation measurements after removing stop terms.   Fig. 7: Visualization of saturation measurements on the RAW bags of terms extracted by UPM  Term Extractor.   Compared to the saturation measurements for the bags of terms extracted   by TerMine, the values of thd for the bags of terms extracted by UPM Extractor form  a clearer picture of the absence of saturation. In fact, the thd values measured on UPM  Extractor results before removing the stop terms are 2.5-3 times higher than those  measured on TerMine results after removing the stop terms. So, the results by UPM  Extractor are more highly contrast compared to those of TerMine in terms of detect- ing the absence of saturation. From the other hand, the values of thdr measured on  TerMine results are a much sharper indicator of the need to de-noise the bags of  terms. The thdr values measured on the UPM Extractor results do not differ from the  corresponding thd values. If UPM Extractor is used to detect the absence of satura- tion, there is no real need however to analyze if thdr values indicate the presence of  noise. So, the use of UPM Extractor is preferred in this case as it is a more precise  instrument.  For this collection we did not measure if both tools extract statistically similar bags  of terms. This measurement would have no value in the absence of saturation.  6.1  Terminological Saturation in Real Collections   We now present and analyze our results on measuring terminological saturation in our  real document collections.   For the datasets extracted from DMKD the results look as follows.   We first processed at the bags of terms extracted by TerMine. The results of meas- uring individual term significant thresholds (eps) and terminological differences (thd,  thdr) are visualized in Fig. 8. The diagram at the left visualizes the whole set of  measures. The rounded rectangular circumscribes the area in the diagram at the left,  which is presented in finer detail in the diagram at the right. The dashed vertical line  points to the bag of terms (extracted from D14) in which saturation indicator has been  observed for the first time as thd went below eps.           Fig. 8: Saturation measurements on the DMKD datasets based on the bags of terms extracted  by NaCTeM TerMine  The analysis of these results points out that there is a trend to reaching termino- logical saturation, perhaps for bigger datasets. The eps values have the tendency to go  up and thd, thdr values go down with the increase in dataset numbers. The increase in  the numbers of retained terms is also going down. There are three terminological  peaks in the area of our closer interest at D10-D11, D12-D13, and D14-D15. The  contribution of these peaks is not very significant however as the thd value increases  not very much versus the vicinity – please see DAC results for comparison. Overall, it  is too early to consider DMKD saturated based on the extraction results by TerMine.  The results of measuring saturation based on the bags of terms extracted by UPM  Term Extractor are pictured in Fig. 9. It could be noted that steady saturation is  reached at D5-D6. The number of retained terms (from B6) is 4113, which is substan- tially lower than 5009 at the first potential saturation point in the TerMine case. Inter- estingly, thd and thdr values measured on UPM Term Extractor results behave quite  similarly to those measured on TerMine results, also hinting about terminological  peaks at the same points. The numbers of retained terms are lower, though not signifi- cantly, for UPM Term Extractor results. Saturation is reached due to much higher  values of individual term significance threshold eps. Hence, for this document collec- tion, UPM Term Extractor yields better circumscribed and more compact sets of  significant terms and the cut-off happens at much higher values of term significance  (n-score).          Fig. 9: Saturation measurements on the DMKD datasets based on the bags of terms extracted  by UPM Term Extractor  One hypothesis about the reason for better UPM Term Extractor performance  could be that it extracts not all the terms from the documents it takes in, and NaCTeM  TerMine reaches substantially higher recall. To check that, we measured terminologi- cal differences between the bags of terms extracted, from the same datasets by UPM  Extractor and TerMine. The result is pictured diagrammatically in Fig. 10.         Fig. 10: Comparison of the retained sets of terms extracted from the DMKD collection by  UPM Term Extractor and NaCTeM TerMine  Fig. 10 shows that both tools extract somewhat similar bags of terms. This similar- ity increases with the growth of a dataset. The individual term significance thresholds  (eps) are similar in values to the case of UPM Term Extractor. The numbers of re- tained terms are higher, however, than in Fig. 8 and 9. These also hint that the ex- tracted bags of terms are similar and recall values of individual tools differ not too  much, which is acceptable.   Interestingly, terminological difference (thd) in Fig. 10 goes below eps exactly at  the point when TerMine results show the highest terminological peak (c.f. Fig. 8). So,  it looks like both tools extract similar bags of terms but TerMine reaches the satura- tion level a bit later, when it collects the contribution from the increment at the high- est terminology peak. Yet interestingly, thd values go beyond eps after D11. We  think12 that the reason for that is the increasing influence of the accumulated noise in  the datasets, which is processed differently by the individual tools.  The results of saturation measurements for TIME are pictured in Fig. 11.    The saturation measurements based on the bags of terms extracted by TerMine did  not show any saturation –Fig. 11(a). The thd values did not go below eps.  The ten- dency is similar to the DMKD experiment, however - a trend to reaching terminologi- cal saturation, perhaps for bigger datasets. The eps values go up with the increase in  dataset numbers, though significantly slower than in the DMKD case. The maximal  observed eps value is 5 for TIME versus 9 for DMKD. The thd and thdr values go  down with the increase in dataset numbers, but not quickly enough to go below eps.  As a consequence, the maximal number of retained terms is significantly higher that  in the DMKD case: 8343 versus 5438, though the difference in the extracted numbers  of terms is not that significant: ~287K versus ~253K. Interestingly, the terminological  peaks in the TIME collection are observed at D3-D4, D10-D11, D17-D18, and D19- D20. The highest peak is at D10-D11, which repeats the DMKD case, probably by a  coincidence. Similarly to DMKD, the contribution of these peaks is not very substan- tial as the thd value increases not very much compared to the vicinity.        (a) NaCTeM TerMine       (b) UPM Term Extractor      (c) Comparison of the retained sets of terms  Fig. 11: Saturation measurements for the TIME collection  The saturation measurements based on the bags of terms extracted by UPM Term  Extractor reveal stable saturation starting from D11-D12 – as pictured in Fig. 11(b)  by the vertical dashed line. The values of thd and thdr resemble these of the TerMine                                                              12  We did not yet check this. So, it is only a hypothesis.   case, so the saturation curve has terminological peaks nearly at the same points. The  height of those peaks is however lower. The values of individual term significance  threshold eps are however much higher – similarly to the DMKD experiment. Satura- tion is detected at eps equal to 23.774, whereas the values of eps in the TerMine case  do not increase beyond 5.000. The number of retained terms, from B12 is 7110, which  is only 2.47% of the total number of extracted terms in B12. Therefore, we may draw  a similar conclusion for this experiment. Saturation is reached due to much higher  values of individual term significance threshold eps. For TIME, UPM Term Extrac- tor yields better circumscribed and more compact sets of significant terms and the  cut-off happens for much higher values of term significance (n-score).  We also checked if both tools extract similar bags of terms from the TIME collec- tion. The results have been measured following the same approach as in the case of  DMKD and are pictured in Fig. 11(c). It could be seen, that the terminological differ- ence (thd) between the bags of retained terms at the saturation point D12-D12m13   equals to ~29, while eps equals to 16. So, thd is 1.81 times higher than eps. In the  DMKD case the difference between thd and eps at the saturation point is slightly  lower – 1.80 times. Very similarly to the DMKD case, the difference grows after the  saturation point, which, as we believe, could be explained by the same reason – the  influence of the accumulated noise in the datasets beyond the saturation point. Hence,  manual cleaning of the TIME datasets did not really help a lot, as the results very  much resemble the DMKD case, for which the datasets were not cleaned.     The results of saturation measurements for DAC are shown in Fig. 12. DAC col- lection is much noisier than DMKD and TIME. The results also differ – in values but  not in the overall picture.  The saturation measurements based on the bags of terms extracted by TerMine re- vealed the potential saturation point only in the last measurement at D25-D26 – as  pictured in Fig. 12(a). However, the terminological peak at D24-D25, with thd equal  to 135.49, hints about the further instability. So, speaking about a tendency to reach  stable saturation later would be a speculation. More measurements are needed to  judge about it.   It is also interesting to compare the saturation behaviour in DAC to that in TIME,  as both collections come from the same publisher, so have the same layout, and repre- sent papers of similar size. The difference is that TIME was manually cleaned and  DAC was not. Fig. 11(a) and 12(a), if compared, show the differences in measure- ment values for the dataset pairs of roughly similar sizes.   The comparison of the measurements for TIME and DAC, based on the extraction  results by TerMine, reveals that: (i) the values of eps grow faster for TIME than for  DAC; (ii) the numbers of extracted and retained terms for DAC are substantially  higher than for TIME; (iii) the numbers of retained terms for TIME grow monotoni- cally and this growth slows down – an indicator of possible saturation in the upcom-                                                             13 D12 is the dataset from which B12 is extracted by UPM Extractor and B12m by TerMine.  B12m is further converted to the UPM Extractor format and the pair (B12, B12m) is fed into  the THD module. The module returns eps, thd, and thdr values for the pair as described in  Section 3.   ing measurements; (iv) the number of retained terms for DAC substantially drops  below the previous value at D24-D25 and the thd dramatically picks up from 21.51 to  135.49. We believe, again, that the reason for the peak at D24-D25 is the influence of  the accumulated noise. However, TerMine signals about the problem quite lately.         (a) NaCTeM TerMine       (b) UPM Term Extractor      (c) Comparison of the retained sets of terms  Fig. 12: Saturation measurements for the DAC collection  The saturation measurements based on the bags of terms extracted by UPM Term  Extractor reveal steady saturation starting from D5-D6 with eps at about 20 – as  pictured in Fig. 12(b) by the vertical dashed line. However, the values of eps peak up  to 18 294 at D10-D11 and the numbers of retained terms go down to 34 which is more  than 100 times less than the previous value. A closer examination of the bags of terms  revealed that these 34 terms are nothing but the noise which has been accumulated  much earlier in the case of UPM Extractor. Therefore, in the case of a noisy document  collection, UPM Extractor is much more sensitive in detecting excessive noise, com- pared to TerMine. So, the situation pictured in Fig. 12(b) could be used as an indica- tor of the need to clean the collection before terminology extraction.   Though not very relevant for this collection, we still compared if the bags of terms  extracted by both tools were statistically similar. The result is pictured in Fig. 12(c).  The comparison showed that, starting from D5, where thd equals to 3.97 and eps to  19.65, both tools successfully extracted the very similar sets of accumulated noise  terms.   7 Conclusions and Recommendations  This section summarizes our findings after analyzing the results of the experiments on  cross-evaluating NaCTeM TerMine and UPM Term Extractor. The summary is struc- tured along the cases based on our document collections.   Case 1: 1DOC – quick saturation expected. For the bags of terms extracted by  both tools very stable saturation has been observed quite quickly – which was ex- pected. The differences in saturation measurements are as follows: (i) UPM Extractor  generated bigger bags of terms with c-value > 1: 3 019 terms versus 1 208 in the  TerMine case; (ii) individual term significance thresholds (eps) were about 2.5 times  higher for UPM Extractor; (iii) the number of retained terms with c-value > eps was  approximately 2 times bigger in the UPM Extractor case; (iv) the values of thd and  thdr were significantly lower (~10 000 times) for TerMine. Overall, TerMine results  showed a slightly quicker convergence to saturation than that by UPM Extractor.  From the other hand: (i) the number of retained terms from the saturated sub- collection; and (ii) the cut-off point at the individual term significance threshold were  higher in the UPM Extractor results. Both tools extracted statistically similar bags of  terms despite the fact that the numbers of retained terms differed significantly. Over- all, both tools behaved, in detecting saturation and extracting similar bags of terms,  exactly as expected by the design of the case.   Conclusions (case 1): (i) these results confirm the adequacy of our saturation  metric for the boundary case of quick saturation; (ii) linguistically, TerMine is more  selective in extracting term candidates, (iii) the cut-offs in UPM Extractor outputs  happen for substantially more significant terms; (iv) UPM Extractor circumscribes  more compact, yet more significant sets of terms. and is overall a more precise in- strument.  Case 2: RAW – saturation should not be reached. While measuring saturation in  the bags of terms extracted by TerMine, we observed that saturation has not been  reached. We also noticed that the measurements of thd and thdr on these bags of  terms differed noticeably for the cases before and after removing stop terms. So, these  differences between thd and thdr values signal about a possible need to clean the bags  of terms, or the source texts, by removing the stop terms which have no relevance to  the domain of the collection. The thd values measured on UPM Extractor results be- fore removing the stop terms are 2.5-3 times higher than those measured on TerMine  results after removing the stop terms. So, the results by UPM Extractor are more  highly contrast compared to those of TerMine in terms of detecting the absence of  saturation. Overall, both tools behaved, in failing to detect saturation and extracting  similar bags of terms, as expected by the design of the case.  Conclusions (case 2): (i) TerMine is more sensitive in indicating the need to de- noise the bags of terms; (ii)  UPM Extractor is a more precise instrument to detect  the absence of saturation;  (iii) these results confirm the adequacy of our satura- tion metric for the boundary case for non-reachable saturation  Recommendation: The use of UPM Extractor is preferred to detect that satura- tion is hardly expected.  Case 3: DMKD collection (automatically pre-processed). Overall, it cannot be  reliably judged that the DMKD collection is saturated based on the extraction results  by TerMine. In difference to that, the saturation measurements using the bags of terms  extracted by UPM Extractor reveal steady saturation quite quickly. For this document  collection, UPM Term Extractor yields better circumscribed and more compact  sets of significant terms and the cut-off happens for much higher values of term sig- nificance (n-score). It has also been noticed that both tools extracted statistically simi- lar bags of terms in terms of terminological difference.   Case 4: TIME collection (manually cleaned). Saturation measurements using the  bags of terms extracted by TerMine failed to detect saturation in the TIME collection.  Very similarly to the DMKD case, the saturation measurements using the bags of  terms extracted by UPM Extractor reveal steady saturation quite quickly, also with  much higher individual term importance thresholds eps. These result in significantly  more compact sets of retained significant terms (2.47% of all extracted). Hence, for  this document collection, UPM Term Extractor also yields better circumscribed  and more compact sets of significant terms.   Conclusion (cases 3 and 4): Both tools yielded similar results in detecting satura- tion and retaining significant terms for DMKD and TIME collections. Manual clean- ing of the TIME collection did not help noticeably for improving the results of satura- tion measurements – therefore is not really necessary.   Case 5: DAC collection (very noisy). UPM Extractor demonstrated the capacity  to accumulate excessive noise from the datasets to the bags of terms substantially  earlier than TerMine. The saturation curve, built for the measurements using UPM  Extractor results, signals about this noise quite sharply – with the numbers of retained  significant terms dropping down by two orders of magnitude and individual term  significance thresholds going up by three orders of magnitude.   Conclusion (case 5): In the case of noisy datasets and due to not being very selec- tive in extracting term candidates, UPM Extractor is much more sensitive in detect- ing excessive noise, compared to TerMine.  Recommendation: The use of UPM Extractor is preferred over TerMine to de- tect terminological saturation or excessive noise; this is not constrained by a subject  domain and does not depend on manual de-noising of the source data in the collec- tion.  Acknowledgements     The first author is funded by a PhD grant from Zaporizhzhia National University and  the Ministry of Education and Science of Ukraine. The research leading to this paper  has been done in part in cooperation with the Ontology Engineering Group of the  Universidad Politécnica de Madrid in frame of FP7 Marie Curie IRSES SemData  project (http://www.semdata-project.eu/), grant agreement No PIRSES-GA-2013- 612551. A substantial part of the instrumental software used in the reported experi- ments has been developed in cooperation with BWT Group. The collection of  Springer journal papers dealing with Knowledge Management, including DMKD, has  been provided by Springer-Verlag.   References     1. Kosa, V., Chugunenko, A., Yuschenko, E., Badenes, C., Ermolayev, V., Birukou, A.: Se- mantic Saturation in Retrospective Text Document Collections. In: Mallet, F., Zholt- kevych, G. (eds.) Proc. ICTERI 2017 PhD Symposium, CEUR-WS, vol. 1851, pp. 1--8,  Kyiv, Ukraine, May 16-17 (2017) online  2. Tatarintseva, O., Ermolayev, V., Keller, B., Matzke, W.-E.: Quantifying Ontology Fitness  in OntoElect Using Saturation- and Vote-Based Metrics. In: Ermolayev, V., et al. (eds.)  Revised Selected Papers of ICTERI 2013, CCIS, vol. 412, pp. 136--162 (2013)  3. Osborne, F., Motta, E.: Klink-2: Integrating Multiple Web Sources to Generate Semantic  Topic Networks. In: Arenas, M. et al. (eds.): ISWC 2015, Part I, LNCS, vol. 9366, pp.  408--424 (2015) DOI: 10.1007/978-3-319-25007-6_24   4. Astrakhantsev, N.: ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition  Methods in Scala. arXiv preprint arXiv:1611.07804 (2016)  5. Zhang, Z., Iria, J., Brewster, C., Ciravegna, F.: A comparative evaluation of term recogni- tion algorithms. In: Proc. Sixth Int Conf on Language Resources and Evaluation, LREC08,  Marrakech, Morocco (2008)  6. Fahmi, I., Bouma, G., van der Plas, L.: Improving statistical method using known terms  for automatic term extraction. In: Computational Linguistics in the Netherlands, CLIN 17  (2007)  7. Wermter, J., Hahn, U.: Finding new terminology in very large corpora. In: Clark, P.,  Schreiber, G. (eds.) Proc.3rd Int Conf on Knowledge Capture, K-CAP 2005, pp. 137--144,  Banff, Alberta, Canada, ACM (2005) DOI: 10.1145/1088622.1088648  8. Daille, B.: Study and implementation of combined techniques for automatic extraction of  terminology. In: Klavans, J., Resnik, P. (eds.) The Balancing Act: Combining Symbolic  and Statistical Approaches to Language, pp. 49--66. The MIT Press. Cambridge, Massa- chusetts (1996)  9. Cohen, J. D.: Highlights: Language- and domain-independent automatic indexing terms for  abstracting. Journal of the American Society for Information Science 46(3), 162--174  (1995) DOI: 10.1002/(SICI)1097-4571(199504)46:3<162::AID-ASI2>3.0.CO;2-6  10. Caraballo, S. A., Charniak, E.: Determining the specificity of nouns from text. In: Proc.  1999 Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and  Very Large Corpora, pp. 63--70. (1999)  11. Medelyan, O., Witten, I. H.: Thesaurus based automatic keyphrase indexing. In:  Marchionini, G., Nelson, M. L., Marshall, C. C. (eds.) Proc. ACM/IEEE Joint Conf on  Digital Libraries, JCDL 2006, pp. 296--297, Chapel Hill, NC, USA, ACM (2006)   DOI: 10.1145/1141753.1141819  12. Ahmad, K., Gillam, L., Tostevin, L.: University of surrey participation in trec8: Weirdness  indexing for logical document extrapolation and retrieval (wilder). In: Proc. 8th Text RE- trieval Conf, TREC-8 (1999)  13. Frantzi, K. T., Ananiadou, S.: The c/nc value domain independent method for multi-word  term extraction. Journal of Natural Language Processing 6(3), 145--180 (1999)   DOI: 10.5715/jnlp.6.3_145  "
"","Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisco Yedro2, Matej Kovacic3, Matej Posinkovic3, Ian Makgill4, Chris Taggart5, Elena Simperl6, Till C. Lech1, and Dumitru Roman1 1 SINTEF AS, Oslo, Norway {firstname.lastname}@sintef.no 2 Universidad Polit´ecnica de Madrid, Madrid, Spain 3 Joˇzef Stefan Institute, Ljubljana, Slovenia 4 OpenOpps Ltd, London, the UK 5 OpenCorporates Ltd, London, the UK 6 King’s College London, London, the UK Abstract. Public procurement is a large market affecting almost ev- ery organisation and individual. Governments need to ensure efficiency, transparency, and accountability, while creating healthy, competitive and vibrant economies. In this context, we built a platform, consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border, and cross-lingual procurement knowledge graph. We developed end-user tools on top of the knowledge graph, for anomaly detection and cross-lingual document search. This paper describes our experiences and challenges faced in creating such a platform and knowledge graph and demonstrates the usefulness of Semantic Web technologies for enhancing public procurement. Keywords: Public procurement · Knowledge graph · Linked data. 1 Introduction The market around public procurement is large enough so as to affect almost every single citizen and organisation across a variety of sectors. For this reason, public spending has always been a matter of interest at local, regional, and national levels, and even more so, in times of great austerity and increased public scrutiny. Primarily, governments need to be efficient in delivering services, ensure transparency, prevent fraud and corruption, and build healthy and sustainable economies [1, 12]. In the European Union, every year, over 250.000 public authori- ties spend around 2 trillion euros (about 14% of GDP) on the purchase of services, works, and supplies1; while OECD estimates that more than 82% of fraud and corruption cases remain undetected across all OECD countries [18] costing as 1 https://ec.europa.eu/growth/single-market/public-procurement_en 2 A. Soylu et al. high as 990 billion euros a year in the EU [10]. Moreover, SMEs are often locked out of markets due to the high cost of obtaining the required information, where larger companies can absorb the cost. This leads to a tendency for governments to rely on monolithic suppliers without adequate competition to deliver good value for the taxpayers. The availability of high quality, open, and integrated procurement data could alleviate some of the aforementioned challenges. This includes government agen- cies assessing purchasing options, companies exploring new business contracts, and other parties (such as journalists, researchers, local communities, business associations, transparency activists, and individual citizens) looking for a better understanding of the intricacies of the public procurement landscape through decision-making and analytic tools. Projects such as the UK’s GCloud (Govern- ment Cloud)2 have already shown that small businesses can compete effectively with their larger counterparts, given the right environment. However, managing these competing priorities at a national level and coordinating them across differ- ent states and many disparate agencies is notoriously difficult. There are several directives put forward by the European Commission (e.g., Directive 2003/98/EC and Directive 2014/24/EU8) for improving public procurement practices. These led to the emergence of national public procurement portals living together with regional, local as well as EU-wide public portals [9]. Yet, there is a lack of common agreement across the EU (in many cases, even inside the same country) on the data formats for exposing such data sources and on the data models for representing such data, leading to a highly heterogeneous technical landscape. To this end, in order to deal with the technical heterogeneity and to con- nect disparate data sources currently created and maintained in silos, we built a platform, consisting of a set of modular REST APIs and ontologies, to publish, cu- rate, integrate, analyse, and visualise an EU-wide, cross-border, and cross-lingual procurement knowledge graph [22, 21] (i.e., KG, an interconnected semantic knowledge organisation structure [11, 26]). The knowledge graph includes pro- curement and company data gathered from multiple disparate sources across the EU and integrated through a common ontology network using an extract, transform, load (ETL) approach [3]. We built and used a set of end-user tools and machine learning (ML) algorithms on top of the resulting knowledge graph, so as to find anomalies in data and enable searching across documents in different languages. This paper reports the challenges and experiences we went through, while creating such a platform and knowledge graph, and demonstrates the usefulness of the Semantic Web technologies for enhancing public procurement. The rest of the paper is structured as follows. Section 2 presents the related work, while Section 3 describes the data sets underlying the KG. Section 4 explains the KG construction process (i.e., ontology network, data ingestion, and reconciliation). Section 5 presents the KG publication together with the overall architecture and API resources for the platform, while Section 6 describes the use of the KG for anomaly detection and cross-lingual document search. Finally, Section 7 concludes the paper by presenting the lessons learned. 2 https://www.digitalmarketplace.service.gov.uk Enhancing Public Procurement through an Integrated Knowledge Graph 3 2 Related Work We focus on procurement data related to tenders, awards, and contracts, and basic company data. We analyse relevant related works from the perspective of such types of data. Procurement and company data are fundamental to realising many key business scenarios and may be extended with additional data sources. Public procurement notices play two important roles for the public procure- ment process: as a resource for improving competitive tendering, and as an instrument for transparency and accountability [14]. With the progress of eGov- ernment initiatives, the publication of information on contracting procedures is increasingly being done using electronic means. In return, a growing amount of open procurement data is being released leading to various standardisation initiatives like OpenPEPPOL3, CENBII4, TED eSenders5, CODICE6, and Open Contracting Data Standard (OCDS)7. Data formats and file templates were defined within these standards to structure the messages being exchanged by the various agents involved in the procurement process. These standards primarily focus on the type of information that is transmitted between the various organi- sations involved in the process, aiming to achieve certain interoperability in the structure and semantics of data. The structure of the information is commonly provided by the content of the documents that are exchanged. However, these initiatives still generate a lot of heterogeneity. In order to alleviate these problems, several ontologies including PPROC [15], LOTED2 [8], MOLDEAS [19], or PCO [16], as well as the upcoming eProcurement ontology8 emerged, with different levels of detail and focus (e.g., legal and process-oriented). So far, however, none of them has reached a wide adoption mainly due to their limited practical value. Corporate information, including basic company information, financial as well as contextual data, are highly relevant in the procurement context, not only for enabling many data value chains, but also for transparency and account- ability. Recently, a number of initiatives have been established to harmonise and increase the interoperability of corporate and financial data. These include public initiatives such as the Global Legal Entity Identification System—GLEIS9, Bloomberg’s open FIGI system for securities10, as well as long-established propri- etary initiatives such as the Dun & Bradstreet DUNS number11. Other notable initiatives include the European Business Register (EBR)12, Business Register Exchange (BREX)13, and the eXtensible Business Reporting Language (XBRL) 3 https://peppol.eu 4 http://cenbii.eu 5 https://simap.ted.europa.eu/web/simap/sending-electronic-notices 6 https://contrataciondelestado.es/wps/portal/codice 7 http://standard.open-contracting.org 8 https://joinup.ec.europa.eu/solution/eprocurement-ontology 9 https://www.gleif.org 10 https://www.omg.org/figi 11 http://www.dnb.com/duns-number.html 12 http://www.ebr.org 13 https://brex.io 4 A. Soylu et al. format14. However, these are mostly fragmented across borders, limited in scope and size, and siloed within specific business communities. There are also a num- ber of ontologies developed for capturing company and company-related data including the W3C Organisation ontology (ORG)15, some e-Government Core Vocabularies16, and the Financial Industry Business Ontology (FIBO) [4]. Each one has a different focus (e.g., organisational and financial), and do not cover sufficiently the basic company information or are too complex due to many ontological commitments [20]. There is so far no existing platform or KG (in whatever form) linking and provisioning cross-border and cross-language procurement and company data allowing advanced decision making, analytics, and visualisation. 3 Data Sets The content of our KG is based on the procurement and company data that is provided by two main data providers extracting and aggregating data from multiple sources. The first one is OpenOpps17, which is sourcing procurement data primarily from the Tenders Electronic Daily (TED)18 data feed and from the procurement transparency initiatives of individual countries. TED is dedicated to European public procurement and publishes 520 thousand procurement notices a year. The second provider is OpenCorporates19, which is collecting company data from national company registers and other regulatory sources. OpenOpps is the largest data source of European tenders and contracts, while OpenCorporates is the largest open database of companies in the world. Both OpenOpps and OpenCorporates gather relevant data using a range of tools, including processing API calls and Web scraping and data extraction. Regarding procurement data, in the context of this work, OpenOpps provides gathered, extracted, pre-processed, and normalised data from hundreds of data sources completely openly through an API that can be used for research purposes. OpenOpps currently handles 685 data sources, with 569 of these being from Europe. This totals over 3 million documents dating back to 2010. All of the data for OpenOpps is gathered using a series of over 400 different scripts configured to collect data from each source. Each script is triggered daily and runs to gather all of the documents published in the last twenty-four hours. Each script is deployed on a monitored platform, giving the ability to check which scripts have failed, or which sources have published fewer than expected. Data is collected in the raw form and then mapped to the OCDS format after being cleansed. Where necessary, the data is processed, e.g., splitting single records into several fields, to comply with the data standard. Regarding company data, OpenCorporates provides 14 https://www.xbrl.org 15 https://www.w3.org/TR/vocab-org 16 https://joinup.ec.europa.eu/solution/e-government-core-vocabularies 17 https://openopps.com 18 https://ted.europa.eu 19 https://opencorporates.com Enhancing Public Procurement through an Integrated Knowledge Graph 5 more than 140 million company records from a large number of jurisdictions20. OpenCorporates also pre-processes and normalises data collected, maps collected data to its own data model, and makes data available through an API. The data collected from OpenOpps and OpenCorporates is openly available under the Open Database License (ODbl)21. It is available on GitHub22 in JSON format and is updated on a monthly basis. The data is also made available through Zenodo23 with a digital object identifier (DOI) [25]. 4 Knowledge Graph Construction The KG construction process includes reconciling and linking the two aforemen- tioned and originally disconnected data sets, and mapping and translating them into Linked Data with respect to an ontology network [23]. 4.1 Ontology Network We developed two ontologies, one for representing procurement data and one for company data. We used common techniques recommended by well established ontology development methods [17, 6]. A bottom-up approach was used, including identifying the scope and user group of the ontology, requirements, and ontological and non-ontological resources. In general, we address suppliers, buyers, data journalists, data analysts, control authorities and regular citizens to explore and understand how public procurement decisions affect economic development, efficiencies, competitiveness, and supply chains. This includes providing better access to public tenders; spotting trends in spending and supplier management; identifying areas for cost cuts; and producing advanced analytics. Regarding procurement data, we developed an ontology based on OCDS [24] – a relevant data model getting important traction worldwide, used for representing our underlying procurement data. The OCDS’ data model is organised around the concept of a contracting process, which gathers all the relevant information associated with a single initiation process in a structured form. Phases of this process include mainly planning, tender, award, contract, and implementation. An OCDS document may be one of two kinds: a release or a record. A release is basically associated to an event in the lifetime of a contracting process and presents related information, while a record compiles all the known information about a contracting process. A contracting process may have many releases associated but only one record. We went through the reference specification of OCDS release and interpreted each of the sections and extensions (i.e., structured and unstructured). In total, there are currently 25 classes, 69 object properties, and 81 datatype properties created from the four main OCDS sections and 11 extensions. The core classes are ContractingProcess, Plan, Tender, Award, and Contract. A 20 https://opencorporates.com/registers 21 https://opendatacommons.org/licenses/odbl 22 https://github.com/TBFY/data-sources 23 https://zenodo.org 6 A. Soylu et al. contracting process may have one planning and one tender stage. Each tender may have multiple awards issued, while there may be only one contract issued for each award. Other ontology classes include Item, Lot, Bid, Organisation, and Transaction. We reused terms from external vocabularies and ontologies where appropriate. These include Dublin Core24, FOAF25, Schema.org26, SKOS27, and the W3C Organisation ontology 28. The OCDS ontology is available on GitHub in two versions29: one with the core OCDS terms and another with the extensions. Regarding company data, one of the main resources used during the ontol- ogy development was data models provided by four company data providers: OpenCorporates, SpazioDati30, Brønnøysund Register Centre31, and Ontotext32. The data supplied by these data providers originally came from both official sources and unofficial sources. The need for harmonising and integrating data sets was a guiding factor for the ontology development process, since data sets have different sets of attributes and different representations with similar semantics. The resulting ontology, called euBussinessGraph ontology [20], is composed of 20 classes, 33 object properties, and 56 data properties allowing us to represent basic company-related data. The ontology covers registered organisations (i.e., compa- nies that are registered as legal entities), identifier systems (i.e., a company can have several identifiers), officers (i.e., associated officers and their roles), and data sets (i.e., capturing information about data sets that are offered by company data providers). Registered organisations are the main entities for which information is captured in the ontology. The main classes include RegisteredOrganisation, Identifier, IdentifierSystem, Person, and Dataset. Three types of classifi- cations are defined in the ontology for representing the company type, company status, and company activity. These are modelled as SKOS concept schemes. Some of the other external vocabularies and ontologies used are W3C Organi- sation ontology, W3C Registered Organisation Vocabulary (RegOrg)33, SKOS, Schema.org, and Asset Description Metadata Schema (ADMS)34. The ontology, data sets and some examples are released as open source on GitHub35. 4.2 Data Ingestion The essence of ingestion process includes extracting procurement and company data from the the providers, matching suppliers appearing in procurement data 24 http://dublincore.org 25 http://xmlns.com/foaf/spec 26 https://schema.org 27 https://www.w3.org/2004/02/skos 28 https://www.w3.org/TR/vocab-org 29 https://github.com/TBFY/ocds-ontology/tree/master/model 30 http://spaziodati.eu 31 http://www.brreg.no 32 https://www.ontotext.com 33 https://www.w3.org/TR/vocab-regorg 34 https://www.w3.org/TR/vocab-adms 35 https://github.com/euBusinessGraph/eubg-data Enhancing Public Procurement through an Integrated Knowledge Graph 7 1. Download procurement  data 2. Reconcile  supplier  data 3. Enrich  JSON  data 4. Convert  JSON to  XML 5. Map XML  data to  RDF  6. Publish  RDF to  database 1-2 minutes  per day 0.5–1.5 hours  per day ~1 minute  per day <30 seconds  per day <1 hour per day ~2 minutes  per day Fig. 1. The daily data ingestion process for the KG – on average 2500 OCDS releases are processed and 2400 suppliers (i.e., companies) are looked up per day. against company data (i.e., reconciliation), and translating the data sets into RDF using RML36. The daily process is composed of the following steps (see Figure 1): (1) Download procurement data: Downloads procurement data from the OpenOpps OCDS API37 as JSON data files. (2) Reconcile suppliers: Matches supplier records in awards using the Open- Corporates Reconciliation API38. The matching company data is downloaded using the OpenCorporates Company API39 as JSON data files. (3) Enrich downloaded JSON data: Enriches the JSON data files downloaded in steps 1 and 2, e.g., adding new properties to support the mapping to RDF (e.g., fixing missing identifiers). (4) Convert JSON to XML: Converts the JSON data files from step 3 into corresponding XML data files. Due to limitations in JSONPath, i.e., lack of operations for accessing parent or sibling nodes from a given node, we prefer to use XPath as the query language in RML. (5) Map XML data to RDF: Runs RML Mapper on the enriched XML data files from step 4 and produces N-Triples files. (6) Store and publish RDF: Stores the RDF (N-Triples) files from step 5 to Apache Jena Fuseki and Apache Jena TBD. We have been running the ingestion pipeline on a powerful server (see some performance metrics in Figure 1), with the following hardware specifications: 2x Xeon Gold 6126 (12 Cores, 2.4 GHz, HT) CPU, 512 GB main memory, 1x NVIDIA Tesla K40c GPU, and 15 TB HDD RAID10 & 800 GB SSD storage. Python was used as the primary scripting language, RMLMapper was used as the mapping tool to generate RDF, and finally Apache Jena Fuseki & TDB was chosen as the SPARQL engine and triple store. The Python scripts operate on files (output and input) and services have been dockerised using Docker and made available on Docker Hub40 to ease deployment. All development work and 36 https://rml.io 37 https://openopps.com/api/tbfy/ocds 38 https://api.opencorporates.com/documentation/Open-Refine-Reconciliation-API 39 https://api.opencorporates.com/documentation/API-Reference 40 https://hub.docker.com/r/tbfy/kg-ingestion-service 8 A. Soylu et al. results towards the creation of the knowledge graph are published and maintained as open source software on GitHub41. The data dump of the knowledge graph, including around 100M statements for 2019, is available on Zenodo [25]. 5 Knowledge Graph Provisioning We developed a platform and core API services for the KG ingestion and provi- sioning, using recent Linked Data technologies and REST API design practices. 5.1 Platform Architecture The architecture for our platform was developed following the state-of-the-art principles in software development, considering a low decoupling among all the software components, and the use of REST and Linked Data principles for data provisioning. API Gateway Reconciliation API OC API OO API SPARQL API Core API OpenOpps (OO) procurement database OpenCorporates (OC) company database Distributed  data sets RDF Triple  store Document  store owl:sameAs rdf:seeAlso JSON JSON KG ingestion ETL Cross-lingual Search API Fig. 2. The high-level architecture for KG ingestion and provisioning. Figure 2 provides a general high-level overview of the current architecture. On the left-hand side, we include the ETL processes that are being used to incorporate data sources into the KG, as discussed in Section 4. On the right-hand side we provide an overview of the main data storage mechanisms, including a triple store for the generated RDF-based data and a document store for the documents associated to public procurement (tender notices, award notices, etc.), whose URLs are accessible via specific properties of the KG (using rdfs:seeAlso). Furthermore, for those specific cases where a URI is also available in the original data sources (from OpenOpps and OpenCorporates), such URI is provided in 41 https://github.com/TBFY/knowledge-graph Enhancing Public Procurement through an Integrated Knowledge Graph 9 the KG using a statement with owl:sameAs, so as to facilitate navigation across these different data sources. This would allow our data providers to provide additional information about tenders or companies with a different license or access rights, if considered relevant (e.g., for commercial use). The KG is accessible via a core REST API. Our API catalogue is mostly focused on providing access mechanisms to those who want to make use of the knowledge graph, with a special focus on software developers. Therefore, they are mostly focused on providing access to the KG through the HTTP GET verb and the API catalogue is organised around the main entities that are relevant for public procurement, as discussed in Section 4, such as contracting processes, awards, and contracts. Since the KG is stored as RDF in a triple store, there is also a SPARQL endpoint42 for executing ad-hoc queries. Finally, there is a cross-lingual search API for searching across documents in various languages and an API Gateway providing a single-entry point to the APIs provided by the platform. 5.2 Core API The core API was built using the R4R tool43. This tool is based on Velocity templates44 and allows specifying how the REST API will look like and configure it by means of SPARQL queries, similarly to what has been proposed in other state of the art tools like BASIL (Building Apis SImpLy) [7] or GRLC [13]. Beyond exposing URIs for the resources available in the KG, it also allows including authentication and authorisation, pagination, establishing sorting criteria over specific properties, nesting resources, and other typical functionalities normally available in REST APIs. The current implementation only returns JSON objects for the API calls and will be extended in the future to provide additional content negotiation capabilities and formats (JSON-LD, Turtle, HTML), which are common in Linked Data enabled APIs. There is an online documentation45, which is continuously updated. The core resources derived from the OCDS ontology are: (i) ContractingProcess, (ii) Award, (iii) Contract, (iv) Tender, and (v) Organisation. Table 1 provides the details of the resources provided by our REST API in relation to the OCDS ontology. For all these resources, there is a possibility of paginating (e.g., GET /award?size=5&offset=1), sorting (e.g., GET /contract?sort=-startDate), and filtering (e.g., by the title of the award: GET /award?status=active). 6 Knowledge Graph in Use We implemented a number of real-life use cases on the KG: anomaly detection and cross-lingual document search. 42 http://yasgui.tbfy.eu 43 https://github.com/TBFY/r4r 44 https://velocity.apache.org 45 https://github.com/TBFY/knowledge-graph-API/wiki 10 A. Soylu et al. Table 1. The core API developed around the main resources. /contractingProcess Gets a list of contracting processes /contractingProcess/{id} Finds a contracting process by ID /contractingProcess/{id}/award Awards of a contracting process to return /contractingProcess/{id}/buyer Buyers of a contractingProcess to return /contractingProcess/{id}/contract Contracts of a contracting process to return /contractingProcess/{id}/tender Tender of a contracting process to return /award Gets a list of awards /award/{id} Finds an award by ID /award/{id}/amendment Amendments of an award to return /award/{id}/document Documents of an award to return /award/{id}/item Items of an award to return /award/{id}/supplier Suppliers of an award to return /contract Gets a list of contracts /contract/{id} Finds a contract by ID /contract/{id}/amendment Amendments of a contract to return /contract/{id}/buyer Buyers of a contract to return /contract/{id}/document Documents of a contract to return /contract/{id}/item Items of a contract to return /tender Gets a list of tenders /tender/{id} Finds a tender by ID /tender/{id}/contractingProcess Contracting processes of a tender to return /tender/{id}/document Documents of a tender to return /tender/{id}/item Items of a tender to return /organisation Gets a list of organisations /organisation/{id} Finds an organisation by ID /organisation/{id}/award Awards of an organisation to return /organisation/{id}/contractingProcess Contracting processes of an organisation to return 6.1 Anomaly Detection Public procurement is particularly susceptible to corruption, which can impede economic development, create inefficiencies, and reduce competitiveness. At the same time, manually analysing a large volume of procurement cases for detecting possible frauds is not feasible. In this respect, using ML techniques for identifying patterns and anomalies, such as fraudulent behaviour or monopolies, in procurement processes and networks across data sets produced independently, is highly relevant [5]. For example, by building a network of entities (individuals, companies, governmental institutions, etc.) connected through public procurement events, one can discover exceptional cases as well as large and systematic patterns standing out from the norm, whether they represent examples of good public procurement practice or possible cases of corruption. We applied several ML techniques, i.e., supervised, unsupervised, and sta- tistical, on top of the Slovenian public procurement data in the KG to identify patterns and anomalies. First, clustering was used for anomaly detection (see Figure 3 (a)), since one could quickly spot deviations with this approach. Every tender was transformed into a feature vector. After determining the optimal Enhancing Public Procurement through an Integrated Knowledge Graph 11 (a) (c) (b) Fig. 3. (a) Anomaly detection in public procurement data with k-Means analysis. (b) The decision tree model for identifying successful tenders. (c) A graph showing interdependence between tender value and number of employees of bidder. number of clusters, public procurement data were clustered with the K-means algorithm. Vectors deviating most from their centroids are identified and ordered by the deviation value. The approach was used to identify tenders with highest deviations. For example, among others, our method identified public procurement cases with an unusually high tender value. This approach gives a hint on data that “stick out” and are worth of more in-depth scrutiny. Second, supervised analysis implemented in our platform is based on a decision tree (see Figure 3 (b)). We enabled users to select parameters by their own choice (for instance buyer size, bidder municipality, and the depth of decision tree model), and thus enabling users to compare the importance of subsets of various parameters contributing to the success of public tenders. The success definition is up to decision makers. Currently, we are defining success as every tender that has received more than one bid. According to our preliminary analysis, a tender is successful – i.e. there will be competition (more than one bid) - if public institution who opened the 12 A. Soylu et al. tender is small (less than 1375 employees) and if bidding is done in group. Third, statistical approach was used to deal with various ratios between pre-selected parameters (see Figure 3 (c)). Currently, the ratio between the tender value and the estimated number of employees for a bidder is examined. Bidders are then sorted by their ratio value and every bidder turned into a point: the x value is a consecutive number (in sorted list) and the y value is the ration. We developed a visual presentation of interdependence of tender value and the number of employees. The graph shows deviating behaviour at the beginning as well as at the end of the list. On the upper left corner of the graph, we can see big companies with a high number of employees that won small tenders, and on the bottom right corner, there are companies with a small number of employees that won big tenders. We implemented a system through using these techniques and made it available online46. The system developed is capable of processing tens of millions of records and allows detecting a large class of anomalies in automatic mode or in exploratory mode (with human-machine interaction). 6.2 Cross-lingual Document Search Procurement processes are not only creating structured data, but also constantly creating additional documents (tender specifications, contract clauses, etc.). These are commonly published in the official language of the corresponding public administrations. Only some of these, for instance those published in TED, are multilingual, but the documents in the local language are typically longer and much more detailed than their translations into other languages. A civil servant working at a public administration on a contracting process may be interested in understanding how other public administrations in the same country or in different countries (and with different languages) have worked on similar contexts. Examples may include finding organisations related to a particular procurement process, or search for tenders related to given procurement text. We worked on an added-value service47 in order to provide support to these types of users, with the possibility of finding documents that are similar to a given one independently of the language in which it is made available. We also generated a Jupyter notebook with some representative examples, so as to facilitate its use48. This service is based on the use of unsupervised probabilistic topic models, based on cross-lingual labels from sets of cognitive synonyms (synsets) to establish relations between language-specific topics [2]. Documents are represented as data points in a low-dimensional latent space created by probabilistic topic models for each language separately (Figure 4). Topics are then described by cross-lingual labels created from the list of concepts retrieved from the Open Multilingual WordNet. Each word is queried to retrieve its synsets. The final set of synsets for a topic is the union of the synsets from the individual top-words of a topic (top5 based on empirical evidences). 46 http://tbfy.ijs.si 47 http://tbfy.librairy.linkeddata.es/search-api 48 http://bit.ly/tbfy-search-demo Enhancing Public Procurement through an Integrated Knowledge Graph 13 Fig. 4. (a) Documents are represented in a unique space that relies on the latent layer of cross-lingual topics obtained by LDA and hash functions through hierarchies of synsets. (b) Theme-aligned topics described by top 5 words based on EUROVOC annotations. The JRC-Acquis dataset49 was used to build the model relating the documents. It is a collection of legislative texts written in 23 languages that have been manually classified into subject domains according to the EUROVOC 50 thesaurus. The English, Spanish, French, Italian and Portuguese editions (about 20.000 documents per edition) of the corpora were used for each language-specific model. The EUROVOC taxonomy was pre-processed to satisfy the topic independence assumption of probabilistic topic models, by using hierarchical relations. The initial 7.193 concepts from 21 domain areas such as politics, law or economics were reduced to 452 categories, that are independent and can be used to train the topic models. Documents were pre-processed (Part-of-Speech filtering and lemmatized format) by the librAIry NLP51 service and projected into the previously created 49 https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis 50 http://eurovoc.europa.eu 51 http://librairy.linkeddata.es/nlp 14 A. Soylu et al. topic space. The method is evaluated in several document retrieval tasks by using a set of documents previously tagged with EUROVOC categories. Results are quite promising across languages with a performance close to 0.8 in terms of accuracy, although a better performance is achieved with English texts (as expected from the quality of the tools in those languages). 7 Lessons Learned There are plenty of lessons learned in the context of this work, which may be applicable to the construction of other KGs in similar or different domains. First, we provide a non-exhaustive list of major takeaways related to the whole process: (i) The KG enabled easier and advanced analytics, which was otherwise not possible, by connecting companies (i.e., suppliers) appearing in the procure- ment data set to companies in company data set. However, getting and pre-processing the data (e.g., data curation) was a major and time-consuming task, requiring attention from national and EU-wide data providers. (ii) The existing Semantic Web technologies and tools scaled well for ingesting and provisioning large amounts of data and RESTful approach was useful for bringing the Linked Data to non-Semantic Web application developers. However, more support is required such as visual editors for creating mapping definitions and specifying data transformations. (iii) The process of building a high-quality KG that can be used extensively by users would be clearly improved if all data sources were providing their procurement data in a more structured manner. Data quality problems are still a relevant issue, as described in the followings, and reduce the result quality of ML processes such as anomaly detection and reconciliation. (iv) There are still many documents associated to the procurement processes that are provided as PDFs (in some cases even scanned PDFs). Providing all documents in the form of raw texts as well, would simplify the processing that needs to be done, and would allow applying more easily the techniques like the ones described for cross-lingual search. (v) Data providers should also aim at publishing the information of all types of contracting processes that they are handling, independently of their size. Cur- rently, due to many types of regulations across countries, not all contracting processes (especially the smallest ones) are published. We also faced a high number of data quality issues, even though there are mandates in place for buyers to provide correct data. This particularly applies to procurement data sources. These data quality issues could be classified as: (i) Missing data: It is frequent that data is missing. Among others, the least frequently completed field in the tender and contracting data is the value field, it is usually completed in less than 10% of tender notices. One item of data that is particularly important to procurement transparency is the reference data required to link a contract award to a tender notice (very common in Enhancing Public Procurement through an Integrated Knowledge Graph 15 the TED data). We found that just 9% of award notices had provided a clear link between tenders and contracts. Subsequently, the majority of contract award notices had been orphaned and there was no link to the source tenders. (ii) Duplicate data: Publishers frequently publish to multiple sources in order to meet the legal requirements of their host country and that of the European Union. This means that all over-threshold tenders are available at least twice. The task of managing duplicates is not always simple, it is common for differ- ent publishing platforms to have different data schemas and interoperability between schemas, is not guaranteed. (iii) Poorly formed data: Sources are frequently providing malformed data or data that cannot be reasonably parsed by code. The tender and contract value field can often include string values rather than numbers (same goes for the dates). Across the sources, approach to using character delimiters in value data is frequently heterogeneous, with different nationalities using different delimiters to separate numbers and to indicate decimals. (iv) Erroneous data: Structured data such as numeric and date records are frequently a problem. Buyers often submit zero value entries in order to comply with the mandate and the lack of validation on date related data has allowed buyers to record inconsistent date data. There are some contracts where the date of publication exceeds the end date of the contract or the start date of the contract is greater than the end date of the contract. (v) Absent data fields: In some cases, the sources lack core pieces of information, for instance, there is no value field in a number of European sources. A large number of sites also fail to publish the currency of their monetary values. In all cases, if a publisher sought to add the additional information, such as a different currency, there would be no capacity in the system to provide the information required in a structured form. Most of these problems can be resolved through the use of standards and validation at the point of data entry. Requiring buyers to publish records to a standard would, in turn, require the platform providers to both mandate the field format and validate data entries. The usage of an ontology network for the development of the KG allowed us to inform public administrations willing to provide data on the minimum set of data items that are needed, and some of them are already adapting their information systems for this purpose [9]. Acknowledgements. The work reported in this paper is partly funded by EC H2020 TheyBuyForYou (780247) and euBusinessGraph (grant 732003) projects. References 1. Alvarez-Rodr´ıguez, J.M., et al.: New trends on e-Procurement applying semantic technologies. Computers in Industry 65(5), 797–799 (2014) 2. Badenes-Olmedo, C., et al.: Scalable Cross-lingual Similarity through language- specific Concept Hierarchies. In: Proc. of K-CAP 2019. pp. 147–153 (2019) "
"","Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Bernardos1, Patricia Mart´ın-Chozas1, Alba Fern´andez-Izquierdo1, Elvira Amador-Dom´ınguez1, Paola Espinoza-Arias1, Luis Pozo1, Edna Ruckhaus1, Esteban Gonz´alez-Guardia1, Raquel Cedazo2, Beatriz L´opez-Centeno3, and Oscar Corcho1 1 Ontology Engineering Group, Universidad Polit´ecnica de Madrid, Boadilla del Monte, Spain 2 High School of Technical Industrial and Design Engineering, Universidad Polit´ecnica de Madrid, Madrid, Spain 3 Subdirecci´on General de Farmacia y Productos Sanitarios, Servicio Madrile˜no de Salud, Madrid, Spain Abstract. As a result of the COVID-19 epidemic peaks worldwide, the provision of some medicines for patients became a major challenge for practitioners and hospital pharmacists, due to the increased demand. In the absence of sufficient medication, disused drugs were employed or the doses of those available needed to be modified. Some evidences for the use of alternative drugs can be found in the existing scientific litera- ture. However, exploiting such large corpus of documents in an efficient manner is not easy, since the same active substances may be present in medicines under different brand names and relationships between medicines are sometimes only presented in an implicit manner in those texts. In Drugs4Covid we have processed and annotated the more than 60K articles and 2M paragraphs of the CORD-19 corpus of research pa- pers around COVID-19, SARS, and other related coronaviruses, in order to create an open catalogue of drugs that may be related to COVID-19. Our work uses natural language processing and Semantic Web technolo- gies to enable a drug-oriented exploration of large collections of scientific literature. Drugs and diseases were identified according to the ATC clas- sification and MeSH categories respectively. Results are publicly available through a drug browser, a keyword-guided text explorer, and a KG. Keywords: Ontology-based technologies · NLP · Bio-annotations · Drugs- catalogue · Knowledge Graph · COVID-19. 1 Introduction During March and April 2020, news on the scarcity of medicines to treat COVID- 19 have appeared in many countries around the world. Due to the increasing demand during the peaks of infections worldwide and other problems associated to logistics, doctors and pharmacists were struggling to treat their patients. We 2 Badenes-Olmedo et al. bioannotator ATC MeSH CORD-19 Documents vector space modeller space partitioner TOPIC MODEL W2V TF/IDF Search  engine Drugs4Covid Ontology KG KG KG Drugs4Covid KG text, drugs, diseases Indexes text, drugs, diseases Groups Entries Process External resource Service Internal resource KG  construction bioannotator ATC MeSH CORD-19 Documents vector space modeller space partitioner TOPIC MODEL W2V TF/IDF Search  engine Drugs4Covid Ontology KG KG KG Drugs4Covid KG text, drugs, diseases Indexes text, drugs, diseases Groups Process External resource S Service Internal resource KG  construction Drugs4Covid S S bioannotator ATC MeSH CORD-19 Documents vector space modeller space partitioner TOPIC MODEL W2V TF/IDF Search  engine Drugs4Covid Vocabulary KG KG KG Drugs4Covid KG text, drugs, diseases Indexes text, drugs, diseases Groups Process External resource S Service Internal resource KG  construction Drugs4Covid S S Fig. 1: Creation workflow of a search engine and a knowledge graph through annotations created from the CORD-19 dataset. confirmed this situation in Madrid, a region heavily affected by COVID-19, after a series of interviews with doctors, pharmacists and other people from regional Health Services. In the absence of sufficient medication for such a large amount of patients, practitioners started applying disused drugs that were available in hospital pharmacies and modifying the usual doses of those available. However, identifying which drugs can be used as replacements for others to treat this disease and its associated symptoms is a difficult challenge. New experiments and results are continually being published, and people in charge of clinical protocols cannot keep up to date with all of them [5]. This situation calls for solutions that help health care providers and researchers easily extract such knowledge from the enourmous scientific corpus that is being created. Several initiatives have emerged to bring together scientific publications in this domain. For instance, the dataset maintained by the EU COVID-19 data portal4, or those published by Humandata5, focused on COVID-19 cases around the world. Similarly, other research groups and universities have also published valuable datasets, such as the one by The Open COVID-19 Data Working Group6, The DisGeNET COVID-19 dataset or the data gathered by John Hop- kins University7, among others. The Allen Institute for Artificial Intelligence created a continuously growing corpus8 with all publicly available COVID-19 and coronavirus-related research (e.g. SARS, MERS, etc.) in the last fifty years (Fig. 2). The COVID-19 Open Research Dataset (CORD-19) [21] is a collection of documents that can be used as a source of information to extract knowledge related to the disease. At the time of submitting this paper, it is composed of 23,428 open access articles from 4 https://www.covid19dataportal.org 5 https://data.humdata.org/event/covid-19 6 https://github.com/beoutbreakprepared/nCoV2019 7 https://github.com/CSSEGISandData/COVID-19 8 https://www.semanticscholar.org/cord19 Drugs4Covid 3 Fig. 2: Distribution of scientific publications on SARS, COVID-19 and other coronaviruses per year in CORD-19. PubMed Central, 35,240 research articles from a corpus maintained by the World Health Organization (WHO), and 1,945 bioRxiv and medRxiv pre-prints. Given that volume, natural language processing and knowledge extraction technologies can provide doctors and medical researchers with tools that make their work easier by structuring the information contained within the papers. Our goal is to automatically extract, organize and publish the knowledge from the scientific literature needed to answer common questions posed by domain experts: What are the effects of using chloroquine and hydroxychloroquine to treat COVID-19 patients? Have drugs that combine immunosuppressive and an- timalarial activity with macrolide antibiotics been used? In which experiments have mefloquine and azithromycin been used and related to which diseases? We processed more than 60k medical publications to identify and create relations amongst drugs and diseases. The resulting tools help quickly create domain-specific search engines and a knowledge graph (KG) (Figure 1) over any corpus of scientific documents. Such techniques can be reused for other similar crisis in the future, and in any other situation where these tools could be valuable. All the work presented in this paper is available in a GitHub repository9. 2 Related Work Due to the fast evolution of the SARS-COV-2, worldwide scientists have been working as fast as possible to provide tools that help health professionals treat patients during this pandemic. Most of the ongoing work has been shared inside research communities (e.g. W3C Semantic Web and W3C Healthcare and Life Sciences), in hackathons (e.g. the European Commission hosted EUvsVirus10, VenceAlVirus11 in the region of Madrid), and in preliminary reports in blogs and archival services. Multiple datasets12 have been provided for finding new insights about the novel coronavirus. Semantic Web technologies have been traditionally used to publish many data sources in the biomedical domain such as Bio2RDF13 that provides a net- 9 https://github.com/oeg-upm/drugs4covid19 10 https://www.euvsvirus.org 11 https://vencealvirus.org 12 https://data.world/resources/coronavirus/ 13 https://bio2rdf.org/ 4 Badenes-Olmedo et al. work of linked data for the biomedical domain, DisGeNET RDF14 with diseases and the genes related to them, SNOMED CT15 on clinical healthcare terminol- ogy, etc. Some of this earlier work has been used by language technologies to add semantic annotations to scientific literature. The CORD19-NEKG16 identi- fied and disambiguated named entities in CORD-19 using DBpedia Spotlight17, entity-fishing18 and the NCBO BioPortal annotator19. Similarly, the CORD- 19-on-FHIR20 project introduced annotations on conditions, medications, and procedures into a KG based on CORD-19. Knowledge graphs have also been created. The Knowledge4COVID-1921 project at the EUvsVirus Hackathon created a knowledge graph that integrates knowl- edge from scientific literature and databases, and uses predictive models to derive interactions among drugs used in the treatment of COVID-19. Other approaches are oriented to a collective generation of knowledge. The KG-COVID-19 Hub project22 is an initiative that seeks to build collaboratively a KG hub for COVID- 19 so that developers can ingest new sources into this hub. The Covid Graph project23 builds a KG from publications about COVID-19 cases and molecular data and a similar initiative is also proposed in Linked Data Fragments24. Others have combined data from various sources of interest to focus the knowledge in a particular domain. SIB COVID-1925 is specialized on genetic information, and COVID-19 by STKO Lab26 handles events of recorded cases and suspensions of commercial airline routes. None of these approaches provide yet sufficient information on how that knowledge is generated. When two drugs are related through an interaction in the knowledge graph, the information used to induce that kind of relation is very relevant for its potential users. We believe that conclusions are as important as the data that led to them during the learning process. Hence, in this paper we describe the process followed to create a knowledge graph that, together with the diseases and drugs identified in the CORD-19 corpus, links to the texts (i.e paragraphs, sentences or even full articles) that support the relationships. 14 https://www.disgenet.org/rdf 15 http://snomed.sparklingideas.co.uk/home 16 https://github.com/Wimmics/cord19-nekg 17 https://www.dbpedia-spotlight.org/ 18 https://github.com/kermitt2/entity-fishing 19 http://bioportal.bioontology.org/annotatorplus 20 https://github.com/fhircat/CORD-19-on-FHIR 21 https://devpost.com/software/covid-19-kg 22 https://github.com/Knowledge-Graph-Hub/kg-covid-19 23 https://live.yworks.com/covidgraph 24 https://query-covid19.linkeddatafragments.org 25 https://covid-19-sparql.expasy.org/ 26 https://covid.geog.ucsb.edu/ Drugs4Covid 5 3 Drug and Disease Annotations on Scientific Texts Searching for similar drugs and exploring major diseases covered by different publications are key activities when browsing medical papers. This manual know- ledge-intensive task becomes less tedious and even leads to unexpected relevant findings when applying unsupervised algorithms to help researchers. In this work we have addressed the problem of generating drug- and disease- based annotations for the documents within a large collection of research articles (Section. 3.2). We have created vectorial representations of each of them using state-of-the-art techniques based on word embeddings and probabilistic topic models (Section. 4). We have related each type of element with those similar and suggested drugs that may be considered as replacements for others. More than 60k scientific papers were analyzed from the CORD-19 dataset. Around 5M sentences and 2M paragraphs were annotated with the drugs and diseases mentioned in them. A total of 6,400 diseases and 2,400 drugs were characterized to enrich the searches on the corpus and the knowledge graph. 3.1 Pharmacological classes used for annotation Textual searches on a collection like CORD-19 bring accuracy issues since the ter- minology associated with drugs and diseases varies between countries. A search based on an active substance should lead to the drugs that contain it. Such drugs can be traded under different brand names among countries,hence appear- ing with different names in different papers. For example, the active substance Enalapril/Hydrochlorothiazide is distributed in Spain under the trade names Co- Renitec, Crinoretic or Dabond Plus, while its trade name in Portugal is Renidur, in Denmark it is distributed as Corodil and in Hungary its name is Ednyt HCT. Thus, we need to unify drugs and diseases with codes that abstract them from their particular names or textual representations. We have considered the Anatomical Therapeutic Chemical (ATC) classifica- tion system27 to annotate drugs. It groups active substances according to the organ or system on which they act and their therapeutic, pharmacological, and chemical properties. Drugs are classified into groups at five different levels. The first one corresponds to main groups, the second one to pharmacological or ther- apeutic subgroups, the third and the fourth one are chemical-pharmacological- therapeutic subgroups and the last one is the chemical substance. Diseases, have been annotated with the Medical Subject Headings (MeSH) vocabulary, used for indexing, cataloging, and searching biomedical information by the MEDLINE/PubMed article database, the NLM´s books catalog, and ClinicalTrials.gov among others28. This vocabulary is organized in a hierarchical, numbered tree structure which enables browsing from broader to narrower topics. 27 https://www.whocc.no/atc ddd index 28 https://www.nlm.nih.gov/mesh/meshhome.html 6 Badenes-Olmedo et al. Table 1: Top5 words related to Chloroquine (P01BA01), Hydroxychloroquine (P01BA02) and Lopinavir/Ritonavir (J05AR10) according to the PTM created from the active substances discovered in the CORD-19 corpus. Top-Word P01BA01 P01BA02 J05AR10 1 chloroquine chloroquine mer 2 chikv hydroxychloroquine camel 3 autophagy glycosylation cov 4 endosome analog mild 5 cathepsin rheumatoid lopinavir/ritonavir 3.2 Drug and disease annotation techniques Common entities identified in medical literature are diseases, drugs and chemi- cal names among others [19, 10, 18, 6]. These named entities can also be used to discover relations between them. We created a pipeline to prepare the textual content from the CORD-19 dataset [21] and index it together with some addi- tional information in a document-oriented database. Our code is based on the harvester service of librAIry [2], which provides a high-performance infrastruc- ture for text mining tasks. For this study, we focused on the following fields: – id: unique identifier for each research article. – name: publication title. – abstract: brief summary made by the author. – text: full textual content of the article. – url: online resource available from Allen AI repository. Named Entity Recognition has been performed combining different state- of-the-art methods. SciSpaCy [13], a Python library with trained models from different corpora, and CliNER [4] were tested to identify diseases, trials and treatments from texts. However, these models do not unify the identification of drugs and diseases described in the above classifications (Section. 3.1). Some- times they are identified as different drugs or diseases, since they are referred by different names, when in fact they correspond to the same element. To adjust this behavior we have developed a new service29 using the MeSH vocabulary and the ATC codes as gazetteers of terms respectively. 4 Representational Models for Drugs and Diseases In order to establish relations among drugs or diseases, so as to improve searches on the corpus, we have created representational models capable of abstracting the entities from their textual forms and relating them to each other. 4.1 Identifying Related Texts Some text mining algorithms represent documents in a common feature space that abstracts away from the specific sequence of words used in each of them. 29 https://github.com/librairy/bio-nlp Drugs4Covid 7 Fig. 3: Documents described by a PTM from the active substances (ATC-code level5) in the CORD-19 corpus and organized by similar topic distributions. That is, they treat documents as bags-of-words. With appropriate representa- tions, they ease the analysis of relationships between documents, even when written using different vocabularies. – Representational Model: We have used topic models, which are widely used to uncover the latent semantic structure from text corpora. In particu- lar, Probabilistic Topic Models (PTM) represent documents as a mixture of topics, where topics are probability distributions over words. This eases the explotation of large document collections, since documents are mapped into a low-dimensional latent space (i.e they are described by a small number of topics). It also abstracts the representation of the documents from the words themselves. Topics emerge as density distributions on the vocabulary to define the dimensions of the vectors that represent the documents. We used librAIry[2] to create a PTM that combines the papers with the an- notated drugs. Each topic matches an active substance (Table 1), avoiding distinctions between different drug names by country. The model is created and distributed as a Rest-API30 – Similarity Metric: Due to low storage cost and fast retrieval speed, hashing is a popular solution for the calculation of approximate nearest neighbors in the probability simplex space created from topic models. We opted for a density-based hierarchical hashing method [3] to compare texts contained in CORD-19. This approach has proven to obtain high-precision results and can accommodate additional query restrictions. – Space Partitioning: Since documents are compared from sets of topics dis- tributed in levels according to their relevance, the representational space can 30 http://librairy.linkeddata.es/covid19-model 8 Badenes-Olmedo et al. (a) Silhouette analysis for 120 clusters (b) Two-dimensional drug clusters Fig. 4: Validation of the drug clusters created. be organized in k-d trees. The most discriminating topics and levels define the branches of the tree and each text is organized among them according to its topic distributions. We create a document browser31 to navigate through the corpus based on active substances and content-based relations (Fig. 3). 4.2 Finding Related Drugs Drugs were described by the diseases with which they are mentioned in texts (e.g sentence, paragraph, or full-text). Paragraphs showed an appropriate balance between generality and particularity to describe drugs, since their content is more strongly related than the full-text, and less incomplete than the sentences. – Representational Model: Drugs were represented in a vector space model as documents whose words are the diseases. A drugs-diseases matrix was created with the times a disease appears in the same paragraph than a drug. Then, we used Term Frequency-Inverse Document Frequency (TFIDF) to transform drugs into numeric vectors. Two drugs described by TFIDF will be similar if they share rare, but informative, diseases. – Similarity Metric: Cosine similarity was adopted as the measure of interest as it is well-known and frequently-used in vector space models [17]. – Space Partitioning: Similar drugs are organized into the same groups to suggest their use as replacements. Since the number of groups is unknown, we created a dendrogram based on simple linkage from their vector repre- sentations. As a result we obtained a hierarchical aggregation of drugs that suggests an optimal number of groups according to the Silhouette coefficient. Several settings were tested and finally 120 clusters offered the best perfor- mance (Fig. 4). We developed a Rest-API32 that makes use of the Annoy33 module to index drugs in a kd-tree. 31 https://librairy.github.io/covid19/explorer 32 https://librairy.linkeddata.es/bio-api/replacements?keywords=chloroquine 33 https://github.com/spotify/annoy Drugs4Covid 9 Table 2: Main terms discovered in the CORD-19 corpus CORD-19 top 25 terms T cell respiratory tract viral replication infected cell health care public health infectious disease viral RNA cell lines RNA viruses amino acid immune response epithelial cell respiratory syndrome nucleic acid immune response accute respiratory E. coli virus infection acute respiratory syndrome influenza virus gene expression viral infection immune system respiratory viruses 4.3 Finding Related Diseases In order to measure the similarity amongst diseases, a three-step procedure was followed: 1) Creation of disease-focused word embedding models, 2) Generation of a terminology representing the whole corpus, used to align the compared diseases, 3) Disease similarity calculation from the identified key terms. – Terminology Generation:The aim of this step was to discover common terminology amongst diseases. We extracted the most frequent terms from a representative sample of 100.000 paragraphs from CORD-19 corpus. Given the complex nature of the source texts (full of technical expressions, num- bers, figures, symbols) this task presented a challenge when tested with two terminology extraction methods: Rake [16] and TBXTools[20]. While the former did not solve many of the above mentioned hindrances and some ex- tracted terms were linguistically incorrect, the latter gave good results that needed only some postprocessing work (Table 2). – Representational Model: Out of the existing state-of-the-art word em- bedding models, Word2Vec [12] was selected to generate a representational model for each disease. To ensure comparability between models, the same initialization values were used, extracted from the pre-trained model BioWord- Vec [22]. Using a proper initialization not only enables direct comparability between disease models, but eases the convergence of the model. – Similarity Metric: According to their representational models, diseases were represented as vectorial models, where each word featured in the train- ing corpus found a corresponding embedding. However, not all words in the resulting vocabulary were equally explanatory of the disease, and thus com- parisons between models should not be established using the totality of the embedded words. The previously identified top25 terms (Table 2) were used as pivotal points to compare the models. Graphs can be used to visually represent each disease, considering each term as a node, and the distance between each pair of nodes as edges. Word Mover’s Distance [11] was used to measure the distance between terms, as it captures the underlying seman- tics between the resulting word representations. – Space Partitioning: Disease models were initialized using the same values, meaning that the initial embedding of each term was the same across every disease. During training, these embeddings evolve, or move throughout the vectorial space to fit the context constraints from the disease-specific train- ing corpus. Therefore, while the initial representations were equal, the final representations differ, as they also embedded specific information about the 10 Badenes-Olmedo et al. (a) COVID-19 and Malaria (b) COVID-19 and Conjunctivitis Fig. 5: Comparison of diseases based on distances among common terminology described in the embedding spaces created for each of them. disease. However, the resulting representations can still be compared. Pair- wise comparison between diseases can be established by measuring the Eu- clidean distance between the embeddings of same terms on each model. Small distances between term representations mean that they occur in a similar context, thus their final embeddings remain close. Figure 5 illustrates pair- wise comparison between three diseases. According to the obtained results, COVID-19 is closer to Malaria than to Conjunctivitis, as the cumulative distance of all pairwise term distances is lesser in that case. 5 Construction of the Drugs4Covid KG All the work described in Sections 3 and 4 allowed us to create the knowledge to facilitate the search and navigation over the corpus. This knowledge has been made available in different REST APIs, as described in those sections. To facili- tate its (re)use, we also created a knowledge graph that contains this knowledge. This section describes the development of the vocabulary and details the steps followed to build the Drugs4Covid KG by combining the drug and disease an- notations inferred from the CORD-19 corpus with external resources. 5.1 The Drugs4Covid vocabulary The vocabulary defined to support the Drugs4Covid knowledge graph generation and queries was developed following the LOT methodology [15], relying heavily in the communication with users (i.e. developers generating annotations and KG developers) and domain experts (i.e. biologists and pharmacists) throughout the ontology development process. The main goals of the vocabulary were to represent and relate some parts of the papers analyzed with biomedical concepts, as obtained with the application of language technologies. Drugs4Covid 11 mentions hasEffect scientificName: String drugName: String rdfs:seeAlso Drug Symptom Effect causedByDrug dc:abstract:  dc:source contains mentions identifier: string dc:title dc:license problem: string dc:date rdfs:seeAlso mentions identifier: string text: string activeSubstanceName: String codeATC: String levelATC: Integer ActiveSubstance hasActiveSubstance Legend Scientific literature sub-domain Drug sub-domain relation applicable  to the classes subClassOf attribute applicable to the class hasSymptom treatsDisorder isSymptomOfDesease ChemicalSubstance mentions Paper db:BibliographicResource Sentence isTreatedWithDrug isContraindicatedTo Disorder codeSNOMED: String codeMESH: String levelMESH: Integer synonyms: String Disease disorderName: String isActiveSubstanceOfDrug Namespaces: base: https://w3id.org/def/DRUGS4COVID19 dc: http://purl.org/dc/terms/ mentions identifier: string text: string section:string Paragraph contains (1..N) prevents contains mentions mentions mapsTo isBroader Than isNarrowerThan isBroader Than isNarrowerThan Class Fig. 6: Main classes, properties and attributes of the Drugs4Covid vocabulary. The process started with a meeting where the potential annotations (de- scribed in Section 3) were analyzed. A first set of requirements was extracted, among which the following ones could be found: 1) A paper contains paragraphs and sentences; 2) A paragraph and a sentence can mention chemical substances and disorders; 3) A paper mentions a drug; 4) A drug has active substances, which are a type of chemical substances; 5) A drug treats a disorder; 6) A disease has symptoms. The vocabulary requirements evolved throughout the de- velopment of the project (e.g. including disorders as a more general concept than diseases and symptoms). The vocabulary conceptualization was carried out following the guidelines for ontology diagrams presented in [9] and splitting the team into the two main domains involved in the data, namely “publications” and “biomedical” domains. Figure 6 depicts the vocabulary conceptualization, whose main concepts are: – Paper: A document of the corpus, subject of annotations. It can contain paragraphs (one of a series of subsections each usually devoted to one idea) and sentences (a sequence of words capable of standing alone to make an assertion, ask a question, or give a command). These concepts can mention disorders and/or chemical substances. – Disorder: A disruption to regular bodily structure and function. A disease is often known to be a medical disorder that is associated with specific symp- toms, while a symptom refers to a physical or mental feature that is regarded as indicating a condition of disease. – Chemical substance: A substance produced by or used in a chemical pro- cess. Active substances are chemical substances that are the main ingredients of drugs used to treat or prevent a disorder. Each active substance matches an ATC code as described in Section 3.1. 12 Badenes-Olmedo et al. The vocabulary reuses terms from DCMI Metadata Terms34 for bibliographic resources. Examples of the reused terms are the abstract, the license, and the source. Due to the urgency for ontology creation and the cost of reusing ontolo- gies in terms of study, analysis, comprehension or comparison, no ontologies from the biomedical domain have been reused at this stage, however they have been consulted afterwards to confirm our conceptualization. The explicit alignment with existing ontologies in the biomedical domain is left for future steps, even though some modeling decisions have already been taken in this direction, for example aligning the active substance class attributes to the ATC annotations35. We carried out a twofold evaluation: (1) quality and correctness of the on- tology using OOPS![14], and (2) a data-directed evaluation where we checked that the data in the indexed corpus can be transformed, annotated and queried with the vocabulary. Finally, the HTML documentation of the vocabulary was generated with Widoco [8] and published36 using OnToology [1]. 5.2 Knowledge Graph Construction The Drugs4Covid KG contains RDF annotations extracted from the CORD-19 dataset, represented according to the Drugs4Covid vocabulary. The relationships between the source data (annotations) and the developed ontology were declared using the RML mapping language [7]. We used a materialization approach to generate the knowledge graph because it offers the benefits of modularity and reuse, in addition to the possibility of having different versions of the KG. The building process was automated and modular, based on standard W3C technologies in order to ensure the maintainability of the KG construction pro- cess. The following steps were performed: – Data Preparation: Annotations from the CORD-19 corpus were exported into CSV files, cleaned and normalized exploiting CSVW annotations. – Mapping Definition: RML mapping rules between the data and the on- tology were defined, as well as the rules to link other external KGs. – RDF Generation: The cleaned data and mapping rules were processed with SDM-RDFizer37 to generate the corresponding RDF dataset. – Publication: The resulting RDF, along with the ATC data available in BioPortal, were published through a Virtuoso SPARQL endpoint38. In addi- tion, federated queries across different datasets can be performed to external knowledge graphs to enhance the completeness of the provided knowledge. 6 Use Cases As a result of our work, now users can quickly locate the paragraphs or sen- tences in the articles where immunosuppressive and antimalarial activity with 34 https://dublincore.org/ 35 https://bioportal.bioontology.org/ontologies/ATC 36 https://w3id.org/def/DRUGS4COVID19 37 https://github.com/SDM-TIB/SDM-RDFizer 38 https://kg.drugs4covid.oeg-upm.net/sparql Drugs4Covid 13 Fig. 7: Drugs-oriented search interface. In the example, the search interface shows drugs and diseases related to lopinavir. macrolide antibiotics are discussed; know the effects of using chloroquine and hydroxychloroquine to treat the COVID-19; or have a list of related diseases to mefloquine and azithromycin. All this is available for end users in our search en- gine, including some examples, and for anybody who wants to reuse the results in the open knowledge graph described in section 5. We now describe some of the use case scenarios that are enabled: – Drug-oriented Search: Our search engine39, given a drug name (i.e generic name, ATC-code or commercial name), a user can find in (Fig. 7): its ATC classification and commercial distributions retrieved from external sources40; the paragraphs in the corpus where the drug is mentioned, either by name or by its ATC code; and a list of related drugs and related diseases inferred from the representational models created for each of them. 39 https://search.drugs4covid.oeg-upm.net 40 https://cima.aemps.es 14 Badenes-Olmedo et al. – Keyword-oriented Search:From a RESTful API, and guided by a web template41, users can browse scientific literature through the relations in- ferred among drugs and diseases. The drugs used in conjunction with lopinavir 42, or the diseases treated with chloroquine43 can be easily explored. – Pattern-oriented Search: Users can also explore the relations discovered between drugs and diseases using SPARQL queries. We created a set of queries according to different user stories provided by practitioners, phar- macists and general public44. Listing 1.1 shows a query to retrieve the dis- eases related to drugs that combine immunosuppressant and antimalarial activities with macrolide antibiotics. Listing 1.1: List of drugs by a SPARQL query 1 PREFIX d4covid: <https://w3id.org/def/DRUGS4COVID19#> 2 3 SELECT DISTINCT ?section ?paperTitle ?notation2 ?titleDisease WHERE { 4 ?paragraph a d4covid:Paragraph . 5 ?paragraph d4covid:section ?section . 6 ?paper d4covid:contains ?paragraph . 7 ?paper dc:title ?paperTitle . 8 ?paragraph d4covid:mentions ?activeSubstance1 . 9 ?paragraph d4covid:mentions ?activeSubstance2 . 10 ?activeSubstance1 skos:notation ""P01BA02""^^xsd:string . 11 ?activeSubstance2 skos:notation ?notation2 . 12 ?paragraph d4covid:mentions ?disease . 13 ?disease a d4covid:Disease . 14 ?disease d4covid:MESHCode ’C000657245’ . 15 ?disease dc:title ?titleDisease . 16 FILTER (STRSTARTS(?notation2,""J01FA"")) 17 } 7 Lessons Learned and Future Work This paper presents our work to create a knowledge graph and a search engine to facilitate the exploration of the CORD-19 corpus through the relations dis- covered between drugs, diseases and texts. It required an effective collaboration between language technology and semantic web researchers, and with domain experts (doctors and pharmacists) who helped us to resolve doubts and guide functional decisions. The obtained results, software, models and resources are publicly available for anyone to use. After the initial release of our services and the creation of a knowledge graph together with its associated vocabulary, we have received numerous requests to continue expanding the types of annotations that have been created. Hence we plan to add annotations of additional entities, such as proteins and drug effects to extend the information offered by the knowledge graph. This would lead to an increment of the possibilities offered to users when browsing the corpus. 41 https://search.drugs4covid.oeg-upm.net/customsearch 42 https://librairy.linkeddata.es/bio-api/drugs?keywords=lopinavir 43 https://librairy.linkeddata.es/bio-api/diseases?keywords=chloroquine 44 https://kg.drugs4covid.oeg-upm.net Drugs4Covid 15 Moreover, the exploitation of these annotations in the knowledge graph leads us to increase the reuse of existing biomedical ontologies and terminologies. We are also working on a Citizen Science initiative with students of Phar- macy and Medicine to validate our annotations and help understand the type of relations that may exist between drugs and diseases. The consensus acquired from the contributions made by these citizen scientists will serve to measure the quality of the results and improve the system as a whole. As in many other Citizen Science projects, a score-based system will be created using parameters such as user participation, quality of contributions (based on gold standards and community contributions), etc. And finally, a helpdesk will allow us receiving feedback from users to continue improving the results that we provide. Other more general functionalities that have been already requested by end users are related to usability and to the creation of alerts when a drug, protein or combinations of them appear in the corpus. References 1. Alobaid, A., Garijo, D., Poveda-Villal´on, M., Santana-Perez, I., Fern´andez- Izquierdo, A., Corcho, O.: Automating ontology engineering support activities with OnToology. Journal of Web Semantics (2018) 2. Badenes-Olmedo, C., Redondo-Garcia, J.L., Corcho, O.: Distributing Text Mining tasks with librAIry. In: Proceedings of the 2017 ACM Symposium on Document Engineering - DocEng ’17. pp. 63–66. ACM Press (2017). https://doi.org/10.1145/3103010.3121040 3. Badenes-Olmedo, C., Redondo-Garc´ıa, J.L., Corcho, O.: Large-scale semantic ex- ploration of scientific literature using topic-based hashing algorithms. Semantic Web pp. 1–16 (2020). https://doi.org/10.3233/SW-200373 4. Boag, W., Sergeeva, E., Kulshreshtha, S., Szolovits, P., Rumshisky, A., Naumann, T.: Cliner 2.0: Accessible and accurate clinical concept extraction. arXiv preprint arXiv:1803.02245 (2018) 5. Brainard, J.: Scientists are drowning in COVID-19 pa- pers. Can new tools keep them afloat? Science (2020), https://www.sciencemag.org/news/2020/05/scientists-are-drowning-covid-19- papers-can-new-tools-keep-them-afloat 6. Cho, H., Lee, H.: Biomedical named entity recognition using deep neural networks with contextual information. BMC bioinformatics 20(1), 735 (2019) 7. Dimou, A., Sande, M.V., Colpaert, P., Verborgh, R., Mannens, E., de Walle, R.V.: RML: A Generic Language for Integrated RDF Mappings of Heterogeneous Data. In: LDOW. CEUR Workshop Proceedings, vol. 1184. CEUR-WS.org (2014) 8. Garijo, D.: WIDOCO: a wizard for documenting ontologies. In: International Se- mantic Web Conference. pp. 94–102. Springer (2017) 9. Garijo, D., Poveda-Villal´on, M.: Best Practices for Implementing FAIR Vocabu- laries and Ontologies on the Web (2020) 10. Goulart, R.R.V., de Lima, V.L.S., Xavier, C.C.: A systematic review of named entity recognition in biomedical texts. Journal of the Brazilian Computer Society 17(2), 103–116 (2011) 11. Kusner, M., Sun, Y., Kolkin, N., Weinberger, K.: From Word Embeddings To Document Distances. In: Bach, F., Blei, D. (eds.) Proceedings of the 32nd In- ternational Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 37, pp. 957–966. PMLR (2015) "
"Distributing Text Mining tasks with librAIry","Distributing Text Mining tasks with librAIry Carlos Badenes-Olmedo cbadenes@f.upm.es Universidad Polit´ecnica de Madrid Ontology Engineering Group Boadilla del Monte, Spain Jos´e Luis Redondo-Garc´ıa jlredondo@f.upm.es Universidad Polit´ecnica de Madrid Ontology Engineering Group Boadilla del Monte, Spain Oscar Corcho ocorcho@f.upm.es Universidad Polit´ecnica de Madrid Ontology Engineering Group Boadilla del Monte, Spain ABSTRACT We present librAIry, a novel architecture to store, process and an- alyze large collections of textual resources, integrating existing algorithms and tools into a common, distributed, high-performance workfow. Available text mining techniques can be incorporated as independent plug&play modules working in a collaborative manner into the framework. In the absence of a pre-defned fow, librAIry leverages on the aggregation of operations executed by diferent components in response to an emergent chain of events. Extensive use of Linked Data (LD) and Representational State Transfer (REST) principles are made to provide individually addressable resources from textual documents. We have described the architecture design and its implementation and tested its efectiveness in real-world scenarios such as collections of research papers, patents or ICT aids, with the objective of providing solutions for decision makers and experts in those domains. Major advantages of the framework and lessons-learned from these experiments are reported. CCS CONCEPTS •Applied computing → Document management and text pro- cessing; •Computer systems organization → Architectures ; KEYWORDS large-scale text analysis; NLP; scholarly data; text mining; data integration 1 INTRODUCTION Given the huge amount of textual data about any domain that is daily being produced or captured in any imaginable domain, it becomes crucial to provide mechanisms for programmatically pro- cessing this raw data so we can make sense out of it: discarding all the noisy, non-relevant information and keeping only the data that can bring value for the involved agents (general consumers, experts, companies, investors…). While some specifc tools already allow for advanced sense-making operations, others opt for composing a Tis work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R, fnanced by the Spanish Ministry MINECO and co-fnanced by FEDER. Author’s addresses: C. Badenes-Olmedo and J.L. Redondo-Garc´ıa and O. Corcho , Ontology Engineering Group, Universidad Polit´ecnica de Madrid. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permited. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. DocEng ’17, September 04–07, 2017, Valleta, Malta. © 2017 ACM. 978-1-4503-4689-4/17/09...$15.00 DOI: htps://doi.org/10.1145/3103010.3121040 solution where diferent analysis techniques are integrated under a uniform data schema. However, this integration involves signif- cant eforts on reconciling data sources, coordinating processing operations, and efciently exploiting results from the execution of those techniques. Tere is the need for a more fexible paradigm where tools and algorithms for textual document analysis, from diferent programming languages and technologies, can operate independently and in a collaborative manner creating a common document oriented work-fow through their actions. In the context of the scientifc publications, the personalized recommendation of research papers based on their content is a key novel feature for performing a smart selection of relevant resources over very big collections of scientifc content. From the set of values and diferent atributes extracted from the papers and by generating advanced knowledge models about the information they contain we can bridge across the diferent relevant pieces of information and allow users to navigate them in a more efcient and powerful way. Tis knowledge about a specifc document is frequently ac- quired by diferent techniques focused on revealing certain aspects of it, that are later combined to achieve one particular task. Te architecture presented in this paper aims to ease the way diferent sofware modules work together and lays the foundation for ef- ciently process big volumes of textual documents in a distributed, decoupled manner. 2 RELATED WORK Te annotation of human-readable documents is a well-known problem in the Artifcial Intelligence domain in general and Infor- mation Retrieval and Natural Language Processing felds in par- ticular. Tere already exist a broad set of tools and frameworks able to analyze text for automatically producing such annotations, at very diferent levels of granularity: from minimal units such as terms and entities, to descriptors at the level of the entire collec- tion such as topics or summaries. For example, StanfordNLP [7] framework allows to perform diferent operations such as PoS or Named Entity Recognition in various languages. Others like Mal- let1 or SparkLDA2 perform topic modeling and clustering. Te system we propose looks at the transversal problem of making those standalone tools coexisting under the same solution. Being able to efectively integrating them under a common ecosystem helps to seamlessly obtain diferent kind of annotations and boost the way those solutions can make sense of document collections. Certain systems among the research and industrial communities have already integrated some of the annotation tools introduced 1htp://mallet.cs.umass.edu 2htps://spark.apache.org/mllib/ above. For example, [2] works with records from the biomedi- cal domain, where robustness and high precision are prioritized. Terefore they rely on techniques supported by GATE3 framework, which widely supports hand-crafed, domain specifc techniques such as rules or fnite state transducers. On the other side of the spectrum we fnd [6], where the authors try to annotate text from a much noisier, sparser and error-prone medium: a tweet stream. Terefore they do not rely on any linguistic feature, due to the unpredictable way short social media post are writen. We observe how each of those examples has very specifc needs and leverages on certain annotation tools in order to accomplish the tasks it was originally created for. In both systems the involved components are highly coupled so they can not be easily extended to contemplate complementary annotation tools or alternative modules. On the contrary, librAIry advocates loosely interconnected components that make the architecture more reusable and expandable in other systems across domains. One crucial problem regarding the re-usability and expansion possibilities of those systems and the tools they leverage on is the language they have been developed in. For example, Mallet uses Java, but others like spaCy 4 are python-based. To the best of our knowledge, there has not been any signifcant eforts on reconciling into a single architecture such heterogeneous set of tools, therefore minimizing the engineering efort and maximizing scalability of the system so it can be applied to very diferent domains and textual annotation tasks. In addition, available annotation systems rely on certain storage solutions that are suited for some tasks but are less adequate others. For example [5] uses a relational database (MySQL5) to ensure reliability and speed in managing the indexed information. In [8], the authors leverage on Virtuoso triple-store to provide native graph operations over the data. But new requirements may be considered for those systems so diferent storage needs can come into play. For example, column oriented databases (Cassandra6) can help to beter handle high-volume queries on specifc data felds. Same goes with text oriented indexes such as ElasticSearch 7, which can provide customized text-based search operations over the available information. librAIry straightforward supports the coexistence of diferent storage solutions, so it can be agnostic to the kind of underlying storage modules implemented. Tanks to the distributed nature of the proposed architecture, diferent databases can be synchronized under the same common environment working together to store and deliver results in a more efcient manner. 3 LIBRAIRY librAIry is a framework where diferent text mining tools, available in various languages and technologies, can operate in a distributed, high-performance and isolated manner creating a common work- fow through their actions. Instead to work towards a pre-defned sequence of actions, synchronization across modules is achieved through the aggregation of the operations executed by them in response to an emergent chain of events. Tis raises both technical 3htps://gate.ac.uk/ 4htps://spacy.io 5htps://www.mysql.com/ 6htp://cassandra.apache.org 7htps://www.elastic.co Figure 1: Domain deleted fow. and functional challenges to coordinate multiple executions. From the technical point of view, isolated environments and communica- tion mechanisms are provided so initially dissimilar tools can be executed with maximum guarantees. From the functional point of view, all executions are coordinated to reach a fnal result as aggregation of partial results derived from each execution. 3.1 Functional Features Te architecture is articulated around three main concepts: (1) the resource such as document, a part of a document, or a domain. (2) the actions performed over them: create, update or delete a resource. And (3) the new state that is reached by the resource afer an action is performed, such as created, updated or deleted. An event is a message containing details about those three aspects, published on a shared event-bus available for all the modules deployed in the framework. Tis will, in turn, allow that any module can perform actions on one or more resources in response to a new state reached by a given resource. Actions executed in parallel from distributed environments. 3.1.1 Resources. Two main kinds of resources are considered: those derived from external sources such as (1) documents from textual fles (e.g. a research paper), (2) parts from logical divisions of a document (e.g. rhetorical classes or sections), and (3) domains from sets of documents (e.g. a conference or journal), and those derived from processing the previous ones such as annotations. To beter illustrate this model, consider to explore the research papers published at the SIGGRAPH conference in 2016. First, ev- ery paper will be materialized as a new document containing the full-text. Immediately afer, the document will be automatically associated to several parts, each of them grouping sentences by rhetorical class (e.g. approach, background, challenge, future work and outcome) and by section (e.g abstract, introduction). Finally, a new domain will be created grouping all these documents. Dif- ferent analysis will be performed extending the initial set of re- sources with more annotations at several representational levels: at document level, full-text based annotations are provided such as named-entities, compounds and descriptive tags. At relational level, connection between resources are found (e.g. semantic similarity- based relationships). And fnally, at domain level annotations such as tags and summaries are composed describing the corpus of doc- uments. 3.1.2 Event-based Paradigm. An event illustrates a performed action, i.e. a resource and its new state. It follows the Representa- tional State Transfer (REST)[4] paradigm, but taking into account Figure 2: Resource states. the state reached afer an action, i.e created, deleted or updated. Tus, an event contains the resource type and the new state reached by a specifc resource. 3.1.3 Linked Data Principles . Data in librAIry is individually addressable and linkable [9] following the Linked Data principles defned by T. Berners-Lee [1]. Tus, resources (i.e. a domain, a document, a part or an annotations) have: (1) a URI as name, (2) a retrievable (or dereferenceable) HTTP URI so that it can be looked up, (3) a useful information provided by using standard notation (e.g. JavaScript Object Notation (JSON)) when it is looked up by URI, and (4) links to other URIs so that other resources can be discovered from it. 3.2 Framework Architecture Following a publisher/subscriber approach, all the modules in the framework can publish and read events to notify and to be noti- fed about the state of a resource. Terefore, the system fow is not unique and is not explicitly implemented, instead distributed and emergent fows can appear according to particular actions on resources. 3.2.1 Event-Bus. We use the Advanced Message Qeuing Pro- tocol (AMQP) as the messaging standard in librAIry to avoid any cross-platform problem and any dependency to the selected mes- sage broker. Tis protocol defnes: exchanges, queues, routing-keys and binding-keys to communicate publishers and consumers.A mes- sage sent by a publisher to an exchange is tagged with a routing-key. Consumers matching that routing-key with the binding-key used to link the queue to that exchange will receive the message. In librAIry this key follows the structure: resource.status.Since a wildcard-based defnition can be used to set the key, this paradigm allow modules both listening to individual type events (e.g. ´domains.created´for new domains), or multiple type events (e.g. #.created for all new resources). 3.2.2 API. A HTTP-Rest Application Program Interface (API) was designed for interaction with end-users. Any external oper- ation motivated by a user will be handled here. Some of them, usually those related to reading operations, will be completely managed by this module geting all the data from the internal stor- age. However, those operations implying a modifcation of the status of some resource (e.g. creation of a document), may be also performed by other modules listening for that type of event asyn- chronously. Tis module publishes to the following routing-keys: domain.(created;updated;deleted), document.(created;updated;deleted), part.(created;updated;deleted), and annotation.(created;updated;deleted). Figure 3: Modules. 3.2.3 Storage. Multiple types of data can be handled in this ecosystem. Inspired in the Data Access Object (DAO) patern, we have created a Unifed Data Manager (UDM) providing access to any type of data used in the system. Tree types of databases have been considered: • column-oriented database: Focused on unique identi- fed and/or structured data. Tis storage allow us searching key elements across resources. • document-oriented database: Focused on indexing raw text. Tis storage allow us to execute advanced search operations over all the information gathered about a textual resource. • graph database: Focused on relations. Tis storage allow us exploring resources through the relationships between them. 3.2.4 Modules. Te modules composing librAIry have been de- signed following the microservices architectural style. A module is a cohesive (i.e. it implements only functionalities strongly related to the concern that it is meant to model [3]) and independent process working on the framework with a specifc purpose. Tis purpose is defned by both the routing-key and the binding-key associated to the events handled by the module. Tese are the main types of modules identifed in librAIry: • Harvester: creates system resources such as documents, parts and domains, from local or remote located textual fles. – Listening for: nothing – Publishing to: document.(created), part.(created), domain.(created;updated) • Annotator: retrieves named-entities, compounds, lem- mas and other annotations resulting of Natural Language Processing (NLP) task execution from documents and parts. – Listening for: document.(created;updated), part.(created;updated) – Publishing to: annotation.(created;deleted) • Modeler: builds representational models from a given domain. – Listening for: domain.(created;updated) – Publishing to: annotation.(created;deleted) 4 EXPERIMENTS AND LESSONS-LEARNED librAIry has been used in some real scenarios such as a research- paper repository for the European project DrInventor 8, a support to decision makers for analyzing patents and public aids for the 8htp://drinventor.eu "
"","Semantic Web 0 (0) 1 1 IOS Press Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms Editor(s): Tomi Kauppinen, Aalto University, Finland; Daniel Garijo, University of Southern California, USA; Natalia Villanueva, The University of Texas at El Paso, USA Solicited review(s): Anita de Waard, Elsevier Labs Carlos Badenes-Olmedo a,∗, José Luis Redondo-García b and Oscar Corcho a a Ontology Engineering Group, Universidad Politécnica de Madrid, Boadilla del Monte, Spain E-mails: cbadenes@fi.upm.es, ocorcho@fi.upm.es ORCIDs: 0000-0002-2753-9917, 0000-0002-9260-0753 b Amazon Research, Cambridge, UK E-mail: jluisred@amazon.com ORCID: 0000-0002-7413-447X Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activities when browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead to unexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms represent documents in a common feature space that abstract them away from the specific sequence of words used in them. Probabilistic Topic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latent space some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematic information gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics to justify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniques that uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extending those queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations on both scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency. Keywords: Document Similarity, Information Search and Retrieval, Clustering, Topic Models, Hashing 1. Introduction Huge amounts of documents are publicly available on the Web offering the possibility of extracting knowl- edge from them (e.g. scientific papers in digital jour- nals). Document similarity comparisons in many in- formation retrieval (IR) and natural language process- ing (NLP) areas are too costly to be performed in such huge collections of data and require more efficient ap- *Corresponding author. E-mail: cbadenes@fi.upm.es. proaches than having to calculate all pairwise similari- ties. In this paper we address the problem of programmat- ically generating annotations for each of the items in- side big collections of textual documents, in a way that is computationally affordable and enables a semantic- aware exploration of the knowledge inside it that state- of-the-art methods relying on topic models are not able to materialize. Most text mining algorithms represent documents in a common feature space that abstracts the specific 1570-0844/0-1900/$35.00 c⃝ 0 – IOS Press and the authors. All rights reserved 2 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm sequence of words used in each document and, with appropriate representations, facilitate the analysis of relationships between documents even when written using different vocabularies. Although a sparse word or n-gram vectors are popular representational choices, some researchers have explored other representations to manage these vast amounts of information. Latent Semantic Indexing (LSI) [17], Probabilistic Latent Se- mantic Indexing (PLSI) [25] and more recently, Latent Dirichlet Allocation (LDA) [11], which is the simplest probabilistic topic model (PTM) [10], are algorithms focused on reducing feature space by annotating docu- ments with thematic information. PLSI and PTM also allow a better understanding of the corpus through the topics discovered, since they use probability distribu- tions over the complete vocabulary to describe them. However, only PTM’s are able to identify topics in previously unseen texts. One of the greatest advantages of using PTM in large document collections is the ability to represent docu- ments as probability distributions over a small num- ber of topics, thereby mapping documents into a low- dimensional latent space (the K-dimensional probabil- ity simplex, where K is the number of topics). A doc- ument, represented as a point in this simplex, is said to have a particular topic distribution. This brings a lot of potential when applied over different IR tasks, as evidenced by recent works in different domains such as scholarly [23][19], health [45] [38] [50], legal [43][20], news [24] and social networks [47][15]. This low-dimensional feature space could also be suitable for document similarity tasks, especially on big real- world data sets, since topic distributions are continuous and not as sparse as discrete-term feature vectors. Exact similarity computations for most topic distri- butions require to have complexity O(n2) for neigh- bours detection tasks or O(kn) computations when k queries are compared against a dataset of n documents. Computation can be an approximate nearest neighbor (ANN) search problem. ANN search is an optimization problem that finds nearest neighbors of a given query q in a metric space of n points. Due to the low stor- age cost and fast retrieval speed, hashing is one of the most popular solutions for ANN search [34] [4] [60]. This technique transforms data points from the original feature space into a binary-code space, so that simi- lar data points have larger probability of collision (i.e. having the same hash code). This type of formulation for the document similarity comparison problem has proven to yield good results in the metric space due to the fact that ANN search has been designed to handle distance metrics (e.g. cosine, Euclidean, Manhattan) [49][46][29], even in high-dimensional simplex spaces handling information-theoretically motivated metrics ( e.g. Hellinger, Kullback-Leibler divergence, Jensen- Shannon divergence) as demonstrated by [39]. However, the smaller space created by existing hash- ing methods loses the exploratory capabilities of topics to support document similarity. The notion of topics is lost and therefore the ability to make thematic ex- plorations of documents. Moreover, metrics in simplex space are difficult to interpret and the ability to explain the similarity score on the basis of the topics involved in the exploration can be helpful. While other models based on vector representations of documents are sim- ply agnostic to the human concept of themes, topic mod- els can help finding the reasons why two documents are similar. Semantic knowledge can be thought of as knowledge about relations among several types of elements, in- cluding words, concepts, and percepts [22]. Since topic models create latent themes from word co-occurrence statistics in corpus, a topic (i.e latent theme) reflects the knowledge about the word-word relations it contains. This abstraction can be extended to cover the knowl- edge derived from sets of topics. The topics obtained via state-of-the art methods (LDA) are hierarchically divided into groups with different degrees of seman- tic specificity in a document. Documents can then be annotated with the semantic inferred from the topics detected, and from their relation between topics inside each hierarchy level. Let’s look at a practical example to clarify this idea. A topic model is created from texts la- beled with Eurovoc 1 categories. This model2 annotates texts with categories inferred from their topic distribu- tions. For the document ""Commission Decision of 23 December 2003.. on seeds and propagating material of gramineae, Triticum aestivum.."" 3, the top5 categories are: (1) research, (2) sugar, (3) fats, (4) textile_industry and (5) marketing . In contrast to these categories that standard topic modelling methods are able to offer, a 3-level hierarchical set of topics would be: (1) research, (2) sugar and fats, and (3) textile_industry and market- ing. The knowledge provided by each of these annota- tions is derived from the relations between the topics that compose it. Based on these semantic annotations, the content-based similarity among documents is calcu- 1http://publications.europa.eu/resource/dataset/eurovoc 2http://librairy.linkeddata.es/jrc-en-model/ 3https://eur-lex.europa.eu/eli/dec/2004/57(1)/oj C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 3 lated and the exploration of large document collections is performed following an ANN search. Thus, in this paper, we propose a hashing algorithm that (1) groups similar documents, (2) preserves their topic distributions, and (3) works over unseen docu- ments. Therefore our contributions are: – a novel hashing algorithm based on topic mod- els that not only performs efficient searches, but also introduces semantic in the hierarchy of con- cepts as a way to restrict those queries and provide explanatory information. – an optimized and easily customizable open-source implementation of the algorithm [9] – data-sets and pre-trained models to facilitate other researchers to replicate our experiments and validate and test their own ideas [9] 2. Document Similarity In the probability simplex space created from topic models, documents are represented as vectors con- taining topic distributions. Distance metrics based on vector-type data such as Euclidean distance (l2), Man- hattan distance (l1), and angular metric (θ) are not op- timal in this space [39]. Information-theoretically mo- tivated metrics such as Kullback-Leibler (KL) diver- gence (Eq.1) (also known as relative entropy), Jensen- Shannon (JS) divergence (Eq.2) (as its symmetric ver- sion) and Hellinger (He) distance (Eq.3) are often more reasonable [39]: KL(P, Q) = K � i=1 p(xi) log p(xi) q(xi) (1) JS (P, Q) = 1 2 KL � p, p + q 2 � + 1 2 KL � q, p + q 2 � (2) He(P, Q) = K � i=1 �� p(xi) − � q(xi) �2 (3) where P and Q are two known distributions, K is the dimensionality of P and Q, and pi and qi are the values of the ith component of P and Q, respectively. He distance is also symmetric and, along with JS divergence, are usually used in various fields where a comparison between two probability distributions is re- Fig. 1. Distance values based on KL-divergence between 10 pair of documents from topic models with 100-to-2000 dimensions. quired. However, all these metrics are not well-defined distance metrics, that is, they do not satisfy triangle inequality [14]. This inequality considers d(x, z) <= d(x, y) + d(y, z) for a metric d [22]. It places strong constraints on distance measures and on the locations of points in a space given a set of distances. As a metric axiom the triangle inequality must be satisfied in order to take advantage of the inferences that can be deduced from it. Thus, if similarity is assumed to be a mono- tonically decreasing function of distance, this inequal- ity avoids the calculation of all pairs of similarities by considering that if x is similar to y and y is similar to z, then x must be similar to z. S2JSD was introduced by [18] to satisfy the triangle inequality. It is the square root of two times the JS divergence: S 2JS D(P, Q) = � 2 ∗ JS (P, Q) (4) However, making sense out of the similarity score is not easy. As shown in figures 1 to 4, given a set of pairs of documents, their similarity scores vary according to the number of topics. So the distances between those pairs fluctuate from being more to less distant when changing the number of topics. Distances between documents generally increase as the number of dimensions of the space increases. This is due to the fact that as the number of topics describing the model increases, the more specific the topics will be. Topics shared by a pair of documents can be bro- ken down into more specific topics that are not shared by those documents. Thus, similarity between pairs of documents is dependent on the model used to represent them when considering this type of metrics. We know that absolute distances between documents vary when we tune hyperparameters differently, but in this study 4 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm Fig. 2. Distance values based on JS -divergence between 10 pair of documents from topic models with 100-to-2000 dimensions. Fig. 3. Distance values based on He-divergence between 10 pair of documents from topic models with 100-to-2000 dimensions. Fig. 4. Distance values based on S 2JS D between 10 pair of docu- ments from topic models with 100-to-2000 dimensions. we also see that ""relative distances"" also change: e.g. for model M1, A is closer to B than C, but according to a M2 trained in the same corpora with different pa- rameters , A is closer to C than B (cross-lines in figs 1-4). This behaviour highlights the difficulty of estab- lishing absolute similarity thresholds and the complex- ity to measure distances taking into account all dimen- sions. Distance thresholds should be model-dependent rather than general and metrics flexible enough to han- dle dimensional changes. These challenges are tack- led through the proposed hashing algorithms by means of clusters of topics to measure similarity, instead of directly using their weights. 3. Hashing Topic Distributions Hashing methods transform the data points from the original feature space into a binary-code Ham- ming space, where the similarities in the original space are preserved. They can learn hash functions (data- dependent) or use projections (data-independent) from the training data [56]. Data-independent methods unlike data-dependent ones do not need to be re-calculated when data changes, i.e. adding or removing documents to the collection. Taking large-scale scenarios into ac- count (e.g. Document clustering, Content-based Recom- mendation, Duplicate Detection), this is a key feature along with the ability to infer hash codes individually (for each document) rather than on a set of documents. Data-independent hashing methods depend on two key elements: (1) data type and (2) distance metric. For vector-type data, as introduced in section 2, based on lp distance with pϵ[0, 2) lots of hashing methods have been proposed, such as p-stable Locality-Sensitive Hashing (LSH) [16], Leech lattice LSH [3], Spheri- cal LSH [51], and Beyond LSH [5]. Based on the θ distance many methods have been developed such as Kernel LSH [31] and Hyperplane hashing [53]. But only few methods handle density metrics in a simplex space. A first approach transformed the He divergence into an Euclidean distance so that existing ANN tech- niques, such as LSH and k-d tree, could be applied [30]. But this solution does not consider the special attribu- tions of probability distributions, such as Non-negative and Sum-equal-one. Recently, a hashing schema [39] taking into account the symmetry has been proposed, non-negativity and triangle inequality features of the S2JSD metric for probability distributions. For set-type data, Jaccard Coefficient is the main metric used. Some examples are K-min Sketch [33], Min-max hash [28], B-bit minwise hashing [32] and Sim-min-hash [59]. All of them have demonstrated efficiency in the search for similar documents, but none of them allows the search for documents (1) by thematic areas or (2) by similarity levels, nor they offer (3) an explanation about the similarity obtained beyond the vectors used to calculate it. Binary-hash codes drop a very precious information: the topic relevance. C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 5 Fig. 5. Hash method based on hierarchical set of topics from a given topic distribution A new hierarchical set-type data is proposed. Each level of the hierarchy indicates the importance of the topic according to its distribution. Level 0 contains the topics with the highest score. Level 1 contains the top- ics with highest score once the first ones have been eliminated, and so on. From a vector of components, where each of the components is the score of topic t, a vector containing set of topics is proposed, where each of the dimensions means a topic relevance. Thus, for the topic distribution q = [0.3, 0.15, 0.4, 0.15], a hier- archical set of topics may be h = {(t2), (t0), (t1, t3)}. It means that topic t2 (0.4) is the most relevant, then topic t0 (0.3) and, finally, topics t1 (0.15) and t3 (0.15). This is just an example about the data structure that will support the different hashing strategies. In section 3.3 some approaches to create hash codes based on this data structure are described. 3.1. Data Type A traditional approach to text representation usually requires encoding of documents into numerical vectors. Words are extracted from a corpus as feature candidates and based on a certain criterion they are assigned values to describe the documents: term-frequency, TF-IDF, information gain, and chi-square are typical measures. But this causes two main problems: huge number of dimensions and sparse distribution. The use of topics as feature space has been extended to mapping documents into low-dimensional vectors. However, as shown in Figures 1 to 4, the distance metrics based on probability densities vary according to the dimensions of the model and reveal the difficulty of calculating the similarity values using the vectors with the topic distributions. Since hashing techniques can transform both vector and set-based data [39] [28] into a new space where the similarity (i.e. closeness of points) in the original feature space is preserved, a new set-based data struc- ture is proposed in this paper. It is created from clusters of topics organized by relevance levels and it aims to extend the ability of building queries with topic-based restrictions over the searching space while maintaining high level of accuracy. The new hierarchical set-type data describes each document as a sequence of sets of topics sorted by relevance. Each level of the hierarchy expresses how important those topics are in that document. In the first level (i.e level 0) are the topics with the highest score. In the second level (i.e level 1) are the topics with the highest score once the first ones have been removed, and so on. In this work, several clustering approaches have been considered to assign topics to each level. In a feature space created from a PTM with eight topics, for example, each data point p is described 6 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm by a eight-dimensional vector with the topic distribu- tions: vp = [t0, t1, t2, t3, t4, t5, t6, t7] . Then, given a point q1 = [0.18, 0.15, 0.2, 0.05, 0.14, 0.11, 0.09, 0.08], the three-level hierarchical set of topics may be h = [{t2}, {t0}, {t1, t4}]. It means that t2 is the most rel- evant topic, then topic t0 and finally topics t1 and t4. This is just an example about the data structure that will support the hashing strategies. In section 3.3 some ap- proaches to create hash codes based on this data struc- ture are described. Domain-specific features such as vocabulary, writ- ing style, or speech type, have a major influence on the topic models, but not in the hashing algorithms de- scribed in this article. The methods for creating hash codes are agnostic of these particularities since they are only based on the topic distributions generated by the models. 3.2. Distance Metric Since documents are described by set-type data, the proposed distance metric is based on the Jaccard co- efficient. This metric computes the similarity of sets by looking at the relative size of their intersection as follows: J(A, B) = |A ∩ B| |A ∪ B| (5) where A and B are set of topics. More specifically, dJ is based on the Jaccard distance, which is obtained by subtracting the Jaccard coefficient J from 1: dJ(A, B) = 1 − J(A, B) (6) The proposed distance measure dH used to compare hash codes created from set of topics is the sum of the Jaccard distances dj for each hierarchy level, i.e. for each set of topics: dH(H1, H2) = L � l=1 � dJ(H1(xl), H2(xl)) � (7) where H1 and H2 are hash codes, H1(xl) and H2(xl) are the set of topics up to level l for each hash code H and L is the maximum hierarchy level. A corner case is L = T, where T is the number of topics in the model. Fig. 6. Threshold-based Hierarchical Hash (L=3) 3.3. Hash Function The hash function clusters topics based on relevance levels. Three approaches are proposed depending on the criteria used to group topics: threshold-based, centroid- based and density-based. 3.3.1. Threshold-based Hierarchical Hashing Method This approach is just an initial and naive way of grouping topics by threshold values into each relevance level. They can be manually defined or automatically generated by thresholds dividing the topic distributions as follows: thinc = 1 (L + 1) · T (8) where L is the number of hierarchy levels, and T the number of topics. If L = 3 and T = 10 for a topic distribution td defined as follows: td = [0.017, 0.141, 0.010, 0.172, 0.030, 0.090, 0.199, 0.133, 0.031, 0.171] (9) Then, a threshold-based hierarchical hash HT, with an automatically created threshold defined by equation 8, is equals to HT = {(t1, t3, t5, t6, t7, t9), (), (t4, t8)} with thinc = 0.025 (Fig 6). 3.3.2. Centroid-based Hierarchical Hashing Method This approach assumes topic distributions can be partitioned into k clusters where each topic belongs to the cluster with the nearest mean score. It is based on C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 7 Fig. 7. Centroid-based Hierarchical Hash (L=3) the k-Means clustering algorithm, where k is obtained by adding 1 to the number of hierarchy levels. Unlike the previous method, threshold values used to define the hierarchy levels may vary between documents, i.e. for each topic distribution, since they are calculated for each distribution separately. Following the previous example, if L = 3 and T = 10 for a topic distribution td defined in equation 9, then a centroid-based hieararchical hash HC equals to HC = {(t6), (t9, t7, t3, t1), (t5)} (Fig 7). 3.3.3. Density-based Hierarchical Hashing Method This approach also considers relative hierarchical thresholds for each relevance level. Now, a topic distri- bution is described by points in a single dimension. In this space, topics closely packed together are grouped together. This approach does not require a fixed number of groups. It only requires a maximum distance (eps) to consider two points close and grouped together. This value can be estimated from the own distribution of topics (e.g. variance). Following the above example, if L = 3 and td is the topic distribution defined in equation 9, then a density-based hierarchical hash HD is equals to HD = {(t6), (t9, t3), (t1)} when eps equals to the variance of the topic distribution (Fig 8). 3.4. Online-mode Hashing Hashing methods are batch-mode learning models that require huge data for learning an optimal model and cannot handle unseen data. Recent work address on- line mode by learning algorithms [26] that get hashing model accommodate to each new pair of data. But these Fig. 8. Density-based Hierarchical Hash (L=3) approaches require the hashing model to be updated during each round based on the new pairs of data. Our methods rely on topic models to build hash codes. These models do not require to be updated to make inferences about data not seen during training. In this way, the proposed hashing algorithms can work on large-scale and real-time data, as the size and the nov- elty of the collection does not influence the annotation process. 4. Experiments As mentioned above (Section 2), it is difficult to interpret the similarity score calculated by metrics in a probability space. Since all of them are based on adding the distance between each dimension of the model (eq. 1, 2 and 3), distributions that share a fair amount of the less representative topics may still get higher similarity values than those that share the most representative ones specially if the model has a high number of dimensions. Figures 9 and 10 show overlapped topic distributions of two pairs of documents. In the first case (fig 9), none of the most representative topics of each document is shared between them. However, the similarity score cal- culated from divergence-based metrics (eq 2) is higher than in the second case (fig 10), where the most rep- resentative topic is shared (topic 26). This behavior is due to the sum of the distances between the less rep- resentative topics (i.e. topics with a low weight value) being greater than the sum of the distances between the most representative ones (i.e. topic with a high weight value). In high-dimensional models, that sum may be more representative than the one obtained with the most 8 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm Fig. 9. Topic Distribution of two documents. Similarity score, based on JSD, is equals to 0.74 Fig. 10. Topic Distribution of two documents. Similarity score, based on JSD, is equals to 0.71 relevant topics, which are fewer in number than the less relevant ones. The following experiments aim to validate that hash codes based on hierarchical set of topics not only make it possible to search for similar documents with high accuracy, but also to extend queries with new restrictions and to offer information that helps explaining why two documents are similar. 4.1. Datasets and Evaluation Metrics Three datasets [9] are used to validate the proposed approach. The OPEN-RESEARCH4 dataset consist of 500k research papers in Computer Science, Neuro- science, and Biomedical randomly selected from the Open Research Corpus [54]. The CORDIS5 dataset contains 100k documents describing research and in- novation projects funded by the European Union under a framework programme since 1990. The PATENTS 4https://labs.semanticscholar.org/corpus/ 5https://data.europa.eu/euodp/data/dataset/cordisref-data dataset consists of 1M patents randomly selected from the USPTO6 collection. For each dataset, documents are mapped to two latent topic spaces with different dimensions using LDA. We perform parameter estima- tion using collapsed Gibbs sampling for LDA [21] from the open-source librAIry [7] software. It is a frame- work that combines natural language processing (NLP) techniques with machine learning algorithms on top of the Mallet toolkit [40], an open-source machine learning package. The number of topics varies to study their influence on the performance of the algorithm (i.e. CORDIS-70 indicates a latent space created with 70 topics). Experiments use JS divergence as an information- theoretically motivated metric in the probabilistic space created by topic models. Since it is a smoothed and symmetric alternative to the KL divergence, which is a standard measure for comparing distributions [13], it has been extensively used as state-of-the-art metric over topic distributions in literature [52][1][39]. Our upper bound is created from the brute-force comparison of the reference documents with all documents in the collection to obtain the list of similar documents. In this scenario the goal is to minimize the accuracy loss introduced by hashing algorithms. Since this is a large-scale problem and an accuracy-oriented task, re- call is not a good measure to be considered and preci- sion is only relevant for sets much smaller than the total size of data (between 3-5 candidates). All the experimental results are averaged over ran- dom training/set partitions. For each topic space, 100 documents are selected as references, and the remaining documents as search space. As noted above, only p@5 will be used to report the results of the experiments. 4.2. Retrieving Similar Documents It is challenging to create an exhaustive gold standard, given the significant amount of human labour that is required to get a comprehensive view of the subjects being covered in it. In order to overcome this problem, the list of similar documents to a given one is obtained after comparing the document with all the documents of the repository and sorting the result. We have observed that different distance functions perform similarly in this scenario (figs 1 to 4), so we have decided to use only the JS divergence (eq. 2) in our experiments. 6https://www.uspto.gov/learning-and-resources/ip- policy/economic-research/research-datasets C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 9 OPEN-RES-100 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.22 0.20 0.86 1.00 0.66 0.80 3 0.23 0.20 0.87 1.00 0.81 1.00 4 0.27 0.20 0.89 1.00 0.86 1.00 5 0.27 0.20 0.92 1.00 0.89 1.00 6 0.27 0.20 0.94 1.00 0.92 1.00 Table 1 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Open Research dataset using a model with 100 topics. LEVEL column indicates the number of hierarchies used. OPEN-RES-500 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.23 0.20 0.76 0.80 0.67 0.80 3 0.24 0.20 0.80 1.00 0.71 0.80 4 0.25 0.20 0.83 1.00 0.74 0.80 5 0.25 0.20 0.86 1.00 0.81 1.00 6 0.24 0.20 0.89 1.00 0.86 1.00 Table 2 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Open Research dataset using a model with 500 topics. LEVEL column indicates the number of hierarchies used. Only the top N documents obtained from this method are used as reference set to measure the performance of the algorithms proposed in this paper. The value of N is equals to 0.5% of the corpus size (i.e. if the corpus size is equal to 1000 elements, only the top 5 most similar documents are considered relevant for a given docu- ment). This value has been considered after reviewing datasets used in similar experiments [30][39]. In those experiments, the reference data is obtained from ex- isting categories, and the minimum average between corpus size and categorized documents is around 0.5%. Once the reference list of documents similar to a given one is defined, the most similar documents through the proposed methods (i.e. threshold-based hi- erarchical hashing method (thhm), centroid-based hier- archical hashing method (chhm) and density-based hier- archical hashing method (dhhm)) are also obtained. An inverted index has been implemented by using Apache Lucene7 as document repository. The source code of both the algorithms and tests is publicly available [9]. Let’s look at an example to better understand the procedure. We want to measure the accuracy and data size ratio used to identify the top5 similar documents to 7http://lucene.apache.org CORDIS-70 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.18 0.20 0.92 1.00 0.66 0.70 3 0.20 0.20 0.92 1.00 0.80 0.80 4 0.22 0.20 0.94 1.00 0.86 1.00 5 0.23 0.20 0.91 1.00 0.89 1.00 6 0.19 0.20 0.92 1.00 0.91 1.00 Table 3 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on CORDIS dataset using a model with 70 topics. LEVEL column indicates the number of hierarchies used. CORDIS-150 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.19 0.20 0.88 1.00 0.78 0.80 3 0.19 0.20 0.92 1.00 0.80 1.00 4 0.25 0.20 0.91 1.00 0.82 1.00 5 0.25 0.20 0.91 1.00 0.83 1.00 6 0.27 0.20 0.91 1.00 0.86 1.00 Table 4 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on CORDIS dataset using a model with 150 topics. LEVEL column indicates the number of hierarchies used. a new document d1 from a corpus of 1000 documents . The similarity between d1 and all the documents in the corpus is calculated based on JS divergence. The top50 (0.5%) documents with the highest values will be the set of documents considered as similar to d1. As we are going to use an ANN-based approach, we need the hash expressions of all documents to measure similarity. The data structure proposed in this work is a hierarchy of sets of topics, so that the most similar documents are those that share most of the topics at the highest levels of the hierarchy. The representational model for this example only considers 8 topics, that is, a document is described by a vector with 8 dimensions where each dimension corre- sponds to a topic (i.e [t0, t1, t2, t3, t4, t5, t6, t7] ) and its value will be the weight of that topic in the document, for example d1 = [0.18, 0.15, 0.2, 0.05, 0.14, 0.11, 0.09, 0.08]. The hierarchy level (L) will be equal to 2, i.e. the hash expression has two hierarchical sets of topics: h = {h0, h1}. According to methods described at Section 3.3, there are 3 ways to create the hierarchical hash codes for documents: 1. threshold-based (thhm): 2 thresholds are defined as described in section 3.3.1, for example 0.15 and 10 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 0.1 . h0 includes the topics with a weight greater than 0.15, and h1 the remaining topics with a weight greater than 0.1. Then h0 = {t0, t1, t2} and h1 = {t4, t5}. Based on the hash expression h = {(t0, t1, t2), (t4, t5)}, the documents that share more topics in those levels (i.e h0 = (t0 OR t1 OR t2), h1 = (t4 OR t5)) or in other levels but with less relevance are ordered. Since there are many topics in the expression, potentially many documents are similar when sharing at least one of them. This increases the data ratio. Accuracy is also affected, as the algorithm is not able to bring under the same bucket similar documents. In short, the hash expression is not representative of the document, for the given exploratory task. 2. centroid-based (chhm): sets of topics are created using a clustering algorithm based on centroids as described in section 3.3.2. The cardinalities of the hierarchical groups are generally more uniform with this method. Since k = L + 1 = 3 in this example, h0 = {t0, t2} and h1 = {t1, t4}. The number of representative topics at each level of the hierarchy is usually lower, and this causes the data ratio used to discover similar documents to decrease as well. This approach increases the pre- cision because now the hierarchy is more selective to distinguish similar documents. However, the size of region of similar candidates is still high. 3. density-based (dhhm): now the clustering algo- rithm is based on how dense certain regions in the topic relevance dimensions are as described in section 3.3.3. It can group topics that have un- balanced distributions and, therefore, generates more discriminating hash expressions than with the previous algorithm. In the example, we would have a hash expression like this: h0 = {t2} and h1 = {t0}. This significantly reduces the data ratio used to discover similar documents and does not excessively penalize accuracy. Obviously, in- creasing L (i.e. number of hierarchies) increases precision, but with L > 3 that gain is not so sig- nificant. As it can be seen in tables 1 to 6, the mean and me- dian of precision are calculated to compare the perfor- mance of the methods. In this assessment environment, the variance is not robust-enough because score values don’t follow a normal distribution. We consider the re- sult obtained as significant, based on the fact that mean and median values are fairly close. The centroid-based method (chhm) and the density-based method (dhhm) PATENTS-250 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.03 0.00 0.71 0.80 0.67 0.80 3 0.08 0.00 0.91 1.00 0.90 1.00 4 0.11 0.00 0.95 1.00 0.95 1.00 5 0.12 0.00 0.95 1.00 0.96 1.00 6 0.11 0.00 0.97 1.00 0.97 1.00 Table 5 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Patents dataset using a model with 250 topics. LEVEL column indicates the number of hierarchies used. PATENTS-750 (p@5) LEVEL THHM CHHM DHHM mean median mean median mean median 2 0.02 0.00 0.77 0.80 0.76 0.80 3 0.04 0.00 0.94 1.00 0.95 1.00 4 0.06 0.00 0.97 1.00 0.97 1.00 5 0.08 0.00 0.97 1.00 0.97 1.00 6 0.06 0.00 0.97 1.00 0.97 1.00 Table 6 Precision at 5 (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Patents dataset using a model with 750 topics. LEVEL column indicates the number of hierarchies used. OPEN-RES-100 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 99.8 99.9 45.2 45.9 4.9 2.5 3 99.9 99.9 74.4 77.6 13.4 10.7 4 99.9 99.9 87.4 90.2 27.2 22.8 5 99.9 99.9 95.4 96.3 49.9 42.6 6 99.9 99.9 97.9 98.7 72.2 65.8 Table 7 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Open Research dataset and 100 topics. OPEN-RES-500 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 95.9 96.3 22.2 22.1 1.4 0.3 3 99.1 99.2 43.9 43.7 5.1 4.1 4 99.6 99.6 57.1 57.3 11.7 10.3 5 99.6 99.6 70.7 70.7 28.8 22.0 6 99.9 99.9 81.5 80.6 50.3 40.1 Table 8 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Open Research dataset and 500 topics. C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 11 CORDIS-70 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 99.9 99.9 51.3 56.3 5.1 5.0 3 99.9 99.9 84.8 89.5 10.5 10.6 4 99.9 99.9 96.1 97.6 20.8 19.5 5 99.9 99.9 98.9 99.4 35.0 32.7 6 99.9 99.9 99.7 99.8 53.1 51.2 Table 9 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on CORDIS dataset and 70 topics. CORDIS-150 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 99.9 99.9 40.9 41.2 3.1 2.9 3 99.9 99.9 75.3 76.7 6.2 6.1 4 99.9 99.9 90.0 92.1 12.1 11.8 5 99.9 99.9 96.4 96.9 21.6 20.6 6 99.9 99.9 98.1 98.9 36.5 33.9 Table 10 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on CORDIS dataset and 150 topics. show a similar behaviour to the one offered by the use of brute force by means of JS divergence. In terms of efficiency, we consider the times to com- pare pairs of topic distributions constant, and we focus on the number of comparisons needed. Thus, algorithms with larger candidate spaces will be less efficient than others when the accuracy in both is the same. Tables 7-12 show the percentage of the corpus used by each of the algorithms to discover similar documents. Tables 1-6 show the accuracy of each algorithm for each of these scenarios. Density-based algorithm (dhhm) shows better balance between accuracy and volume of infor- mation (efficiency). It uses smaller samples (i.e lower ratio size) than others in all tests and even when it only uses a subset that is a 6.2% (Table 10) of the entire corpus, it obtains an accuracy of 0.808 (Table 4). The precision achieved by the algorithm based on density (dhhm), which is much more restrictive than the others, suggests that few topics are required to represent a document in order to obtain similar ones. In addition, the number of topics does not seem to influence the per- formance of the algorithms, since their precision values are similar among the datasets of the same corpus. This shows that hashing methods based on hierarchical set of topics are robust to models with different dimensions. The behavior of the algorithms have also been ana- lyzed when the number of topics in the model varies. PATENTS-250 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 99.9 99.9 43.2 32.7 35.1 23.0 3 99.9 100.0 82.4 100.0 78.2 100.0 4 99.9 100.0 96.5 100.0 95.1 100.0 5 99.9 99.9 99.2 100.0 98.9 100.0 6 100.0 100.0 99.8 100.0 99.7 100.0 Table 11 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Patents dataset and 250 topics. PATENTS-750 (data-ratio) LEVEL THHM CHHM DHHM mean median mean median mean median 2 99.9 100.0 35.2 23.6 31.8 19.9 3 99.9 99.9 81.4 99.8 79.6 98.8 4 99.9 99.9 96.5 99.9 95.5 99.5 5 97.7 96.6 99.0 99.9 98.6 99.7 6 99.1 98.6 99.7 99.9 99.5 99.8 Table 12 Data size ratio used (mean and median) of threshold-based (THHM), centroid-based (CHHM) and density-based (DHHM) hierarchical hashing methods on Patents dataset and 750 topics. Models with 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000 topics were created from the CORDIS corpus. For each model, the p@5 of the hashing meth- ods is calculated taking into account the hierarchy lev- els: 2, 3, 4, 5 and 6. Figures 11 to 13 show the results obtained for each algorithm. It can be seen how the performance, i.e precision, of each of the algorithms is not influenced by the dimensions of the model. 4.3. Exploration In a certain domain, we may want to retrieve similar documents to one given. For example, searching for articles in the Biomedical domain that are similar to an article about Semantic Web. In terms of topics this kind of search requires to narrow down the initial search space to a subset with only documents that contain the topics that better describe the queried domain. Existing hashing techniques based on a binary-code Hamming space do not allow to customize the search query beyond the reference document itself. However, the algorithms proposed in this work allow adding new restrictions to the initial query based on the reference document, since they use a hierarchy of set of topics as hash codes. Through the following example we describe the workflow to enable such retrieval operations. For sim- 12 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm Fig. 11. Precision at 5 (mean) of threshold-based hashing method when number of topics varies in CORDIS dataset. Fig. 12. Precision at 5 (mean) of centroid-based hashing method when number of topics varies in CORDIS dataset. Fig. 13. Precision at 5 (mean) of density-based hashing method when number of topics varies in CORDIS dataset. plicity we consider hash expressions with only two hier- archy levels. The reference document d1 has the follow- ing hash expression: h = {h0, h1} = {(t10), (t18)}. The first query, Q1, searches for documents similar to the reference document d1 among all documents in the corpus. One of the ways to formalise this query OPEN-RESEARCH-100 hash q1 q2 ratio thhm 499,755 160,660 67.8 chhm 356,111 1,976 99.44 dhhm 49,068 766 98.43 Table 13 Number of documents similar to a given one (q1) and also in a specific domain (q2) for threshold-based (thhm), centroid-based (chhm) and density-based (dhhm) hierarchical hashing methods. looks like this: Q1 = h0 : t10ˆ100 or h0 : t18ˆ50 or h1 : t10ˆ50 or h1 : t18ˆ100. It sets a maximum boost (100) when the same restrictions as the reference document (t10 in h0 and t18 in h1) are fulfilled, and a lower boost (50) for the others (t18 in h0 and t10 in h1). In the specific case of applying this query to the CORDIS dataset, we observed that most of the retrieved documents included topic t18 (fig 14). But if we were only interested in similar documents to d1 that have topic t10, we could restrict the previous query Q1 to express this condition in the following way: Q2 = (h0 : t10ˆ100 or h1 : t10ˆ50) and (h1 : t10ˆ50 or h1 : t18ˆ100). The result obtained by Q2 (fig 14) shows that the condition has been considered since there is a balance between topics t10 and t18 among the documents similar to d1. This type of restrictions based on the semantics of- fered by topics in the hash expression get enabled thanks to the methods proposed in this work. 5. Conclusions The usefulness of topics created by probabilistic models when exploring collections of scientific articles on large-scale has been widely studied in the literature. Each document in the corpus is described by probability distributions that measure the presence of those topics in their content. These vectors can also be used to mea- sure the similarity between documents by using met- rics such as Jensen-Shannon divergence. But with large amounts of items in the collection, discovering the en- tire set of nearest neighbors to a given document would be infeasible. Due to the low storage cost and fast re- trieval speed, hashing is one of the popular solutions for approximate nearest neighbors. However, existing hash- ing methods for probability distributions only focus on the efficiency of searches from a given document, with- out handling complex queries or offering hints about why one document is considered more similar than an- other. A new data structure is proposed to represent C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm 13 Fig. 14. Most relevant topics in similar documents from using a document as query (Q1) and setting topic t10 as mandatory (Q2). hash codes, based on topic hierarchies created from the topic distributions. This approach has proven to obtain high-precision results and can accommodate additional query restriction. This way of encoding documents can also help to understand why two documents are simi- lar, based on the intersection of topics at hierarchies of relevance. In this paper we have focused on (1) comparing the performance of topic-based hashing methods with re- spect to the distance metrics based on probability distri- butions (e.g. JS divergence), (2) their ability to support more complex queries based on topic-based filters and (3) the expressiveness of their annotations (topics hier- archically divided into groups with different degrees of semantic specificity) to justify the relations obtained. A manually annotated corpus with content similarity relations would further confirm the ability of the met- rics proposed in this paper to reflect similarity as hu- mans perceive it. Ongoing work on this line includes the creation of questionnaires 8 to more accurately capture how similar two documents are from the perspective of human evaluators who read them both. This is an ambi- tious task that need to deals with the evaluators’ own interpretation of similarity. What an expert perceives as different (since his knowledge in the domain allows him to identify discrepancies between the two texts), may be considered as similar by an inexperienced user that might not be able to capture those fine grained differences. The next steps in our research are to extend the met- ric proposed in this paper from the point of view of the perception of similarity that a human makes, and to per- form a more in-depth investigation about the meaning of the topics grouped by levels of relevance. 8http://librairy.linkeddata.es/survey 6. Acknowledgments This research was supported by the Spanish Ministe- rio de Economía, Industria y Competitividad and EU FEDER funds under the DATOS 4.0: RETOS Y SOLU- CIONES - UPM Spanish national project (TIN2016- 78011-C4-4-R) References [1] N. Aletras, T. Baldwin, J. H. Lau, and M. Stevenson. Evaluating topic representations for exploring document collections. Jour- nal of the Association for Information Science and Technology, 68(1):154–167, 2017. [2] N. Aletras and M. Stevenson. Measuring the Similarity between Automatically Generated Topics. In EACL, pages 22–27, 2015. [3] A. Andoni and P. Indyk. Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pages 459–468. IEEE, 2006. [4] A. Andoni, P. Indyk, H. L. Nguyen, and I. Razenshteyn. Beyond Locality-Sensitive Hashing. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, 6 2013. [5] A. Andoni, P. Indyk, H. L. Nguyá˙zˇEn, and I. Razenshteyn. Be- yond Locality-Sensitive Hashing. In Proceedings of the Twenty- Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1018–1028. Society for Industrial and Applied Mathe- matics, Philadelphia, PA, 1 2014. [6] A. Andoni and I. Razenshteyn. Optimal Data-Dependent Hash- ing for Approximate Near Neighbors. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Com- puting - STOC ’15, pages 793–801, New York, New York, USA, 2015. ACM Press. [7] C. Badenes-Olmedo, J. L. Redondo-Garcia, and O. Corcho. Distributing Text Mining tasks with librAIry. In 17th ACM Symposium on Document Engineering (DocEng), 2017. [8] C. Badenes-Olmedo, J. L. Redondo-Garcia, and O. Corcho. Ef- ficient Clustering from Distributions over Topics. In 9th Inter- national Conference on Knowledge Capture (K-CAP), page 8, 2017. [9] C. Badenes-Olmedo, J. L. Redondo-Garcia, and O. Corcho. Large-scale Topic-based Search Java Algorithm, 2019. [10] D. Blei, L. Carin, and D. Dunson. Probabilistic topic models. IEEE Signal Processing Magazine, 27(6):55–65, 2010. [11] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Alloca- tion. Journal of Machine Learning Research, 3(4-5):993–1022, 2003. [12] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic clustering of the Web. Computer Networks and ISDN Systems, 29(8-13):1157–1166, 9 1997. [13] S.-H. Cha. Comprehensive Survey on Distance/Similarity Mea- sures between Probability Density Functions. International Journal of Mathematical Models and Methods in Applied Sci- ences, 1(4):1–8, 2007. [14] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing - STOC ’02, page 380, New York, New York, USA, 2002. ACM Press. 14 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithm [15] X. Cheng, X. Yan, Y. Lan, and J. Guo. BTM : Topic Modeling over Short Texts. IEEE Transactions on Knowledge and Data Engineering, 26(12):2928–2941, 2014. [16] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality- sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computa- tional geometry - SCG ’04, page 253, New York, New York, USA, 2004. ACM Press. [17] S. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. Harshman. Indexing by Latent Semantic Analysis. JASIS, 41(6):391–407, 1990. [18] D. Endres and J. Schindelin. A new metric for probability distri- butions. IEEE Transactions on Information Theory, 49(7):1858– 1860, 7 2003. [19] C. J. Gatti, J. D. Brooks, and S. G. Nurre. A Historical Analysis of the Field of OR/MS using Topic Models. CoRR, abs/1510.0, 2015. [20] D. Greene and J. P. Cross. Exploring the political agenda of the european parliament using a dynamic topic modeling approach. Political Analysis, 25(1):77–94, 2016. [21] T. L. Griffiths and M. Steyvers. Finding scientific topics. Pro- ceedings of the National Academy of Sciences of the United States of America, 101 Suppl:5228–35, 2004. [22] T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. Topics in semantic representation. Psychological Review, 114(2):211– 244, 2007. [23] D. Hall, D. Jurafsky, and C. D. Manning. Studying the History of Ideas Using Topic Models. Association for Computational Linguistics, 2008. [24] J. He, L. Li, and X. Wu. A self-adaptive sliding window based topic model for non-uniform texts. In Proceedings - IEEE International Conference on Data Mining, ICDM, volume 2017- Novem, pages 147–156, 2017. [25] T. Hofmann. Probabilistic Latent Semantic Indexing. SIGIR, pages 50–57, 1999. [26] L. K. Huang, Q. Yang, and W. S. Zheng. Online Hashing. IEEE Transactions on Neural Networks and Learning Systems, 29(6):2309–2322, 2018. [27] P. Indyk and R. Motwani. Approximate nearest neighbors. In Proceedings of the thirtieth annual ACM symposium on Theory of computing - STOC ’98, pages 604–613, New York, New York, USA, 1998. ACM Press. [28] J. Ji, J. Li, S. Yan, Q. Tian, and B. Zhang. Min-Max Hash for Jaccard Similarity. In 2013 IEEE 13th International Conference on Data Mining, pages 301–309. IEEE, 12 2013. [29] K. Krstovski and D. A. Smith. A Minimally Supervised Ap- proach for Detecting and Ranking Document Translation Pairs. In Workshop on Statistical MT, 2011. [30] K. Krstovski, D. A. Smith, H. M. Wallach, and A. McGregor. Efficient Nearest-Neighbor Search in the Probability Simplex. In Proceedings of the 2013 Conference on the Theory of Infor- mation Retrieval - ICTIR ’13, pages 101–108, New York, New York, USA, 2013. ACM Press. [31] B. Kulis and K. Grauman. Kernelized Locality-Sensitive Hash- ing. IEEE Transactions on Pattern Analysis and Machine Intel- ligence, 34(6):1092–1104, 6 2012. [32] P. Li and C. König. b-Bit minwise hashing. In Proceedings of the 19th international conference on World wide web - WWW ’10, page 671, New York, New York, USA, 2010. ACM Press. [33] P. Li, A. B. Owen, and C.-H. Zhang. One Permutation Hashing. Advances in Neural Information Processing, 2012. [34] W. Liu, C. Mu, S. Kumar, and S.-F. Chang. Discrete Graph Hashing. NIPS, 2014. [35] W. Liu, C. Mu, S. Kumar, and S.-F. Chang. Discrete Graph Hashing. Advances in Neural Information Processing Systems, pages 3113–3121, 2014. [36] Y. Liu, J. Cui, Z. Huang, H. Li, and H. T. Shen. SK-LSH. An efficient index structure for Approximate Nearest Neighbor Search. Proceedings of the VLDB Endowment, 7(9):745–756, 5 2014. [37] N. Ljubeši´c, D. Boras, N. Bakari´c, and J. Njavro. Comparing measures of semantic similarity. Proceedings of the Interna- tional Conference on Information Technology Interfaces, ITI, pages 675–681, 2008. [38] H.-m. Lu, C.-p. Wei, and F.-y. Hsiao. Modeling healthcare data using multiple-channel latent Dirichlet allocation. JOURNAL OF BIOMEDICAL INFORMATICS, 60:210–223, 2016. [39] X. Mao, B.-S. Feng, Y.-J. Hao, L. Nie, H. Huang, and G. Wen. S2JSD-LSH: A Locality-Sensitive Hashing Schema for Proba- bility Distributions. In AAAI, 2017. [40] A. McCallum. MALLET: A Machine Learning for Language Toolkit, 2002. [41] A. Niekler and P. Jähnichen. Matching results of latent dirichlet allocation for text. In Proceedings of ICCM, pages 317–322, 2012. [42] R. Oâ ˘A´ZDonnell, Y. Wu, and Y. Zhou. Optimal Lower Bounds for Locality-Sensitive Hashing (Except When q is Tiny). ACM Transactions on Computation Theory, 6(1):1–13, 3 2014. [43] J. Oâ ˘A´ZNeill, C. Robin, L. Oâ ˘A´ZBrien, and P. Buitelaar. An analysis of topic modelling for legislative texts. CEUR Work- shop Proceedings, 2143, 2017. [44] M. Paul and R. Girju. Topic Modeling of Research Fields: An Interdisciplinary Perspective. In Recent Advances in Natural Language Processing, pages 337–342, 2009. [45] M. J. Paul and M. Dredze. Discovering health topics in social media using topic models. PLoS ONE, 9(8), 2014. [46] S. Petrovic, M. Osborne, and V. Lavrenko. Streaming First Story Detection with application to Twitter. NAACL, 2010. [47] D. Ramage, S. Dumais, and D. Liebling. Characterizing Mi- croblogs with Topic Models. Icwsm, pages 1–8, 2010. [48] D. Ramage, E. Rosen, J. Chuang, C. D. Manning, and D. A. McFarland. Topic Modeling for the Social Sciences. In Twenty- Third Annual Conference on Neural Information Processing Systems, pages 1–4, 2009. [49] D. Ravichandran, P. Pantel, and E. H. Hovy. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. ACL, 2005. [50] M. D. Tapi Nzali, S. Bringay, C. Lavergne, C. Mollevi, and T. Opitz. What Patients Can Tell Us: Topic Analysis for Social Media on Breast Cancer. JMIR medical informatics, 5(3):e23, 7 2017. [51] K. Terasawa and Y. Tanaka. Spherical LSH for Approximate Nearest Neighbor Search on Unit Hypersphere. In Algorithms and Data Structures, pages 27–38. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007. [52] W. B. Towne, C. P. Rosé, and J. Herbsleb. Measuring Similarity Similarly: LDA and Human Perception. ACM Transactions on Intelligent Systems and Technology ACM Reference Format ACM Trans. Intell. Syst. Technol, 7(2):1–25, 2016. [53] S. Vijayanarasimhan, P. Jain, and K. Grauman. Hashing Hyper- plane Queries to Near Points with Applications to Large-Scale Active Learning. IEEE Transactions on Pattern Analysis and "
"","An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts Carlos Badenes-Olmedo1, Jos´e Luis Redondo-Garc´ıa2, and Oscar Corcho1 1 Universidad Polit´ecnica de Madrid, Ontology Engineering Group, Spain {cbadenes, ocorcho}@fi.upm.es 2 Amazon Research, Cambridge UK jluisred@amazon.com Abstract. Summaries and abstracts of research papers have been tra- ditionally used for many purposes by scientists, research practitioners, editors, programme committee members or reviewers (e.g. to identify relevant papers to read or publish, cite them, explore new fields and disciplines). As a result, many paper repositories only store or expose abstracts, what may limit the capacity of finding the right paper for a specific research purpose. Given the size limitations and the concise nature of abstracts, they usu- ally omit explicit references to some contributions and impacts of the paper. Therefore for certain information retrieval tasks they cannot be considered as the most appropriate excerpt of the paper to base these operations on. In this paper we have studied other kinds of summaries, built upon textual fragments falling under certain categories of the sci- entific discourse, such as outcome, background, approach, etc, in order to decide which one is more appropriate in order to substitute the origi- nal text. In particular, two novel measures are proposed: (1) internal- representativeness, which evaluates how well a summary describes what the full-text is about and (2) external-representativeness, which evaluates the potential of a summary to discover related texts. Results suggest that summaries explaining the method of a scientific article express a more accurate description of the full-content than oth- ers. In addition, more relevant related articles are also discovered from summaries describing the method, together with those containing the background knowledge or the outcomes of the research paper. 1 Introduction In this paper we present our first steps on the analysis of the quality of research article summaries. Our goal is to find the strengths and weaknesses of approaches leveraging exclusively on abstracts against those based on scientific discourse cat- egories such as approach, challenge, background, outcomes and future work. Since the main contributions and impacts of a research article are not always included explicitly in the abstract, as in the case of [9] describing an architecture, where details about the model architectures are missing, they cannot always be con- sidered as the most adequate scientific summary of a research paper. In order to judge on this accuracy, two novel measures are proposed based on the capability of the summary to substitute the original paper: (1) internal-representativeness, which evaluates how well the summary represents the original full-text and (2) external-representativeness, which evaluates the summary according to how the summary is able to produce a set of related texts that are similar to what the original full-text has triggered. The paper is organized as follows: Section 2 highlights recent studies on text mining research articles and presents the steps followed to measure the repre- sentativeness of abstracts and research article summaries based on rhetorical categories. It describes both the classifier used to identify those categories in papers and the representational model and similarity metric used to compare textual units. Experimental results comparing the different kind of summaries are shown in Section 3. Finally, Section 4 presents conclusions from the experi- ments. 2 Background and Approach Recent studies [16][13] have shown that text mining of full research articles give consistently better results than using only their corresponding abstracts. Given the size limitations and concise nature of abstracts, they often omit descriptions or results that are considered to be less relevant but still are important in certain Information Retrieval (IR) tasks. Thus, when other researchers cite a particular paper, 20% of the keywords that they mention are not present in the abstract [6]. In this paper, we show our initial analysis about the representativeness of research article summaries, considering those based exclusively on abstracts and those based on their discursive structure (approach, challenge, background, out- comes and future work)[14]. The representativeness of a summary with respect to the original full-text is defined as the degree of relation with the original one (internal-representativeness), along with the capacity of mimicking the full text when finding related items (external-representativeness). In order to quantify this notions of internal-external representativeness, a probabilistic topic model is trained over the entire set of papers to have a vectorial representation of each text retrieved from a paper: full content-based and summary-based. The vecto- rial representations of full-papers is used to measure the distance between them and those derived from abstract or summaries (internal-representativeness), and also to find similar documents (external- representativeness) based on the dis- tance between their vectorial representations. An upper distance threshold is specified to filter less similar pairs and compose a set of related papers for each paper. Then, a comparison in terms of precision and recall is performed between sets obtained by only using the vectorial representation of full-papers, against sets produced by using other kind of summaries. 2.1 Annotation of Rhetorical Discourse Parts First of all, we need to identify the rhetorical parts of a research paper. Some approaches have been proposed to summarize scientific articles [4] taking advan- tage of the citation context and the document discourse model. We have used the scientific discourse annotator proposed by [11] to automatically create sum- maries from scientific articles by classifying each sentence as belonging to one of the following scientific discourse categories: approach, challenge, background, outcomes and future work. These categories were identified from the schemata proposed by [15] with the original purpose of characterizing the content of Com- puter Graphics papers. The annotator is based on a Support Vector Machine classifier that combines both lexical and syntactic features to model each sen- tence in a paper. This tool3 was integrated in the librAIry [1] Rhetoric Module4 to automatically annotate research papers with their rhetorical content. 2.2 Representational Model A representational model is required not only to measure distances between text fragments but, more importantly, to help to understand the differences in their content. Topic models are widely used to uncover the latent semantic structure from text corpora. In particular, Probabilistic Topic Models represent documents as a mixture of topics, where topics are probability distributions over words. Latent Dirichlet Allocation (LDA)[2] is the simplest generative topic model that adds Dirichlet priors for the document-specific topic mixtures, making it possible to characterize documents not previously used during the training task. This is a key feature for our evaluations because, although the model used for the experiments will be trained from the full-content of papers, it will be also used to describe the texts summaries. Thus, we have used a LDA model to describe the inherent topic distribution of papers in the corpus. Some hyper-parameters need to be estimated: the number of topics (k), the concentration parameter (α) for the prior placed on documents’ distributions over topics and the concentration parameter (β) for the prior placed on topics distributions over terms. Since the target of this experiment is not to evaluate the quality of the representational model, but to compare their topic distributions, we accepted as valid values those widely used in the literature: α = 0.1, β = 0.1 , and k = 2 ∗ � n/2 = 44 where n is the size of the corpus. Similarity Measure Feature vectors in Topic Models are topic distributions expressed as vectors of probabilities. Hence we opt for Jensen-Shannon diver- gence (JSD)[8] instead of the commonly used Kullback-Liebler divergence (KLD). The reason for this is that KLD (1) is not defined when a topic distribution is zero and (2) is not symmetric, what does not fit well with semantic similarity measures which in general are symmetric [12]. JSD considers the average of the distributions as follows : 3 http://backingdata.org/dri/library/ 4 https://github.com/librairy/annotator-rhetoric JSD(p, q) = T � i=1 pi ∗ log 2 ∗ pi pi + qi + T � i=1 qi ∗ log 2 ∗ qi qi + pi (1) where T is the number of topics and p, q are the topics distributions. And the similarity measure used in our analysis is based on the JSD trans- formed into a similarity measure as follows [5] : similarity(Di, Dj) = 10−JSD(p,q) (2) where Di, Dj are the documents and p, q the topics distributions of each of them. 3 Experiments The corpus used in the experiments was created by combining journals in differ- ent scientific domains such as Advances in Space Research, Procedia Chemistry, Journal of Pharmaceutical Analysis and Journal of Web Semantics. In total 1,000 papers were added, 250 from each journal. Both the abstract and the full- content of these documents were directly retrieved from the Elsevier API 5 by using the librAIry [1] Harvester module 6. The code used to perform the analysis along with the results obtained are available in GitHub7. Since the annotation process to automatically discovers the rhetorical parts of a research paper (Section 2.1) is sensitive to the structure of the phrases that are used when writing the text, only 20% of papers in the corpus could be fully annotated with all the fragments considered. In fact, these categories are not present in the same proportion in the corpus: approach (90%), background (78%), outcome (73%), challenge (57%) and future work (21%) 3.1 Internal Representativeness The internal-representativeness of a summary measures the similarity of this summary against the original full-text research paper. This similarity is based on the JSD between the topic distribution of each of them. Since LDA considers documents as bag-of-words, the text length (e.g. full- content or summaries) affects the accuracy of the topic distributions inferred by the topic model described in Section 2.2. The occurrences of words in short texts are less discriminative than in long texts where the model has more word counts to know how words are related [7]. In view of the above, the approach, the background and the outcome content of a paper generate more accurate topic distributions than those created from other approaches such as the abstract. Also, the relative presence of each of them in a paper (figure 2) shows an un- expected result when compared to the IMRaD format [10]. This style proposes to distribute the content of an abstract, and by extension the full-paper, as follows: Introduction(25%), Methods(25%), Results(35%) and Discussion(15%). 5 https://dev.elsevier.com 6 https://github.com/librairy/harvester-elsevier 7 https://github.com/librairy/study-semantic-similarity Fig. 1. length of summaries Fig. 2. relative size of parts of an article However, the results (figure 2) show that Method section (approach content) is more extensive than Results section (outcome content) in our corpus. All pairwise similarities between full-papers, abstracts and rhetorical-based summaries are calculated to measure the internal-representativeness of a summary with respect to the original text, i.e. the topic-based similarity value (equation 2) between the probability distributions of the full-text and each of the summaries. Results (table 1) suggest than summaries created from the ap- proach content are more representative than others, i.e. the distribution of topics describing the text created from the approach content is the most similar to the one corresponding to the full-content of the paper. Min Lower Quartile Upper Quartile Max Dev Median abstract 0.0489 0.9109 0.9840 1.0000 0.1443 0.9741 approach 0.0499 0.9969 1.0000 1.0000 0.0872 0.9998 background 0.0463 0.8967 0.9937 0.9988 0.2037 0.9822 challenge 0.0426 0.7503 0.9517 0.9940 0.2224 0.8829 futureWork 0.0000 0.6003 0.9435 0.9948 0.2842 0.8814 outcome 0.0485 0.9267 0.9925 0.9990 0.1721 0.9835 Table 1. Internal-Representativeness 3.2 External-Representativeness The external-representativeness metric tries to measure how different is the set of related documents obtained from summaries with respect to those derived from the original full-text. In terms of precision, recall and f-measure, a comparison has been performed to analyze the behavior of the summaries when trying to discover related content compared to use the full-text of the article. By using the same topic model previously created, similarities among all pairs of documents were also calculated according to equation 2. Then, a minimum score or similarity threshold is required to define when a pair of papers are related. Each threshold is used to create a gold-standard which relates articles to others based on their similarity values. In order to discover that lower bound of similarity, a study about trends in the similarity scores (fig 3) as well as distributions of topics in the corpus (fig 4) was performed. We can see that topics are not equally balanced across papers. This fact generates separated groups of strongly related papers. We think this phenomena is due to our usage of a corpus created from journals where different domains are equally balanced. Then, we considered a similarity score equals to 0.99 (fig 3) as the threshold from which strong relations appear. However, to cover different interpretations of similarity, from those based on sharing general ideas or themes to those that imply to share a more specific content, the following list of thresholds was considered in the experiments: 0.5, 0.6, 0.7, 0.8, 0.9, 0.95 and 0.99. Fig. 3. number of pairwises by similarity score (rounded up to two decimals) Fig. 4. topics per article with value above 0.5 For each similarity threshold, a gold-standard was created based on con- sidering as related those papers with a similarity value upper than the selected threshold. Results ( figure 5) comparing the related papers inferred from the full- content with those inferred from the partial-content representation (i.e. abstract or rhetorical parts) suggest that strongly related papers are mainly discovered by using the summary created from the approach section. The reason for this may be based on the average size of this type of summaries or the particular content included in this part of a paper. While other summaries include more general-domain words, the approach content includes more specific words that describe the method or the final objective of the paper. So, for higher similarity thresholds, i.e. for strongly related papers, the recommendations discovered by using the approach are more precise than those discovered by using the abstract. In terms of recall (figure 6), the upward trend followed by the approach, the outcome and the background content remarks the assumption of summaries con- taining key words allow to discover more similar papers than others. Moreover, since recall overlooks false-negatives classifications, it suggests that these parts of a research paper share more words than others with strongly related papers but they may also present commonalities with highly related papers, except in case of approach which still exhibits higher precision. As expected, only summaries created from the approach, the outcome and the background content maintain high accuracy values (fig 7) even for high similarity Fig. 5. P at different similarity thresholds Fig. 6. R at different similarity thresholds thresholds. Along with the results showed in figure 8, where the same three rhetorical classes present the lowest standard deviation over the f-measure, they can be considered as the most robust summaries containing the ideas that better characterize the paper compared to others. Fig. 7. f-measure VS similarity thresholds Fig. 8. σ of the f-measure 4 Conclusions and Future Work We have studied the Topic-based similarities among scientific documents based on their abstract sections with respect to summaries corresponding to their sci- entific discourse categories. For this purpose, two novel measures have been pro- posed: (1) internal-representativeness and (2) external-representativeness. Results show that summaries created from the approach, outcome or back- ground content of a paper describe more accurately its full-content in terms of overall ideas and related documents than abstracts. Although those summaries are more extensive in number of characters than other with similar precision such as the abstract content, they have proven to be particularly helpful discovering strongly related papers, i.e. papers with a similarity value close to 1.0. In order to avoid an influence of the size of the summaries on the accuracy of the results, in future work we plan to use probabilistic topic model algorithms oriented to handle short-texts such as BTM [3] to describe texts. "
"Efficient Clustering from Distributions over Topics","E￿icient Clustering from Distributions over Topics Carlos Badenes-Olmedo cbadenes@￿.upm.es Ontology Engineering Group Universidad Polit´ecnica de Madrid Boadilla del Monte, Spain Jos´e Luis Redondo-Garc´ıa jluisred@amazon.com Amazon Research Cambridge, UK Oscar Corcho ocorcho@￿.upm.es Ontology Engineering Group Universidad Polit´ecnica de Madrid Boadilla del Monte, Spain ABSTRACT ￿ere are many scenarios where we may want to ￿nd pairs of tex- tually similar documents in a large corpus (e.g. a researcher doing literature review, or an R&D project manager analyzing project proposals). To programmatically discover those connections can help experts to achieve those goals, but brute-force pairwise com- parisons are not computationally adequate when the size of the document corpus is too large. Some algorithms in the literature divide the search space into regions containing potentially simi- lar documents, which are later processed separately from the rest in order to reduce the number of pairs compared. However, this kind of unsupervised methods still incur in high temporal costs. In this paper, we present an approach that relies on the results of a topic modeling algorithm over the documents in a collection, as a means to identify smaller subsets of documents where the simi- larity function can then be computed. ￿is approach has proved to obtain promising results when identifying similar documents in the domain of scienti￿c publications. We have compared our approach against state of the art clustering techniques and with di￿erent con￿gurations for the topic modeling algorithm. Results suggest that our approach outperforms (> 0.5) the other analyzed techniques in terms of e￿ciency. CCS CONCEPTS •Mathematics of computing ! Probability and statistics ; •Information systems ! Document topic models; •Applied computing ! Document management and text processing; KEYWORDS topic models; semantic similarity; large-scale text analysis; schol- arly data 1 INTRODUCTION Given the huge amount of information about any domain that is being produced or captured daily, it becomes crucial to provide mechanisms for automatically identifying the elements that can bring value for the involved agents (general consumers, experts, ￿is work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R, ￿nanced by the Spanish Ministry MINECO and co-￿nanced by FEDER.. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro￿t or commercial advantage and that copies bear this notice and the full citation on the ￿rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi￿ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a fee. Request permissions from permissions@acm.org. K-CAP 2017, Austin, TX, USA © 2017 ACM. 978-1-4503-5553-7/17/12...$15.00 DOI: 10.1145/3148011.3148019 companies, investors…) and discard the noisy, non-relevant infor- mation. Much of the information is presented in the form of textual documents, making necessary for experts to browse through many of these texts to ￿nd relevant data. A way to explore the knowledge inside collection of documents is by moving from one information element to another based on certain criteria that relates them. ￿is approach requires to calculate a similarity matrix with all possible comparisons between elements, so we can later select the most pertinent ones. Since computing a n ⇥ n matrix takes O(n2) time, obtaining all possible pairs of similarities in a large collection of documents can be unfeasible because of the exponential cost of comparing every pair of elements. Our work is derived from a real need in the domain of digital libraries, where we targeted the task of ￿nding relations among texts based on similar content inside a corpus containing 7,487 digital books and 97,532 chapters (104,960 documents in total). Since the time consumed in calculating the similarity score between two documents was t = 7.62 ⇤ 10�4 seconds in a 15x CPU@2.30Ghz and 64GB RAM server, the total time to compute all combinations over the whole corpus went up to around 5 days. Considering that other tasks leveraging the entire collection such as training a Topic Model only required 48 minutes to be executed, calculating the similarity scores between pairs of documents becomes a signi￿cant bo￿leneck when making sense of big collections of documents. One possible way of ￿nding similarity-based links between pair of documents, is to 1) process the items following di￿erent anno- tation techniques (entities, keywords, etc) that allow machines to programmatically leverage on their content. 2) create a vectorial representation based on those features for each document and 3) compare them following some distance/divergence functions [21]. In order to reduce the execution time, some approaches have introduced mechanisms (mainly clustering algorithms and pre- election methods) to alleviate the problem of making this calculation over the whole set of pairs in the collection. However those methods are still quite costly. A novel clustering technique based on topic model distributions is proposed in this paper, in order to reduce the required time to ￿nd relations between documents in a large corpus of textual documents without compromising e￿ciency. We leverage on Probabilistic Topic Models (PTM) [6] as represen- tational models and, in particular, Latent Dirichlet Allocation (LDA) [10] as the way to make this process of ￿nding relations among documents in a corpus more agile and computationally feasible. Probabilistic Topic Modeling techniques [7] are statistical methods that analyze the words of the original texts to discover the themes that run through them. Based on these insights, we can further study how those subjects are connected to each other, and how they change over time. Originally developed as a text-mining tool, K-CAP 2017, December 4–6, 2017, Austin, TX, USA C. Badenes-Olmedo et al. topic models are now being used to detect instructive structures in data [5] such as computer vision to classify images [24], connect images and captions [8], or build image hierarchies [4] [22]; popu- lation genetics [25], and social networks [20]. LDA reduces each document to a vector composed by a ￿xed set of real numbers, each of which represents a probability distribution of a given topic. One of the main advantages is that PTM’s do not require any prior annotations or labeling of the documents. ￿e topics emerge, as hidden structures, from the analysis of the original texts. ￿e topics produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and dis- cover, based on the statistics of the words contained in each, what the topics might be and what is the topic balance for each docu- ment. ￿ose topics o￿er a much more intuitive, yet sophisticated way of performing knowledge discovery tasks in big collections of documents. In contrast to existing unsupervised approaches based on cen- troids or density measures, our algorithm relies on the outcomes of PTM’s to assign each document to a cluster without having to consider the other elements in the corpus. ￿us, it only takes O(n) time to compute all clusters. In the following section, we provide an overview of the problem to be solved along with existing solutions. A￿er that, a detailed description of our algorithm is given in Section 3. We then (Sec- tion 4) experimentally verify the e￿ciency and e￿ectiveness of our clustering algorithms using real data, and demonstrate that our approach is competitive enough against both a centroid-based and a density-based clustering baselines. Finally, the most relevant results and conclusions are presented together with some future lines work in Section 5. 2 BACKGROUND Traditional retrieval tasks over large collections of textual docu- ments [18] highly rely on individual features like term frequencies (TF-IDF). However, new ways of characterizing documents based on the automatic generation of models surfacing the main subjects covered in the corpus have been developed during recent years. Probabilistic Topic Modeling [6] algorithms are statistical methods that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, or how they change over time. Probabilistic topic models do not require any prior annotations or labeling of the documents. ￿e topics emerge, as hidden structures, from the analysis of the original texts. ￿ese structures are topics distributions, per-resource topic distributions or per-resource per- word topic assignments. In turn, a topic is a distribution over terms that is biased around those words associated to a single theme. ￿is interpretable hidden structure annotates each resource in the collection and these annotations can be used to perform deeper analysis about relationships between resources. In this way, topic modeling provides us an algorithmic solution to organize and annotate large collections of textual documents according to their topics. ￿e simplest generative topic model is Latent Dirichlet Alloca- tion (LDA) [10]. ￿is and other topic models such as Probabilistic Latent Semantic Analysis (PLSA) [19] are part of the ￿eld known as probabilistic modeling. ￿ey are well-known latent variable models for high dimensional data, such as the bag-of-words representa- tion for textual data or any other count-based data representation. While LDA has roots in Latent Semantic Analysis (LSA) [14] and PLSA (it was proposed as a generalization of PLSA), it was also in￿uenced by the generative Bayesian framework to avoid some of the over-￿￿ing issues that were observed with PLSA. ￿is statistical model tries to capture the intuition that docu- ments can exhibit multiple topics. Each document exhibits each topic in di￿erent proportion, and each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over topics. All the documents in the collection share the same set of topics, but each document exhibits these topics in a di￿erent proportion. Documents are rep- resented as a vector of counts with W components, where W is the number of words in the vocabulary. Each document in the corpus is modeled as a mixture over K topics, and each topick is a distribution over the vocabulary of W words. Formally, a topic is a multinomial distribution over words of a ￿xed vocabulary representing some concept. Each topic is drawn from a Dirichlet distribution with parameter �, while each document’s mixture is sampled from a Dirichlet distribution with parameter �. ￿ese two priors, � and �, are also known as hyper-parameters and they are estimated following some heuristic. A Dirichlet distribution is a continuous multivariate probabil- ity distribution parameterized by a vector of positive reals whose elements sum to 1. It is continuous because the relative likelihood for a random variable to take on a given value is described by a probability density function, and also it is multivariate because it has a list of variables with unknown values. In fact, the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution. Unlike a restrictive clustering model, where each document is assigned to one cluster, LDA allows documents to exhibit multiple topics. Moreover, since LDA is unsupervised, the topics covered in a set of documents are discovered from the own corpus; the mixed-membership assumptions lead to sharper estimates of word co-occurrence pa￿erns. 2.1 Similarity Measures Across Documents In a Topic Model the feature vector is a topic distribution expressed as vector of probabilities. Taking into account this premise, the similarity between two topic-based resources will be based on the distance between their topic distributions, which can be also seen as two probability mass functions. A commonly used metric is the Kullback-Liebler (KL) divergence. However, it presents two major problems: (1) when a topic distribution is zero, KL divergence is not de￿ned and (2) it is not symmetric, which does not ￿t well with semantic similarity measures that are usually symmetric [27]. Jensen-Shannon (JS) divergence [26][23] solves these problems considering the average of the distributions as below [12]: �S(p,q) = K ’ i=1 pi ⇤ log 2 ⇤ pi pi + qi + K ’ i=1 qi ⇤ log 2 ⇤ qi qi + pi (1) E￿icient Clustering from Distributions over Topics K-CAP 2017, December 4–6, 2017, Austin, TX, USA where K is the number of topics and p,q are the topics distributions It can be transformed into a similarity measure as follows [13] : sim�S(Di, Dj) = 10��S(p,q) (2) where Di, Dj are the documents and p,q the topic distributions of each of them. Hellinger (He) distance is also symmetric and is used along with JS divergence in various ￿elds where a comparison between two probability distributions is required [9] [17] [11]: He(p,q) = 1 p 2 · v u t K ’ i=1 (ppi � pqi)2) (3) It can be transformed into a similarity measure by subtracting it from 1 [27] such that a zero distance means max. similarity score and vice versa: simHe(Di, Dj) = 1 � He(p,q) (4) 3 THE APPROACH Our algorithms draw inspiration from other clustering techniques to divide the initial space of elements into smaller sub-groups where the complexity of calculating all possible distances is signi￿cantly reduced. Existing unsupervised approaches based on centroids or density measures require to make comparisons between elements to ￿nd groups of similar elements in the collection. ￿ey normally follow an iterative methodology to produce the ￿nal solution, based on calculating distances between the elements inside each interme- diate state. A na¨ıve approach would need to calculate all possible distances between elements, which takes O(n2) time for a n ⇥ n matrix. ￿at makes it impossible to apply such techniques on large collections of documents, since the cost of comparing each element with the others escalates quickly. For those big volumes of data, a clustering task that only takes linear time to discover the clusters can signi￿cantly alleviate this problem. For example, a classi￿cation method that does not require any other data except the element information to assign the item to the corresponding cluster will take O(n) time to compose those groups. ￿e classi￿cation method needs to take advantage of both the vectorial representations of the documents and the similarity mea- sure used to relate them in a corpus. Since the representational model considered is based on Probabilistic Topic Models (and more speci￿cally on LDA), the classi￿cation method leverages on the particular behavior of Dirichlet distributions, which describes each document by a density vector where the sum of all the probability values must be equal to 1.0. ￿us, analyzing the relations between the topics that compose a topic distribution becomes more impor- tant than comparing their probability values with another topic distribution. Our hypothesis is that, given a collection of topic distributions, an unsupervised classi￿cation with high precision and linear com- puting time can be performed by considering only the topic distri- bution of each document and without needing to further compare it with other document’s distributions. All algorithms have been compared in terms of cost, e￿ective- ness and e￿ciency [16]. Cost is based on the number of pairwise similarity values. E￿ectiveness handles relevance measures such as precision and recall. And e￿ciency tries to measure the overall balance between cost and e￿ectiveness. More details about those measures will be included in Section 4. 3.1 Trends-based Clustering Topic distributions are formalized as probability distributions fol- lowing a Dirichlet distribution, so their probability values sum to 1. In this way, the relevance of a topic is in￿uenced and at the same time in￿uences the relevance of the others items in the distribution. Our ￿rst approach named Trends on Dirichlet distribution-based Clustering (TDC) considers changes in the relevance, i.e. proba- bility values of the topics instead of directly relying on the scores associated to a given topic distribution. It expresses the oscillations between topic weights considering a ￿xed order between them. ￿e order can be any, as long as it remains constant in all distributions. ￿us, a probability-vector composed by n density values is trans- lated to a trend-expression made out of n � 1 trend-values such as (1) upward, (2) downward and (0) sustained. ￿is trend-expression will identify the cluster the distribution falls into, and therefore the corresponding item belongs to. TDC is de￿ned as: TDC(P) = T (5) where: Ti = 1, when Pi < Pi+1 Ti = 2, when Pi > Pi+1 Ti = 0, when Pi = Pi+1 For example, given the distribution P1 = [0.23, 0.18, 0.33, 0.13, 0.13], the assigned cluster will be T = 2120. ￿e ￿rst value is 2 because 0.23 is greater than 0.18 (same for other values). 3.2 Ranking-based Clustering We propose a clustering technique named Ranking on Dirichlet distribution-based Clustering (RDC) that only considers the top n topics from the ranked list of probability distributions to classify similar topic distributions. It is based on the focal document selec- tion proposed by [29] to validate LDA-based similarity algorithms against human perception of similarity. RDC is de￿ned as: RDC(P) = R (6) where 8i 2 R,Ri >= Ri+1 and 8j 2 P,R1 >= Pj ￿is is based on the assumption that the highest weighted topics have a high in￿uence in the rest of topics in terms of calculating distances, when comparing continuous multivariate probability distributions. Since similarity measures (Section 2.1) based on prob- ability distributions are oriented to determine the uncertainty of the distribution, when a mixture of probability distributions is con- sidered, as in the case of Topic Models, the top n distributions (i.e. the most relevant topics) should be su￿cient to allow us grouping similar distributions. Taking into account the above considerations, the RDC algorithm classi￿es a topic distribution according to only n highest probability values. For instance, given the following topic distribution: P2 = [0.23, 0.18, 0.33, 0.13, 0.13], the assigned cluster is 3 from RDC-1 because that is the topic with the highest weight. K-CAP 2017, December 4–6, 2017, Austin, TX, USA C. Badenes-Olmedo et al. 3.3 Cumulative Ranking-based Clustering A variant of the previous algorithm, named Cumulative Ranking on Dirichlet distribution-based Clustering (CRDC), also aims to discover the most representative topics that can help to group similar topic distributions. While RDC is based on a ￿xed number of topics, CRDC is based on the cumulative sum of the weights of the highest topics. ￿e number of topics is now dynamically determined by a threshold, and once this threshold is reached no more topics are considered. CRDC is de￿ned as: CRDC(P) = C (7) where 8i 2 C,Ci >= Ci+1 and TÕ i=1 Ci >= w with T size of C, and w a cumulative weight threshold. For instance, considering a CRDC algorithm considering a cumu- lative weight threshold of 0.9, and the following topic distribution: P3 = [0.36, 0.58, 0.05, 0.01]. ￿e assigned cluster will be 2—1. To come up with this cluster, a ranked list of topics based on their weights is ￿rst calculated, Rp = 2|1|3|4. ￿en, a sum of weights according to the order described by Rp is performed. When the accumulated sum is greater than the threshold, the topics taking part of the sum will be selected to “label” the cluster. In this case, the cumulative weight threshold is 0.9 therefore using only the ￿rst two topics we exceed the threshold: w = 0.58 + 0.36 = 0.94 4 EXPERIMENTS In this section we present the experimental setup for evaluating our trends-based (TDC), ranking-based (RDC) and cumulative ranking- based (CRDC) clustering approaches, considering both JS diver- gence and He distance as similarity measures. We describe the datasets and baseline algorithms that will be used for comparison. 4.1 Datasets We used two datasets to evaluate the performance of the algo- rithms. ￿e ￿rst dataset, DIRICHLET-RANDOM-MIXTURE (DRM), is synthetic [2]. To generate the dataset, we sampled k probabilis- tic distributions from a randomly k-dimensional selector based on Dirichlet distributions. ￿is implies that all probabilities must to sum to 1 for each sampled point. ￿e number of sampled points from this mixture of Dirichlet distributions is n = 1000. ￿e second dataset has been created from a collection of research papers published in the Advances in Engineering So￿ware (AIES) journal. ￿ey were retrieved from the Springer API by using the li- brAIry [1] framework and a Topic Model based on LDA was created from them. ￿e sample is also composed by n = 1000 documents. Topic models were trained from these datasets by using the cri- teria described by [28]: � = 50/k , � = 0.01 and k = 2 ⇤ ( p (n/2)), where k is the number of topics and n is the number of docu- ments. Since both datasets contain 1000 documents (n), the hyper- parameters � and � are assigned as follow: � = 1.136, and � = 0.01, and the number of topics is ￿xed to k = 44. Further tuning of the se￿ings is not crucial in this evaluation process, because we are not focusing on the quality of the model but on the e￿ciency when calculating similarities from their representational distributions. Figure 1: Similarity values grouped by frequency in AIES 4.2 Similarity ￿reshold Since there is no uni￿ed criteria to select a threshold inside the distance scores spectrum that allows us to determine when two documents are similar, we decided to study the distribution of simi- larity values calculated from all pairwise comparisons. In Figure 1, the result of grouping all similarities by the two most representative decimals, i.e. the ￿rst two decimals of the similarity value, is shown. ￿en, a polynomial function (red line) is approximated to describe the trend of these values. In this function, the similarity score 0.83 emerges as a global minimum and has been used for ￿ltering out the non-similar document pairs. 4.3 Baselines We compare the performance of TDC, RDC and CRDC algorithms against the following baselines: • K-Means as a centroid-based clustering approach. • DBSCAN as a density-based clustering approach. • Random, which randomly selects R from the dataset Initially, K-Means [3] randomly composes a set of centroids and assigns each point of the sample to its nearest cluster based on a distance measure. ￿en, a new set of centroids is calculated from the previous ones according to the assigned points. ￿is process is repeated until the set of centroids does not change signi￿cantly between consecutive iterations or a maximum number of iterations is reached. ￿e scalable K-Means approach used in our experiments is an improved version of k-means which obtains an initial set of centers ideally close to the optimum solution. ￿e algorithm implemented at the Apache Commons Math library 1 was used in the experiments. Based on empirical results, the best con￿guration is: k = number � of � topics = 44 and maxIterations = 50 A widely known density-based algorithm is DBSCAN [15], which compose clusters from the neighborhood of each point considering at least a minimum number of points and a given radius. ￿us, it requires to specify the radius of the point’s neighborhood, Eps, and the minimum number of points in the neighborhood MinPts. Based on empirical results, the best results were obtained with the following con￿guration: eps = 0.1 and minPts = 50 1 h￿p://commons.apache.org/proper/commons-math/ E￿icient Clustering from Distributions over Topics K-CAP 2017, December 4–6, 2017, Austin, TX, USA ￿e Random algorithm takes as input a parameter m and ran- domly divides the dataset into m equal-sized groups of similar documents. For the evaluation, m was set to the number of topics, the dimension of the dataset. With respect to the proposed algorithms and taking into account empirical results, the RDC algorithm is set to use the top1 highest topics, and the cumulative weight threshold for the CRDC algorithm is set to 0.9. 4.4 Measure A gold-standard is created for each dataset and distance metric considered. ￿ey are created by calculating all pairwise similarities from their documents. Since the n ⇥ n similarity matrix requires O(n2) time to be calculated, the selected size of datasets has not been too large n = 1000. We considered three measures to evaluate our algorithms with respect to the baseline: • cost: based on the number of similarity score calculations required by the algorithm: cost = (reqSim � minSim)/(totalSim � minSim) (8) ￿e minSim corresponds to the number of similar docu- ments obtained from using the threshold score previously mentioned in section 4.2. ￿e totalSim corresponds to the Cartesian product of existing documents: totalSim = n ⇤ n = 1, 000, 000. And the reqSim corresponds to the number of similarities calculated by the algorithm. • e￿ectiveness: based on precision and recall. It expresses the quality of the algorithm: ef f ecti�eness = precision2 + recall2 2 (9) • e￿ciency: based on the previous ones, it express a com- promise between quality and performance: ef f icienc� = e f f ecti�eness � cost (10) 4.5 Results ￿e code used to evaluate the algorithms along with the results obtained are available on GitHub [2]. In terms of e￿ectiveness (Figures 2 and 3), the results highlight that K-Means and CRDC outperform the other algorithms. K-Means was expected to be a top performer because the algorithm itself performs comparisons to map clusters. ￿e fact that CRDC has such good performance encourages us to think that, in fact, the most relevant topics when they altogether exceed a certain high weight threshold, are those that best represent the document and allow to group together similar documents. However, as shown in tables 1, 2, 4 and 3, considering a ￿xed number of more relevant topics (RDC) or considering the trend of their weights (TDC) does not seem to perform so well on aggregating similar documents, since their precision and recall values are very low in both cases. It is surprising that the DBSCAN has such low value. Taking a look at its precision and recall values, and also seeing the number of groups that each algorithm has created (Figure 4), we believe that having a corpus containing a very cohesive set of documents (all papers in corpus belong to the same journal) a￿ects the performance of this algorithm since it divides the corpus into a lower number of Figure 2: E￿ectiveness (JS-based) in AIES Figure 3: E￿ectiveness (He-based) in AIES groups. ￿is way, it obtains high values of recall because most of the pair-wise distances are computed, but very low precision. ￿e results also show that the behavior of the algorithms does not di￿er signi￿cantly when using di￿erent similarity measures, for example JS divergence (Figure 2) and He distance (Figure 3). ￿is highlights the importance of the documents’ topic distributions to successfully classify them into smaller groups of similar items, while other particular aspects such as the distance or similarity metric used to compare them are less in￿uential. Size CRDC DBSCAN K-Means RDC TDC Random 200 0.94 0.10 0.96 0.31 0.42 0.12 300 0.93 0.15 0.94 0.30 0.39 0.08 400 0.93 0.15 0.89 0.29 0.39 0.09 500 0.92 0.30 0.90 0.28 0.38 0.09 600 0.92 0.19 0.88 0.28 0.38 0.08 700 0.92 0.20 0.91 0.28 0.38 0.09 800 0.92 0.12 0.89 0.30 0.39 0.10 900 0.92 0.13 0.87 0.30 0.40 0.10 1000 0.93 0.13 0.90 0.30 0.40 0.10 Table 1: Precision (JS-based) in AIES K-CAP 2017, December 4–6, 2017, Austin, TX, USA C. Badenes-Olmedo et al. Figure 4: Clusters in AIES Size CRDC DBSCAN K-Means RDC TDC Random 200 0.75 0.07 0.84 0.23 0.08 0.33 300 0.74 0.08 0.83 0.23 0.06 0.32 400 0.76 0.09 0.76 0.22 0.06 0.32 500 0.73 0.08 0.74 0.21 0.08 0.31 600 0.72 0.08 0.73 0.21 0.06 0.30 700 0.71 0.10 0.76 0.21 0.06 0.30 800 0.73 0.11 0.78 0.22 0.07 0.31 900 0.73 0.12 0.80 0.22 0.08 0.32 1000 0.74 0.15 0.77 0.23 0.08 0.32 Table 2: Precision (He-based) in AIES Size CRDC DBSCAN K-Means RDC TDC Random 200 0.92 1.00 0.79 0.96 0.02 0.87 300 0.91 0.89 0.84 0.96 0.02 0.84 400 0.92 0.92 0.90 0.96 0.02 0.86 500 0.91 0.94 0.88 0.96 0.03 0.85 600 0.91 0.94 0.87 0.96 0.02 0.83 700 0.91 0.92 0.90 0.96 0.02 0.83 800 0.92 0.92 0.88 0.96 0.02 0.83 900 0.92 0.95 0.86 0.96 0.02 0.83 1000 0.92 0.93 0.89 0.97 0.02 0.84 Table 3: Recall (JS-based) in AIES Size‘ CRDC DBSCAN K-Means RDC TDC Random 200 0.84 1.00 0.65 0.96 0.02 0.82 300 0.84 0.98 0.76 0.95 0.02 0.78 400 0.84 0.98 0.79 0.94 0.02 0.79 500 0.85 0.94 0.87 0.95 0.02 0.78 600 0.86 0.96 0.80 0.95 0.02 0.76 700 0.85 0.98 0.80 0.95 0.02 0.76 800 0.85 0.99 0.81 0.95 0.02 0.76 900 0.85 0.99 0.75 0.95 0.02 0.77 1000 0.86 1.00 0.74 0.96 0.02 0.78 Table 4: Recall (He-based) in AIES Figure 5: Cost (JS-based) in AIES Figure 6: Cost (He-based) in AIES In terms of cost (Figures 5 and 6), the best clustering algorithm, as expected, is based on random selection. ￿is is due to the fact that the number of pairs compared by this algorithm is always the minimum, given the dataset is simply randomly divided into m equal-sized groups, where m is equals to the number of topics, i.e. dimension of the dataset. Since K-Means and DBSCAN make comparisons between documents until their internal condition is satis￿ed, they are the most ine￿cient approaches. K-Means involves the highest cost because it compares all the documents with the 44 centroids in each iteration. Among our proposals, the main reason for an algorithm to present a higher cost is due to the number of groups the corpus is divided into (see Figure 4). ￿e greater the number of groups, the fewer the number of later comparisons that have to be made and, therefore, the lower the cost of the algorithm. ￿e behavior of the DBSCAN algorithm depends remarkably on the similarity metric used. We think that this may be due to the way in which both measures satisfy the triangle inequality condition, since one is based on divergence (JS) and the other on distance (He). ￿is property, which de￿nes distance(a,b)  distance(a,c) + distance(c,b), is very important in the calculations that DBSCAN makes to discover the groups, since it only calculates the distances between near points. E￿icient Clustering from Distributions over Topics K-CAP 2017, December 4–6, 2017, Austin, TX, USA Figure 7: E￿ciency (JS-based) in AIES Figure 8: E￿ciency (He-based) in AIES Finally, in terms of e￿ciency (Figures 7, 8), regardless of the similarity measure used, the algorithm that yields the best perfor- mance according to the results obtained is CRDC. Overall, CRDC demonstrates a high accuracy classi￿cation and a lower cost by improving the performance o￿ered by centroid-based or density- based approaches. We have also created a synthetic dataset, DRM (Section 4.1), composed of 1000 Dirichlet distributions with the same dimensions than topics in AIES: k = 44. Unlike AIES, topic distributions have been randomly generated which imply that the similarity values are not so high: min = 0.06, mean = 0.18 and max = 0.61. Following the same criteria than before (Section 4.2), the similarity threshold is now ￿xed to 0.34 (Figure 9). Results in terms of e￿ectiveness (Figure 10) show a poor performance of the RDC and CRDC al- gorithms. ￿e reason is that both are based on the fact that the highest weighted topics are shared between similar distributions. However, this condition is not satis￿ed when the similarity value between them is low. To con￿rm this behavior, we created a third dataset (DRM2) with the same size but with only 4 dimensions (4 topics). ￿e goal is to achieve more similar distributions than in DRM even though they are also randomly generated. Since the similarity values range from min = 0.04, mean = 0.34 to max = 0.99, the similarity threshold is now ￿xed to 0.66 (more details in section 4.2). ￿e results (Figure 11) Figure 9: Similarity values grouped by frequency in DRM Figure 10: E￿ectiveness (JS based) in DRM Figure 11: E￿ectiveness (JS based) in DRM2 show an improvement in the accuracy of both the RDC and CRDC algorithms. Although scores are still not as high as for the AIES dataset, the increase compared to the DRM dataset shows that their precision and recall improve when the similarity threshold is higher. On the other hand, both the DBSCAN and TDC algorithms show similar behavior in both datasets, which means that their performance is not a￿ected by the similarity threshold. "
"LegalDocumentsRetrieval","                LEGAL DOCUMENT RETRIEVAL   ACROSS LANGUAGES:   TOPIC HIERARCHIES BASED ON SYNSETS  Horizon 2020 No 780247 - TheyBuyForYou  Authors:  Badenes-Olmedo, Carlos (Universidad Politécnica de Madrid)  Redondo-Garcia, Jose Luis (Amazon Research)  Corcho, Oscar (Universidad Politécnica de Madrid)              LEGAL DOCUMENT RETRIEVAL ACROSS LANGUAGES:   TOPIC HIERARCHIES BASED ON SYNSETS*    Cross-lingual annotations of legislative texts enable us to explore major themes covered in multi- lingual legal data and are a key facilitator of semantic similarity when searching for similar documents.  Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine  learning models that can be used to perform thematic explorations on collections of texts in multiple  languages. However, these approaches require theme-aligned training data to create a language- independent space, which limits the amount of scenarios where this technique can be used. In this  work, we provide an unsupervised document similarity algorithm based on hierarchies of multi-lingual  concepts to describe topics across languages. The algorithm does not require parallel or comparable  corpora, or any other type of translation resource. Experiments performed on the English, Spanish,  French and Portuguese editions of JCR-Acquis corpora reveal promising results on classifying and  sorting documents by similar content.    1. INTRODUCTION  Searching for similar documents among legal data accross Europe requires multi-language techniques  to be performed. For many languages we may not have access to translation dictionaries or a full  translation system, or they can be expensive to apply in an online search system. In such situations it  is useful to rely on smaller annotation units derived from the text so the full content doesn't need to  be translated, for instance by finding correspondences with regard to the topics discussed. In this case,  it may be advisable to automatically learn cross-lingual topics to browse multi-lingual document  collections.  Within the European project TheyBuyForYou1, we have worked on an unsupervised way of building  cross-lingual labels from sets of cognitive synonyms (synsets) to establish relations between language- specific topics. This avoids the need for parallel corpus (i.e sentence-aligned documents) or  comparable corpus (i.e theme-aligned documents), that is a requirement of the recently emerged  multilingual probabilistic topic models (MuPTM) [1]. The cross-lingual labels can be used for large-scale  document retrieval tasks in multi-lingual corpora.    * This paper is an extension of our previous work [3] by adding Portuguese texts into the evaluation.  1 https://theybuyforyou.eu      Legal Document Retrieval Across Languages.  Página 3    2. APPROACH  Documents are represented as data points in a low-dimensional latent space created by probabilistic  topic models for each language separately (figure 1). Topics are annotated with cross-lingual labels  created from a list of synset [2]  retrieved from the Open Multilingual WordNet2. Word by word are  queried in WordNet to retrieve its synsets. The final set of synsets for a topic is the union of the synsets  from the individual top-words of a topic (top5 based on empirical evidences) [3].     Figure 1 Mono-lingual, multi-lingual and cross-lingual topic-based approaches  Since exact similarity computations are unaffordable for neighbours detection tasks (O(n2)) [7], a  hashing algorithm to group similar documents and to preserve the notion of topics has been proposed  [4]. It defines a hierarchical set-type data where each level of the hierarchy indicates the importance  of the topic according to its distribution (figure 2). The knowledge provided by the topics to describe  the documents is maintained and an efficient exploration of document collections on a large scale can  be performed. Documents are annotated with the relation between topics inside each hierarchy level.   In this feature space, the proposed distance metric is based on the Jaccard coefficient which computes  the similarity of sets by looking at the relative size of their intersection. More specifically, the similarity  metric used to compare the hash codes created from set of topics is the sum of the Jaccard distances  for each hierarchy level, i.e. for each set of topics [4].    2 http://compling.hss.ntu.edu.sg/omw/      Legal Document Retrieval Across Languages.  Página 4    3. Evaluation  The JRC-Acquis3 dataset is used to compare the  performance of this unsupervised algorithm with a  semi-supervised algorithm based on MuPTM. It is a  collection of legislative texts written in 23 languages  that have been manually classified into subject  domains according to the EUROVOC4  thesaurus. The  English, Spanish, French and Portuguese editions  (about 20,000 documents per edition) of the corpora  were used for each language-specific model. The  EUROVOC taxonomy was pre-processed to satisfy the  topic independence assumption of LDA [5] models, by  using hierarchical relations. The initial 7,193 concepts  from 21 domain areas such as politics, law or economics were reduced to 452 categories, that are  independent and can be used to train the topic models. Finally, in addition to LDA models5, LabeledLDA  [6] models6 were also created to force the correspondence between those categories and the latent  topics of the model.   Documents were pre-processed (Part-of-Speech filtering and lemmatized format) by the librAIry7 NLP  service to build a suitable dataset for the models. Theme-aligned probabilistic topic models in English,  Spanish, French and Portuguese share the topics but not their definitions (i.e words) (see table 1).  Topic3@EN  Communication Systems  Topic3@ES  Sistema de Comunicación  Topic26@FR  Systeme de Comunicacion  Topic10@PT  Communication Systems  radio  equipo  communications  rede  equipment  red  reseaux  comunicação  network  comunicación  electroniques  electrónico  communication  espectro  acces  acesso  regulatory  electromagnético  telecommunications  utilizador  Table 1 Theme-aligned topics described by top 5 words based on EUROVOC annotations    3 https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis  4 http://eurovoc.europa.eu/  5 http://librairy.linkeddata.es/jrc-{en|es|fr|pt}-model-unsupervised  6 http://librairy.linkeddata.es/jrc-{es|en|fr|pt}-model  7 http://librairy.linkeddata.es/nlp  Figure 2 hash-expression (H) of a document based on  WordNet-synset annotations created from the top words  of each topic distribution      Legal Document Retrieval Across Languages.  Página 5    3.1  Cross-lingual Document Retrieval Results  Given a set of documents and a query document, the task is to filter and rank the documents according  to their relevance (i.e. semantic similarity) to the query text regardless of the language used. A ground- truth set grouping the documents that share the same EUROVOC codes is considered from the query  document. A collection of 1,000 randomly selected documents (monolingual, bi-lingual and multi- lingual) are annotated by the category-based (semi-supervised model) and synset-based (unsupervised  model) algorithms. We evaluate the performance of the algorithms in terms of precision@3,  precision@5 and precision@10 (tables 2 and 3)    en  es  fr  pt    category  synset  category  synset  category  synset  category  synset  p@3  0.84  0.83  0.81  0.78  0.83  0.74  0.79  0.78  p@5  0.82  0.80  0.79  0.75  0.80  0.72  0.77  0.75  p@10  0.77  0.76  0.75  0.73  0.77  0.58  0.72  0.71  Table 2: Information retrieval performance  of the categories-based and synset-based topic alignment  algorithms in monolingual document collections.    es-en  es-pt  pt-en  en-es-fr-pt    category  synset  category  synset  category  synset  category  synset  p@3  0.84  0.79  0.80  0.78  0.82  0.81  0.81  0.69  p@5  0.82  0.76  0.77  0.76  0.80  0.78  0.78  0.67  p@10  0.78  0.73  0.75  0.72  0.77  0.75  0.73  0.62  Table 3: Information retrieval performance  of the categories-based and synset-based topic alignment  algorithms in multi-lingual document collections.  Although the precision values are lower than those obtained by semi-supervised approximation, they  are sufficiently promising (around 0.75) to think that introducing improvements in the natural  language processing would increase the quality of the WordNet-synset annotations derived from the  most representative words of each topic (precision values close to 0.8 in the English corpus).  4. Conclusion  The algorithm has proved to perform close to the semi-supervised algorithm in information retrieval  tasks, which makes us think that the process of topic annotation by set of synonyms can be improved  to filter those elements that are not sufficiently representative. Our future lines of work will go in that  direction, incorporating context information to identify the most representative synset for each topic.  "
"Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies","Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies Carlos Badenes-Olmedo cbadenes@fi.upm.es Ontology Engineering Group, Universidad Politécnica de Madrid Boadilla del Monte, Spain José Luis Redondo-García jluisred@amazon.com Amazon Research Cambridge, UK Oscar Corcho ocorcho@fi.upm.es Ontology Engineering Group, Universidad Politécnica de Madrid Boadilla del Monte, Spain ABSTRACT With the ongoing growth in number of digital articles in a wider set of languages and the expanding use of different languages, we need annotation methods that enable browsing multi-lingual corpora. Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, these approaches require theme-aligned train- ing data to create a language-independent space. This constraint limits the amount of scenarios that this technique can offer solu- tions to train and makes it difficult to scale up to situations where a huge collection of multi-lingual documents are required during the training phase. This paper presents an unsupervised document similarity algorithm that does not require parallel or comparable corpora, or any other type of translation resource. The algorithm annotates topics automatically created from documents in a sin- gle language with cross-lingual labels and describes documents by hierarchies of multi-lingual concepts from independently-trained models. Experiments performed on the English, Spanish and French editions of JCR-Acquis corpora reveal promising results on classi- fying and sorting documents by similar content. CCS CONCEPTS • Information systems → Digital libraries and archives; In- formation retrieval. KEYWORDS cross-lingual semantic similarity; large-scale text analysis; topic models ACM Reference Format: Carlos Badenes-Olmedo, José Luis Redondo-García, and Oscar Corcho. 2019. Scalable Cross-lingual Document Similarity through Language-specific Con- cept Hierarchies. In Proceedings of the 10th International Conference on Knowledge Capture (K-CAP ’19), November 19–21, 2019, Marina Del Rey, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3360901. 3364444 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-7008-0/19/11...$15.00 https://doi.org/10.1145/3360901.3364444 1 INTRODUCTION Cross-language information extraction deals with the retrieval of documents written in languages different from the language of the user’s query. At execution time, the query in the source language is typically translated into the target language of the documents with the help of a dictionary or a machine-translation system. But for many languages we may not have access to translation dictionaries or a full translation system, or they can be expensive to apply in an online search system. In such situations it is useful to rely on smaller annotation units derived from the text so the full content doesn’t need to be translated, for instance by finding correspondences with regard to the topics discussed. In this case, it may be advisable to automatically learn cross-lingual topics to browse multi-lingual document collections. Multi-lingual topic models discover language-specific descrip- tions of each topic from documents in multi-lingual corpora. They are mainly based on Latent Dirichlet Allocation (LDA) [4], adding supervised associations between languages by using parallel corpus, with sentence-aligned documents (e.g. Europarl1 corpora), or com- parable corpus, with theme-aligned documents (e.g. Wikipedia2 articles), in multiple languages. These requirements restrict the kind of corpora that can be used for training since large parallel corpora are rare in most of the use cases, especially for languages with fewer resources. Wikipedia, for example, contains texts in 304 languages but 255 of them have less than 3% of articles3. Therefore, the requirement of parallel/comparable corpora for multilingual topic models limits their usage in many situations. In addition, these models rely on associations between documents prior to training. So in order to incorporate new languages or update the existing associations, the model must be re-trained with documents from all languages, making it difficult to scale to large corpora [17] [30]. Another approach is to use multi-lingual dictionaries as super- vised methods [25][16]. They are usually easier to obtain and more widely available than parallel corpora (e.g. PANLEX4 covers 5,700 languages and Wiktionary5 covers 8,070 languages). Models built on dictionaries rather than a parallel/comparable corpora are po- tentially applicable to many more use cases. And even coherent multi-lingual topics can be learnt from partially and fully incompa- rable corpora with limited amounts of dictionary resources [18]. 1https://ec.europa.eu/jrc/en/language-technologies/dcep 2https://www.wikipedia.org/ 3https://meta.wikimedia.org/wiki/List_of_Wikipedias 4https://panlex.org 5https://www.wiktionary.org K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA Badenes-Olmedo C., et al. Figure 1: Graphical representation of the model that relies on the latent layer of cross-lingual topics obtained by LDA and hash functions through hierarchies of synsets. Mono-lingual approaches force to translate the documents to the same language to represent them in a unique feature space. Multi-language approaches require previously aligned topics from different lan- guages so that documents can be represented in an equivalent feature space. Cross-lingual Synset-based approach creates a new space by combining the feature spaces of each language (i.e synsets from topn topic words). Documents are then repre- sented in this unique space. But all these probabilistic topic models are based on prior knowl- edge. Connections at document-level (by parallel or comparable cor- pora) or at word-level (by dictionaries) are created in the training- data before building the model. In this way, the pre-established language relations condition the creation of the topics (supervised method), instead of being inferred from the topics themselves as a posteriori knowledge (non-supervised method). We propose a com- pletely unsupervised way of building cross-lingual topic models that uses sets of cognitive synonyms (synsets) to establish relations between language-specific topics once the model is created and does not require parallel or comparable data for training. These models can be used for large-scale multi-lingual (1) document classi- fication and (2) information retrieval tasks. Our main contributions, described in this paper, are: • a novel cross-lingual document similarity algorithm based on hierarchies of synsets. • an open-source implementation 6 of the algorithm • data-sets and pre-trained models to facilitate other re- searchers to replicate our experiments and validate and test their own ideas. 2 RELATED WORK One of the greatest advantages of using probabilistic topic models (PTM) in large document collections is their ability to represent documents as probability distributions over a fixed number of topics, thereby mapping documents into a low-dimensional latent space 6https://github.com/cbadenes/crosslingual-semantic-similarity (the K-dimensional probability simplex, where K is the number of topics). A document, represented as a point in this simplex, is considered to have a particular topic distribution. This brings a lot of potential when applied over different information-retrieval (IR) tasks, as evidenced by recent works in different domains such as scholarly [12], health [24] [38], legal [33][14], news [19] and social networks [35] [7]. Multilingual probabilistic topic models (MuPTM) [39] have re- cently emerged as a group of language-independent generative machine learning models that can be used on large-volume theme- aligned multilingual text. Due to its generic language-independent nature and the power of inference on unseen documents, MuPTM’s have found many interesting applications in many different cross- lingual tasks. They have been used on cross-lingual event clustering [8], document classification [9] [32], semantic similarity of words [29] [40], information retrieval [41] [11], document matching [34] [44], and others. Once a PTM or MuPTM has been generated, documents can be represented by data points in a feature space based on topics to detect similarities among them exploiting inference results and using distance calculation metrics on it. Since exact similarity com- putations are unaffordable for neighbours detection tasks (O(n2)), some algorithms based on approximate nearest neighbor (ANN) techniques have been proposed to efficiently perform document similarity search based on the low-dimensional latent space created by probabilistic topic models[43] [26]. They transform data points Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA from the original feature space into a hash-code space, so that simi- lar data points have larger probability of collision (i.e. having the same hash code). However, the smaller space created by existing hashing methods lose the exploratory capabilities that topic models offer and the explanatory power that topics have to support the document similarity. The notion of topics is discarded and therefore the ability to make thematic explorations of documents. Recently, a hashing algorithm that groups similar documents and preserves the notion of topics has been proposed [2]. It defines a hierarchical set-type data where each level of the hierarchy indicates the impor- tance of the topic according to its distribution. Level 0 contains the topics of the document with the highest score. Level 1 contains the topics with highest score once the first ones have been eliminated, and so on. The knowledge provided by the topics to describe the documents is maintained and an efficient exploration of document collections on a large scale can be performed. In this paper we take these hierarchies of PTM a step further, to make them cross-lingual. Documents from multi-language corpora can then be efficiently browsed and related without the need for translation. An algorithm that annotates topics with knowledge from a lexical data base and describes documents with hierarchical expressions of multi-lingual concepts is presented in this paper. Hash codes are created from those concept hierarchies to perform document classification and information retrieval tasks on large document collections. 3 AN APPROACH TO CALCULATE CROSS-LINGUAL DOCUMENT SIMILARITY EFFICIENTLY Our similarity algorithm considers that cross-lingual models can be built from non-parallel or even non-comparable collections of multi-lingual documents. It first creates a probabilistic topic model for each language separately, and then annotates the topics with cross-lingual labels (Fig 1). In the same way, the topic distribution of documents expressed through weighted vectors are first trans- formed into hierarchies of topics according to their relevance. And then documents are described by a 3-level hierarchy of cross-lingual concepts. 3.1 Synset-based annotations Each topic is annotated with a list of synset [5] retrieved from Word- Net7[27] based on its topn words (Fig 2). Word by word are queried in WordNet to retrieve its synsets. The final set of synsets for a topic is the union of the synsets from the individual top-words of a topics. Based on empirical evidence from different executions of the algo- rithm, n=5 is the configuration that offered the best performance in our tests. Let’s look at an example to clarify how it works. Given the topics of Table 1, the EN-Topic (""communications systems"") is annotated with the following synset list: radio.a.01, radio.v.01, ra- dio.n.03, radio.n.01, radio_receiver.n.01, equipment.n.01, network.n.02, network.n.04, network.v.01, network.n.05, network.n.01, net.n.06, com- munication.n.02, communication.n.03, communication.n.01, regula- tive.s.01. The list of synset for the ES-Topic (""sistema de comu- nicación"") is: kit.n.02,team.n.01, equipment.n.01, net.n.02, net.n.05, 7https://wordnet.princeton.edu/ Figure 2: Cross-lingual hash-expression (H) of a document based on WordNet-synset annotations created from the top words of each topic distribution. The most relevant topics are grouped according to their importance in three levels (h0, h1 and h2) network.n.05, web.n.06, network.n.01, web.n.02, communication.n.02, communication.n.01, announcement.n.02, spectrum.n.02, spectrum.n.01, creep.n.01, ghost.n.01, apparition.n.02, electromagnetic.a.01. And the list for FR-Topic (""systeme de communication"") is: access.n.02, ap- proach.n.07, approach.n.02, access.n.06, access.n.03, access.n.05, as- sault.n.03, bout.n.02, approach.n.01, entree.n.02, entry.n.01, entrance.n.01, entry.n.03, admission.n.01, submission.n.01, introduction.n.01. The librAIry NLP service8 was used to identify the list of synsets from a topic description based on top words. It includes the Open Multi- lingual WordNet9 [6]. 3.2 Document representation Documents (i.e seen as data points in the generated space) are transformed from the original feature space based on mono-lingual topic distributions into a hierarchical-code space, so that similar data points share relevant cross-lingual concepts. Since topic mod- els create latent themes from word co-occurrence statistics in a corpus, a cross-lingual concept specifies the knowledge about the word-word relations it contains for each language. This abstrac- tion can be extended to cover the knowledge derived from sets of topics. The topics are obtained via state-of-the art methods, collapsed Gibbs sampling[15] for LDA, and hierarchically divided into groups with different degrees of semantic specificity in a doc- ument. Documents represented as a weighted mixture of latent topics (per-document topic distributions) are then annotated in these feature spaces with the relation between topics inside each hi- erarchy level. Regardless of their language, they are then described by cross-lingual concepts (based on WordNet-synset annotations) and hash codes are calculated to summarize their content [2]. The 8http://librairy.linkeddata.es/nlp 9http://compling.hss.ntu.edu.sg/omw/ K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA Badenes-Olmedo C., et al. Figure 3: topic distributions from the same document in English (hEN = {(t3062), (t335), (t8278)}) and Spanish (hES = {(t335), (t4060), (t5769)}). hash expression sets a 3-level hierarchy of cross-lingual concepts. Topics with similar presence in a document are grouped together in the same hierarchical level (Fig 2). Each level of the hierarchy indicates the importance of the topic according to its distribution. Level 0 describes the topics with the highest score. Level 1 de- scribes the topics with highest score once the first ones have been eliminated, and so on. Documents are described by vectors contain- ing set of topics (i.e. set of synsets), where each dimension means a topic relevance. Given a document d with a topic distribution q = [t0 = 0.28,t1 = 0.05,t2 = 0.44,t3 = 0.23], the hash expression may be Hd = (ts2), (ts0,ts3), (ts1). It means that topic t2 described by the synset ts2 is the most relevant (i.e 0.44 score), then topics t0 and t3 described by synsets ts0 and ts3 (i.e 0.28 and 0.23 scores) and, finally, topic t1 described by synset ts1 (i.e 0.05). 3.3 Similarity metric Since documents are described by set-type data, the proposed dis- tance metric is based on the Jaccard coefficient. This metric is mainly used for this type of data [23] [21] [22] [42] and computes the sim- ilarity of sets by looking at the relative size of their intersection ( |A∩B | |A∪B | ). More specifically, the similarity metric used to compare the hash codes created from set of topics is the sum of the Jaccard distances for each hierarchy level, i.e. for each set of topics [2]: dH (HA,HB) = L � l=1 � dJ (HA(hl),HB(hl)) � = L � l=1 � 1−HA(hl) ∩ HB(hl) HA(hl) ∪ HB(hl) � (1) where HA and HB are hash codes, HA(hl) and HB(hl) are the set of topics up to level l for each hash code H, and L is the maximum hierarchy level. A corner case is L = T, where T is the number of topics in the model. 4 EVALUATION A way to evaluate our cross-lingual document similarity algorithm is to test how well it performs in practice for different real-life tasks: document classification and information retrieval. Evaluation is done using the B-Cubed metrics [3] to estimate the fit between two clusters, the one obtained from a supervised category-based topic alignment algorithm and the one obtained from our unsupervised synset-based topic alignment algorithm. Let CLi be the cluster that document ti gets clustered in, and Gi its correct cluster from the ground truth. The B-Cubed metric then calculates precision = |CLi∩Gi | |CLi | and recall = |CLi∩Gi | |Gi | . The total precision and recall of the clustering are taken as the average of the precision and recall scores over all documents. Results are also presented in terms of the F1 measure to balance between preci- sion and recall: F1 = 2·precision·recall precision+recall . The aim is to measure the performance of the algorithm taking into account documents with manual category assignments. 4.1 Data Sets A multilingual corpora is required to create the cross-lingual topic models that support our document similarity algorithm. The key fea- ture is that it does not need to be parallel or comparable. However, in order to be able to compare the performance of our unsupervised algorithm with a semi-supervised algorithm (MuPTM-based) it is necessary to use theme-aligned corpora that map topics across languages. We used the JRC-Acquis10 corpus [37]. It is a collec- tion of legislative texts written in 23 languages, although we only use English, Spanish and French for the tests. Most texts have been manually classified into subject domains according to the EU- ROVOC11 thesaurus [10], which exists in one-to-one translations into approximately twenty languages and distinguishes about 6,000 hierarchically organised descriptors (subject domains). More than 20k documents were used for each language-specific model, a total of 82,140 texts are included in the training-test package, which is publicly available12 for reuse. 4.2 Cross-lingual Models The JRC-Acquis corpus is annotated with EUROVOC categories. These categories are shared among languages and will serve as support for building the topic models. Moreover, the topic inde- pendence assumption [4] of LDA models should be also satisfied, so the categories must first be moved to their base concepts and therefore disjointed categories. The EUROVOC taxonomy has 7,193 concepts/labels from 21 domain areas such as politics, international relations, european union, law, economics, etc. There are 4,904 re- ciprocal hierarchical relationships (no polyhierarchy) and 6,992 reciprocal associative relationships. Using hierarchical relations, we identified the root concepts from which all other categories derive. The initial 7,193 labels were then reduced to 452 labels, which are independent (topic independence assumption from LDA is satisfied), and can be used to train the topic models. A pre-processing of the documents was required to clean texts and to build a suitable data set for the model. We assume that terms with high frequency are not specific to a particular topic, so words present in more than 90% of the corpus are considered stopwords and removed from the model. Also, rare terms that occur infrequently are considered not representative of a single topic since they do not appear enough to infer that it is salient for a 10https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis 11http://eurovoc.europa.eu/ 12http://librairy.linkeddata.es/data/jrc/select?q=*:* Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA EN-Topic 3 ES-Topic 3 FR-Topic 26 ""communications systems"" ""sistema de comunicación"" ""systeme de communication"" radio equipo communications equipment red reseaux network comunicación electroniques communication espectro acces regulatory electromagnético telecommunications spectrum electrónico service electronic reglamentación universel access banda reglamentaires standard etsir nationales mobile compatibilidad fourniture Table 1: Randonly selected theme-aligned topics described by top 10 words based on EUROVOC annotations from JRC-Acquis dataset topic. Then, words present in less than 0.5% of the corpus are also removed from the model. Lemmatized expressions of names, verbs and adjectives were used to create the bag-of-words, and documents with less than 100 characters were discarded since LDA has proven to has lower performance with these type of texts [7]. Then, we set the number of topics K = 500 (several configura- tions were evaluated, but this was the closest to the performance obtained with the supervised model based on categories). We run the Gibbs samplers for 1000 training iterations on LDA from the open-source librAIry [1] software. The Dirichlet priors α = 0.1 and β = 0.01 were set following [20]. Once the word distributions for each topic is available, the list of synsets related with the top5 words for each topic are identified (this number is set to offer better performance after trying several alternatives). Finally, the 3-level hierarchy of topics per document is replaced by a 3-level hierar- chy of synsets. Probabilistic topic models in Spanish13, English 14 and French15 were created independently without previously establishing any type of alignment between their topics. In order to compare the performance of this non-supervised approach with approaches based on aligned topics, we need to use a variant of LDA to force the correspondence between the 452 root categories identified in the EUROVOC thesaurus and the latent topics of the model. Thus, LabeledLDA [36], a supervised version of LDA, was used to perform parameter estimation. Theme- aligned probabilistic topic models in Spanish16, English 17 and French18) were created sharing the topics but not its definitions (i.e. vocabulary) (see table 1). A simple way of looking at the output quality of the topic models is by simply inspecting top words associated with a particular topic learned during training. A latent topic is semantically coherent if it assigns high probability scores to words that are semantically related [13] [31] [28]. It is much easier for humans to judge seman- tic coherence of cross-lingual topics and their alignment across languages when observing the actual words constituting a topic. 13http://librairy.linkeddata.es/jrc-es-model-unsupervised 14http://librairy.linkeddata.es/jrc-en-model-unsupervised 15http://librairy.linkeddata.es/jrc-fr-model-unsupervised 16http://librairy.linkeddata.es/jrc-es-model 17http://librairy.linkeddata.es/jrc-en-model 18http://librairy.linkeddata.es/jrc-fr-model These words provide a shallow qualitative representation of the latent topic space, and could be seen as direct and comprehensive word-based summaries of a large document collection. Samples of cross-lingual topics are provided in Table 1. We may consider this visual inspection of the top words associated with each topic as an initial qualitative evaluation, suitable for human judges. Documents present similar topic distributions when projecting their content on topics according to their language as can be seen in fig 3. Since the topic identifiers are not aligned, the graphs appear displaced. 4.3 Cross-lingual Document Classification A random group of 1k documents, which have not been used to train the models, is considered for evaluation as they are manually tagged with EUROVOC categories. For each document, the cluster to which it belongs is identified from its categories. This cluster is then compared (B-Cubed metrics) with the one obtained from the labels generated from its most representative topics (cat) and with the one obtained from the labels generated with the WordNet- Synsets of those topics (syn). Algorithm performance is evaluated in monolingual, bilingual, and multilingual document collections (tables 2 and 3 ) . The results show a higher performance of the semi-supervised algorithm (categories-based topic alignment) in terms of precision, and of the unsupervised algorithm (synset-based topic alignment) in terms of coverage. The cause lies in the set of synonyms generated by WordNet, being able to share the same synset for two different topics. From a more general point of view (fMeasure), the benefit obtained by the increase in coverage (recall) is greater than by the loss of accuracy (precision). 4.4 Cross-lingual Information Retrieval Given a set of documents and a text, the task is to rank the docu- ments according to their relevance to the query text regardless of the language used. The JRC-Acquis corpus is used because by having texts tagged with EUROVOC categories we can build a ground- truth set grouping the documents that share the same codes as those used in the query document. A collection of 1k randomly selected documents (monolingual, bi-lingual and multi-lingual) are K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA Badenes-Olmedo C., et al. JRC-Acquis Corpora en es fr cat syn cat syn cat syn prec min 0.01 0.01 0.01 0.01 0.01 0.01 max 1.00 0.95 1.00 0.87 1.00 0.87 mean 0.58 0.48 0.55 0.48 0.55 0.41 dev 0.27 0.23 0.27 0.22 0.26 0.20 rec min 0.01 0.03 0.01 0.04 0.01 0.05 max 0.96 1.00 0.93 1.00 0.95 1.00 mean 0.39 0.52 0.36 0.49 0.42 0.51 dev 0.24 0.20 0.23 0.20 0.23 0.23 f1 min 0.02 0.03 0.01 0.02 0.02 0.03 max 0.70 0.75 0.70 0.71 0.70 0.73 mean 0.35 0.42 0.32 0.41 0.37 0.39 dev 0.16 0.15 0.15 0.15 0.17 0.17 Table 2: Document classification performance (precision- ’prec’, recall-’rec’ and fMeasure-’f1’) of the categories-based (cat) and synset-based (syn) topic alignment algorithms in monolingual document collections (en, es, fr) JRC-Acquis Corpora en-es en-fr es-fr en-es-fr cat syn cat syn cat syn cat syn prec min 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.02 max 1.00 0.97 1.00 0.98 1.00 0.97 1.00 0.98 mean 0.62 0.55 0.62 0.56 0.61 0.56 0.59 0.52 dev 0.26 0.23 0.25 0.23 0.26 0.23 0.26 0.23 rec min 0.01 0.09 0.00 0.06 0.01 0.07 0.01 0.07 max 1.00 1.00 0.94 0.97 0.91 0.93 0.86 0.93 mean 0.33 0.57 0.36 0.50 0.30 0.40 0.25 0.39 dev 0.16 0.23 0.17 0.19 0.13 0.13 0.13 0.15 f1 min 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.05 max 0.75 0.81 0.76 0.81 0.68 0.72 0.62 0.66 mean 0.36 0.49 0.38 0.47 0.35 0.41 0.30 0.38 dev 0.16 0.18 0.15 0.18 0.14 0.14 0.11 0.12 Table 3: Document classification performance (precision- ’prec’, recall-’rec’ and fMeasure-’f1’) of the categories-based (cat) and synset-based (syn) topic alignment algorithms in multi-lingual document collections (en-es, en-fr, es-fr, en-es- fr) annotated by the category-based and synset-based topic alignment algorithms. Then, we randomly take articles to search in D for doc- uments that share the same categories than the query document (i.e the ground-truth set). Next, the query text is used to search in D for similar documents using category-based annotations and synset-based annotations. We evaluate the performance of the al- gorithms in terms of precision@3, precision@5 and precision@10 (tables 4 and 5 ) . Although the precision values are lower than those obtained by semi-supervised approximation, they are sufficiently promising (around 0.75) to think that introducing improvements in the lemma- tization process would increase the quality of the WordNet-synset annotations derived from the most representative words of each topic (precision values close to 0.8 in the English corpus). JRC-Acquis Corpora en es fr cat syn cat syn cat syn p@3 mean 0.84 0.83 0.81 0.78 0.83 0.74 dev 0.26 0.26 0.27 0.29 0.26 0.32 p@5 mean 0.82 0.80 0.79 0.75 0.80 0.72 dev 0.25 0.25 0.25 0.27 0.25 0.29 p@10 mean 0.77 0.76 0.75 0.73 0.77 0.68 dev 0.23 0.25 0.25 0.27 0.24 0.27 Table 4: Information retrieval performance (precision@3, precision@5 and precision@10) of the categories-based (cat) and synset-based (syn) topic alignment algorithms in mono- lingual document collections (en, es, fr) JRC-Acquis Corpora en-es en-fr es-fr en-es-fr cat syn cat syn cat syn cat syn p@3 mean 0.84 0.79 0.86 0.77 0.85 0.78 0.85 0.75 dev 0.25 0.28 0.23 0.28 0.25 0.29 0.24 0.31 p@5 mean 0.82 0.76 0.84 0.75 0.82 0.76 0.81 0.72 dev 0.24 0.26 0.23 0.27 0.23 0.27 0.23 0.28 p@10 mean 0.78 0.73 0.80 0.70 0.77 0.72 0.76 0.67 dev 0.22 0.24 0.22 0.24 0.23 0.26 0.23 0.26 Table 5: Information retrieval performance (precision@3, precision@5 and precision@10) of the categories-based (cat) and synset-based (syn) topic alignment algorithms in multi- lingual document collections (en-es, en-fr, es-fr, en-es-fr) 5 CONCLUSIONS In this paper we present a first approach towards the calculation of cross-lingual document similarity through unsupervised prob- abilistic topic models and WordNet-synsets without the need for parallel or comparable corpora. As expected, the performance of our algorithm in terms of ac- curacy is not as good as that of the algorithm based on topics previously aligned by documents annotated with categories (theme- aligned training data). However, in terms of coverage, the perfor- mance of the unsupervised approach is much greater than that offered by the semi-supervised approach, to the point of offering better overall performance (i.e f1) in classification tasks. In addition, the algorithm has proved to perform close to the semi-supervised al- gorithm in information retrieval task, which makes us think that the process of topic annotation by set of synonyms should be improved to filter those elements that are not sufficiently representative. Our future lines of work will go in that direction, incorporating context information to identify the most representative synset for each topic. ACKNOWLEDGMENTS This research was partially supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 780247: TheyBuyForYou, and by the Spanish Min- isterio de Economía, Industria y Competitividad and EU FEDER "
"Potentially inappropriate medications in older adults living with HIV","Potentially inappropriate medications in older adults living with HIV B L�opez-Centeno,1,* C Badenes-Olmedo,2 A Mataix-Sanjuan,1 JM Bell�on,3 L P�erez-Latorre,3 JC L�opez,3 J Bened�ı,4,* S Khoo,5 C Marzolini,6 MJ Calvo-Alc�antara1 and J Berenguer 3 1Directorate for Pharmacy and Health Products, Madrid Regional Health Service (SERMAS), Madrid, Spain, 2Ontology Engineering Group (OEG), School of Telecommunications and System Engineering,Polytechnic University of Madrid (UPM), Madrid, Spain, 3Gregorio Mara~n�on General Hospital, Institute of Health Research Gregorio Mara~n�on (IiSGM), Madrid, Spain, 4Department of Pharmacology, Faculty of Pharmacy, Complutense Universtiy (UCM), Madrid, Spain, 5Department of Pharmacology, University of Liverpool, Liverpool, UK and 6Division of Infectious Diseases & Hospital Epidemiology, University Hospital of Basel, Basel, Switzerland Objectives We assessed the prevalence of potentially inappropriate medication (PIM) among older (≥ 65 years) people living with HIV (O-PLWH) in the region of Madrid. Methods We analysed the dispensation registry of community and hospital pharmacies from the Madrid Regional Health Service (SERMAS) for the period between 1 January and 30 June 2017, looking specifically at PIMs according to the 2019 Beers criteria. Co-medications were classified according to the Anatomical Therapeutic Chemical (ATC) classification system. Results A total of 6 636 451 individuals received medications. Of these individuals, 22 945 received antiretrovirals (ARVs), and of these 1292 were O-PLWH. Overall, 1135 (87.8%) O-PLWH were taking at least one co-medication, and polypharmacy (at least five co-medications) was observed in 852 individuals (65.9%). A PIM was identified in 482 (37.3%) O-PLWH. Factors independently associated with PIM were polypharmacy [adjusted odds ratio (aOR) 7.08; 95% confidence interval (CI) 5.16–9.72] and female sex (aOR 1.75; 95% CI 1.30–2.35). The distribution of PIMs according to ATC drug class were nervous system drugs (n = 369; 28.6%), musculoskeletal system drugs (n = 140; 10.8%), gastrointestinal and metabolism drugs (n = 72; 5.6%), cardiovascular drugs (n = 61; 4.7%), respiratory system drugs (n = 13; 1.0%), antineoplastic and immunomodulating drugs (n = 10; 0.8%), and systemic anti-infectives (n = 2; 0.2%). Five drugs accounted for 84.8% of the 482O PLWH with PIMs: lorazepam (38.2%), ibuprofen (18.0%), diazepam (10.2%), metoclopramide (9.9%), and zolpidem (8.5%). Conclusions Prescription of PIMs is highly prevalent in O-PLWH. Consistent with data in uninfected elderly people, the most frequently observed PIMs were benzodiazepines and nonsteroidal anti- inflammatory drugs . Targeted interventions are warranted to reduce inappropriate prescribing and polypharmacy in this vulnerable population. Keywords: aging, antiretroviral drugs, HIV, polypharmacy, potentially inappropriate medication Accepted 4 May 2020 Introduction Potentially inappropriate medication (PIM) is a term used to describe the use of a medicine for which the associated risks outweigh the potential benefits, especially when more effective alternatives are available [1]. Elderly peo- ple living with HIV (PLWH) are more likely to be exposed Correspondence: Dr Juan Berenguer, Unidad de EnfermedadesInfecciosas/ VIH (4100), Hospital General Universitario Gregorio Mara~n�on, Instituto de Investigaci�on Sanitaria Gregorio Mara~n�on (IiSGM), Doctor Esquerdo 46, 28007 Madrid, Spain. Tel: +34 91 586 8592; fax: +34 91 426 5177; e-mail: jbb4@me.com B. L�opez-Centeno and J. Berenguer contributed equally to this work. 1 DOI: 10.1111/hiv.12883 © 2020 British HIV Association HIV Medicine (2020) SHORT COMMUNICATION to PIMs because they often have multiple chronic dis- eases and therefore use a high number of drugs. In a pre- vious analysis, we showed that polypharmacy (defined as taking at least five non-HIV co-medications) was observed overall in 33% of PLWH (median age 48 years) vs. 62% and 80% in PLWH aged 65–75 and ≥ 75 years, respectively [2]. In addition, elderly PLWH experience age-related physiological changes, which can impact drug pharmacokinetics and pharmacodynamics and thereby predispose elderly PLWH to adverse drug reactions [3]. Common tools to detect inappropriate prescribing in elderly individuals include the Beers criteria [4] and the Screening Tool of Older Persons’ Prescriptions (STOPP)/ Screening Tool to Alert Doctors to Right Treatment (START) criteria [5]. These tools list instances of inappro- priate drug dosing, indication, treatment duration and treatment omission as well as inappropriate drugs for use in elderly patients. The advances in antiretroviral therapy (ART) have increased the life expectancy of PLWH and are leading to a growing HIV-infected cohort that is exposed to the risks of age-related comorbidities, age-related physiologi- cal changes, and care by multiple providers [6,7]. All these factors could increase the risk of PIM in this popu- lation group; however, little is known about the subject. Consequently, we designed this study to assess the preva- lence of PIMs among older PLWH. Methods Ours was a cross-sectional population-based study carried out in the region of Madrid (Spain) between 1 January and 30 June 2017. We analysed the dispensation registry of community and hospital pharmacies from the Madrid Regional Health Service (SERMAS) for this period, look- ing specifically at PIMs among older PLWH according to the 2019 American Geriatrics Society (AGS) Beers criteria [4]. Older PLWH were defined as those ≥ 65 years old. The SERMAS registry permits access to demographics and all prescription drugs [antiretrovirals (ARVs) and nonan- tiretroviral medications (co-medications)]. ARVs were cat- egorized according to class. Co-medications were classified according to the Anatomical Therapeutic Chem- ical (ATC) classification system. Nonantiretroviral polypharmacy (polypharmacy hereafter) was defined as the intake of at least five co-medications. For the descriptive study, values are expressed as abso- lute numbers and percentages, and as medians and interquartile ranges (IQRs). Logistic regression analysis was used to investigate factors associated with PIMs. The variables analysed included age, gender, and polypharmacy. IBM SPSS STATISTICS FOR WINDOWS version 21.0 (Armonk, NY, USA) was used for all calculations. All sta- tistical tests were two-sided, and a P-value of < 0.05 was considered statistically significant. Results During the study period, 6 636 451 different individuals received medications in the region of Madrid; of these individuals, 22 945 received ARVs, and of these, 1292 (5.6%) were older PLWH. The median (IQR) age was 69 (67–73) years, and 1027 (79.5%) were male. Overall, 1135 (87.8%) older PLWH were taking at least one co-medica- tion, and polypharmacy was observed in 852 individuals (65.9%). A full description of ARV use in older PLWH is shown in Table S1. The distribution of anchor ARVs per patient was as follows: integrase strand transfer inhibi- tors (INSTIs), 645 (49.9%); nonnucleoside reverse tran- scriptase inhibitors (NNRTIs), 566 (43.8%); and ritonavir- or cobicistat-boosted protease inhibitors (PIs), 328 (25.4%). The most frequently prescribed anchor ARVs were dolutegravir (29.6%), boosted darunavir (18.8%), and rilpivirine (15.9%). Overall, 1023 (79.2%) older PLWH were treated with at least one NRTI. The most frequently used NRTI combinations were abacavir/lami- vudine (44.7%), tenofovir disoproxilfumarate/emtric- itabine (23.6%), and tenofovir alafenamide/emtricitabine (8.0%). A full description of the co-medications prescribed to older PLWH classified by ATC therapeutic subgroup is shown in Table S2. The most frequently prescribed co- medications were cardiovascular drugs (C), 900 patients (69.7%); gastrointestinal and metabolism drugs (A), 881 patients (68.2%); nervous system drugs (N), 788 (61.0%); blood drugs (B) 504 (39.0%); and systemic anti-infectives (J), 435 (33.7%). At least one PIM was identified in 482 (37.3%) older PLWH; in these patients, the most frequent ATC classes involved in PIMs were nervous system drugs N (28.6%), musculoskeletal system drugs (M; 10.8%), A (5.6%), and C (4.7%) (see Fig. 1 for a full description). Thirty-one dif- ferent co-medications caused 667 PIMs among 482 older PLWH; of these PIMs, 293 (60.8%) involved benzodi- azepines, and 131 (27.2%) involved nonsteroidal anti- inflammatory drugs (NSAIDs; see Table 1 for a detailed description). Five co-medications accounted for 84.8% of PIMs: lorazepam (38.2%), ibuprofen (18.0%), diazepam (10.2%), metoclopramide (9.9%), and zolpidem (8.5%). A total of 72 (14.9%) PIMs involved anticholinergic drugs, the most frequent of which were amitriptyline © 2020 British HIV Association HIV Medicine (2020) 2 B L�OPEZ-CENTENO et al. (1.3%), butylscopolamine (1.3%), dexchlorpheniramine (1.0%), hydroxyzine (0.7%), and cyclobenzaprine (0.5%). Factors independently associated with PIM were polypharmacy [adjusted odds ratio (aOR) 7.08; 95% CI 5.16–9.72] and female sex (aOR 1.75; 95% CI 1.30–2.35). Discussion In this population-based study of 1292 older PLWH, two- thirds of whom experienced polypharmacy, a PIM according to the 2019 AGS Beers criteria was identified in 37% of study participants. Benzodiazepines and NSAIDs were the most common inappropriate drugs pre- scribed, and female sex and polypharmacy increased the risk of having a PIM. Inappropriate prescribing is frequent in older individu- als, including older PLWH, and has been associated with adverse health outcomes [8-14]. The literature in older PLWH has focused mainly on drug–drug interactions (DDIs) involving ARVs, with only a few studies having focused explicitly on prescribing issues. In a retrospective study of 89 PLWH aged ≥ 60 years, mainly male Cau- casians, 52% had at least one PIM based on the 2012 AGS Beers criteria [15]. In this study, the main drugs involved in PIMs were testosterone, ibuprofen, zolpidem, and lorazepam, and 17% of PLWH received anticholiner- gic drugs. In a prospective study involving 248 PLWH aged ≥ 50 years, two-thirds of whom were male [16], PIMs were identified in 63% and 54% of individuals according to the 2012 AGS Beers and the STOPP/START criteria, respectively. Benzodiazepines, NSAIDs, first-gen- eration antihistamines, tricyclic antidepressants, and nonbenzodiazepinehypnotics were the most common PIMs, according to the 2012 AGS Beers criteria [16]. In a retrospective study of the Swiss HIV Cohort, two-thirds of 111 PLWH aged ≥ 75 years, mainly male, had at least one potentially inappropriate prescribing issue according to the 2012 AGS Beers and STOPP/START criteria [17]. Potential prescribing errors in this last study included unadjusted dosage, no indication, medication omission, medication not appropriate in older individuals, deleteri- ous DDIs, and treatment duration exceeding recommen- dations; of note, the proportion of patients with more than one prescribing issue was significantly higher in those with polypharmacy. The prevalence of PIM in our study is lower than the prevalences reported in previous studies performed in elderly PLWH [15-17] or in elderly Spanish unin- fected individuals [18], a discrepancy that is probably explained by the fact that we focused exclusively on the prescription of inappropriate drugs and not on other issues such as medication omission, inappropriate dosing, or no indication. Consistent with data from studies in elderly individuals with and without HIV infection, the most frequently observed PIMs were ben- zodiazepines and NSAIDs [3,15,16]. Older PLWH have increased sensitivity to benzodiazepines and decreased metabolism of long-acting agents; in general, all ben- zodiazepines increase the risk of cognitive impairment, delirium, falls, fractures, and motor vehicle crashes in older adults. NSAIDs increase the risk of gastrointesti- nal bleeding or peptic ulcer disease in high-risk adults and can increase blood pressure and induce kidney injury [4]. Fig. 1 Potentially inappropriate medications (PIMs) prescribed to 1292 older people living with HIV (PLWH)according to Anatomical Therapeutic Chemical (ATC) drug class. GI, gastrointestinal. © 2020 British HIV Association HIV Medicine (2020) PIMs in older PLWH 3 In our study, 15% of older PLWH received anticholin- ergic drugs, medications to be avoided in older people because they are associated with a wide variety of adverse effects, both peripheral (constipation, oral and ocular dryness, tachycardia and urinary retention) and central (agitation, confusion, delirium, falls, hallucina- tions and cognitive disorders), to which this population is particularly susceptible [4]. Factors independently associated with increased risk of PIM in our study included polypharmacy, something fre- quently found in other studies, and female sex. Gender- related differences in polypharmacy could explain the increased risk of having a PIM in women. Some of these differences may be explained by the more frequent con- tact with the health care system among women, which may provide women with extra opportunity for detection of diseases and receipt of prescriptions, and also gender- related biological differences in the occurrence of specific comorbidities associated with a chronic need for medica- tion [19,20]. Our study is limited by the absence of information about comorbidities, by the lack of information about the medical management of patients, including potential dosage adjustments, and by the absence of information Table 1 Prevalence of co-medications involved in potentially inappropriate medications (PIMs) according to the 2019 AGS Beers criteria in 1292 older people living with HIV (PLWH) Co-medication (ATC code) Older PLWH n (%) Recommendation Rationale Lorazepam (N05BA06) 184 (14.2) Avoid Older adults have increased sensitivity to benzodiazepines and decreased metabolism of long- acting agents; in general, all benzodiazepines increase risk of cognitive impairment, delirium, falls, fractures, and motor vehicle crashes in older adults Diazepam (N05BA01) 49 (3.8) Alprazolam (N05BA12) 21 (1.6) Potassium clorazepate (N05BA05) 17 (1.3) Clonazepam (N03AE01) 15 (1.2) Flurazepam (N05CD01) 6 (0.5) Triazolam (N05CD05) 1 (0.1) Zolpidem (N05CF02) 41 (3.2) Avoid Adverse events similar to those of benzodiazepines in older adults Amitriptyline (N06AA09) 17 (1.3) Highly anticholinergic Hydroxyzine (N05BB01) 9 (0.7) Highly anticholinergic Paroxetine (N06AB05) 6 (0.5) Highly anticholinergic Phenobarbital (N03AA02) 2 (0.2) High rate of physical dependence Clomipramine (N06AA04) 1 (0.1) Highly anticholinergic Ibuprofen (M01AE01) 87 (6.7) Avoid chronic use Increased risk of gastrointestinal bleeding or peptic ulcer disease in high-risk groups: use of proton-pump inhibitor or misoprostol reduces but does not eliminate risk. Also, can increase blood pressure and induce kidney injury Naproxen (M01AE02) 24 (1.8) Diclofenac (M01AB05) 13 (1.0) Meloxicam (M01AC06) 5 (0.4) Indometacin (M01AB01) 2 (0.2) Cyclobenzaprine (M03BX08) 7 (0.5) Avoid Most muscle relaxants poorly tolerated by older adults because some have anticholinergic adverse effects Methocarbamol (M03BA03) 2 (0.2) Metoclopramide (A03FA01) 48 (3.7) Avoid Extrapyramidal effects Butylscopolamine (A03BB01) 17 (1.3) Highly anticholinergic Glimepiride (A10BB12) 4 (0.3) Higher risk of severe prolonged hypoglycaemia in older adults Glibenclamide (A10BB01) 3 (0.2) Higher risk of severe prolonged hypoglycaemia in older adults Doxazosin (C02CA04) 39 (3.0) Avoid Worse outcomes Digoxin (C01AA05) 15 (1.2) Avoid dosages > 0.125 mg/day Not used in first-line treatment of atrial fibrillation or of heart failure Nifedipine (immediate release) (C08CA05) 5 (0.4) Avoid Worse outcomes Dronedarone (C01BD07) 2 (0.2) Avoid Potential for hypotension Dexchlorpheniramine (R06AB02) 13 (1.0) Avoid Highly anticholinergic Megestrol (L02AB01) 10 (0.8) Avoid Increases risk of thrombotic events Nitrofurantoin (J01XE01) 2 (0.2) Avoid for long- term suppression Potential for pulmonary toxicity AGS, American Geriatrics Society; ATC, Anatomical Therapeutic Chemical. © 2020 British HIV Association HIV Medicine (2020) 4 B L�OPEZ-CENTENO et al. about clinical outcomes of patients with PIMs. Further- more, over-the-counter drugs were not included in the analysis and therefore the use of medications like ibupro- fen and related PIM might have been underestimated. The strengths of our study include its population-based design, the large sample size, and the automatic retrieval of both ARVs and co-medications from an official com- prehensive prescription database. In conclusion, we found that, in the region of Madrid, PIM was highly prevalent in older PLWH, particularly among women and individuals with polypharmacy, and involved mainly benzodiazepines and NSAIDs. Interven- tions to limit PIMs include education about prescribing principles in older PLWH as well as medication reconcili- ation, review and prioritization according to the risks/ benefits for a given patient to prevent unnecessary polypharmacy and prescription of harmful medications. In this regard, a multidisciplinary team approach is advised to optimize treatment of multimorbid older PLWH. Importantly, consultation length should be adapted to allow sufficient time to review prescriptions, particularly in complex and vulnerable patients. Finally, future work should aim to develop computerized pre- scription systems integrating several tools to screen for DDIs and inappropriate drug use to assist clinicians effi- ciently with the identification and prevention of prescrib- ing errors [13,21]. Acknowledgements The authors thank the physicians and pharmacists from ServicioMadrile~no de Salud, Spain; the Ontology Engi- neering Group, Escuela T�ecnica Superior de Ingenier�ıa Inform�atica, Universidad Polit�ecnica de Madrid, Spain; and the Department of Pharmacology, Facultad de Far- macia, Universidad Complutense de Madrid, Spain. Conflicts of interest: JB received a research grant from MSD that made it possible to carry out this project, hono- raria for talks and membership of advisory boards from MSD, Gilead, Janssen, and ViiV, and research grants from MSD, Gilead, and ViiV. The following authors declare that they have no conflict of interests in relation to com- panies or other entities that have an interest in informa- tion in this manuscript: BL-C, CB-O, AM-S, JMB, LP-L, JCL, JB, SK, CM, and MJC-A. Financial disclosure: This work was supported by the Merck Sharp & Dohme Investigator Studies Program (IISP 54912). The work was also funded by the RD16/0025/ 0017 project as part of the Plan Nacional R + D + I and cofunded by Instituto de Salud Carlos III-Subdirecci�on General de Evaluaci�on and the FondoEuropeo de Desar- rollo Regional. Author contributions BL-C and JB conceived the study. CM and SK made sub- stantial contributions to the conception and design of the study. BL-C, CB-O, and JMB analysed the data. AM-S, LP-L, JCL, JB, and MJC-A made substantial contributions to the acquisition of data. BL-C and JB drafted the manu- script, and all authors revised it critically and approved the final version. References 1 Renom-Guiteras A, Meyer G, Thurmann PA. The EU(7)-PIM list: a list of potentially inappropriate medications for older people consented by experts from seven European countries. Eur J Clin Pharmacol 2015; 71: 861–875. 2 Lopez-Centeno B, Badenes-Olmedo C, Mataix-Sanjuan A et al. Polypharmacy and drug-drug interactions in HIV- infected subjects in the region of Madrid, Spain: a population-based study. Clin Infect Dis 2019: ciz811. https:// doi.org/10.1093/cid/ciz811 3 Motter FR, Fritzen JS, Hilmer SN, Paniz EV, Paniz VMV. Potentially inappropriate medication in the elderly: a systematic review of validated explicit criteria. Eur J Clin Pharmacol 2018; 74: 679–700. 4 By the 2019 American Geriatrics Society Beers Criteria� Update Expert Panel. American Geriatrics Society 2019 updated AGS Beers Criteria� for potentially inappropriate medication use in older adults. J Am Geriatr Soc 2019; 67: 674–694. 5 Gallagher P, Ryan C, Byrne S, Kennedy J, O’Mahony D. STOPP (Screening Tool of Older Person’s Prescriptions) and START (Screening Tool to Alert doctors to Right Treatment). Consensus validation. Int J Clin Pharmacol Ther 2008; 46: 72–83. 6 Justice AC. HIV and aging: time for a new paradigm. Curr HIV/AIDS Rep 2010; 7: 69–76. 7 Guaraldi G, Orlando G, Zona S et al. Premature age-related comorbidities among HIV-infected persons compared with the general population. Clin Infect Dis 2011; 53: 1120– 1126. 8 Hamilton H, Gallagher P, Ryan C, Byrne S, O’Mahony D. Potentially inappropriate medications defined by STOPP criteria and the risk of adverse drug events in older hospitalized patients. Arch Intern Med 2011; 171: 1013– 1019. 9 Dalleur O, Spinewine A, Henrard S, Losseau C, Speybroeck N, Boland B. Inappropriate prescribing and related hospital admissions in frail older persons according to the STOPP and START criteria. Drugs Aging 2012; 29: 829–837. 10 Reich O, Rosemann T, Rapold R, Blozik E, Senn O. Potentially inappropriate medication use in older patients in © 2020 British HIV Association HIV Medicine (2020) PIMs in older PLWH 5 "
"","  Semantic Saturation in Retrospective Text Document  Collections      Victoria Kosa​1​, Eugene Yuschenko​2​, Carlos Badenes​3​, Vadim Ermolayev​1​,   and Aliaksandr Birukou​4    1​ Department of Computer Science, Zaporizhzhya National University,   Zhukovskogo st. 66, 69600, Zaporizhzhya, Ukraine  victoriya1402.kosa@gmail.com, vadim@ermolayev.com  2​ BWT Group, ​Mayakovskogo 11, 69035​, Zaporizhzhya, Ukraine  admin@groupbwt.com  3​ Ontology Engineering Group, Universidad Politecnica de Madrid, Madrid, Spain  cbadenes@fi.upm.es  4​ Springer Nature, ​…​, Heidelberg, Germany   Aliaksandr.Birukou@springer.com     Abstract. This paper presents the motivation for, planning of, and very first results of                             the PhD project by the first author. The objective of the project is to discover                               experimentally if the statistical representativeness (completeness) of a retrospective                 textual document collection within a single well circumscribed subject domain could                      be measured adequately by measuring the saturation of its semantic (terminological)                       footprint. The project is performed at the Dept. of Computer Science of Zaporizhzhya                       National University in cooperation with Universidad Politecnica de Madrid, BWT                     Group, and Springer Nature.     Keywords. ​Knowledge extraction from text, text mining, retrospective document                   collection, OntoElect, semantic saturation, completeness.    Key Terms. KnowledgeEngineeringMethodology, KnowledgeEngineeringProcess,         SUbjectExpert, KnowledgeEvolution, KnowledgeRepresentation    1  Introduction  This short paper presents a PhD project aimed at developing the methodological and                         instrumental components for measuring the representativeness of high-quality                 collections of text documents. It is assumed that the documents in a collection cover a                          single and well circumscribed Domain of Discourse and have a timestamp associated                        with them. A typical example of such a collection is the set of the full text papers of a                                professional journal or a conference proceedings series published from the first issue                        to date. The main hypothesis, put forward in this work, is that a collection can be                                considered as representative to describe the domain, in terms of its semantic                         (terminological) footprint, if any additions of extra relevant documents to the                       collection do not noticeably change this footprint. Such a collection could be further                          considered as complete and could be used for extracting domain semantic descriptions                           from it. In fact, the approach to assess the representativeness outlined above does so                             by evaluating the terminological saturation of a document collection.  It is well known that extracting knowledge from texts for developing domain                         ontologies is a complicated and laborious process which requires a substantial part of                        highly qualified human effort. So, knowing the smallest possible representative                     document collection for a domain is very important to efficiently develop ontologies                      with satisfactory domain coverage. Therefore, laying out a method to determine a                       saturated subset of documents within the collection is important. It is also important                        to make this method as efficient and automated as possible to lower the overhead on                              the core knowledge engineering workflow.  Yet one more dimension of complexity in the context of knowledge extraction from                           texts is terminological temporal drift. Indeed, the semantic footprint of a retrospective                      collection could change in time. So, it is not clear how could the saturated subset of                               the collection be formed to account for this drift.  The objective of the presented project is to develop and evaluate in industrial settings                          an efficient and effective experimental method, supported by an instrumental toolset,                       to determine saturated subsets of high-quality domain-bounded retrospective textual                   document collections. As a theoretical background, the project uses the OntoElect                      approach [1]. Term extraction from text is done in cooperation with the Ontology                           Engineering Group of the Universidad Politecnica de Madrid . The instrumental                     1 toolset is developed in cooperation with the BWT Group . The industrial case study,                          2 focused on the Knowledge Management domain, is performed in cooperation with the                       internal LOD project of Springer Nature .  3 The remainder of the paper is structured as follows. Section 2 presents the motivation                          for this project based on the brief analysis of the related work. Section 3 briefly                              outlines the OntoElect approach. Section 4 describes our experimental setting in terms                       of objectives, instruments, datasets, and workflow. Section 5 presents our early                      results. Finally, the plans for the future work are discussed in Section 6.   2  Related Work and Motivation   Perhaps one of the most comprehensive sources surveying the existing approaches                       and techniques for ontology learning from text is [7]. Another collection of research                          contributions in ontology learning and population, complementary to this review, is                      [8]. This is the research area which often combines linguistic and statistical methods                          to process text corpora and extract knowledge fragments in different forms: ranging                         from key phrases and their importance / frequency values (e.g. [4]) to simple ontology                           modules e.g. specified in SKOS [9]. The dominant approach to assess the quality of                             extracted knowledge is comparing the resulting artifact to a Golden Standard [10] in                        the domain. Golden Standards are however quite rarely available. Another way to                         evaluate if the result fits the domain requirements well is to check it against the set of                              1 ​http://www.oeg-upm.net/  2 ​http://www.groupbwt.com/  3 ​http://www.springernature.com/    competency questions [11], provided by the knowledge stakeholders in the domain.                       Unfortunately these experts are also not readily available in the vast majority of cases.                           Therefore an objective indirect method to extract knowledge for producing ontologies                      from a representative document collection for the domain is on demand. An important                         question to answer in this context is: what is the minimal subset of a (potentially very                              big) document collection which is terminologically complete in statistical terms. The                      project presented in this paper aims at developing such an experimental method base                           on the OntoElect approach for ontology. It also aims at a thorough experimental                         evaluation of this method.   3  OntoElect Saturation Metric and Measurement  OntoElect uses the notion of ontology fitness, which is central for evaluating the                          quality of an ontology, in a meaning similar to that of [2], comprising: (i) fitness to                              the goals of the domain knowledge stakeholders; and (ii) fitness to an existing                           knowledge repository. OntoElect extends this notion by providing the formal                     mechanism to measure fitness, based on the metaphor of the stakeholder votes.  OntoElect as a methodology seeks for maximizing the fitness of the developed                        ontology to what the domain knowledge stakeholders think about the domain. Fitness                         is measured as the stakeholders’ votes – a metric that allows assessing the                           stakeholders’ commitment to the ontology under development - reflecting how well                      their sentiment about the requirements is met. The more votes are collected – the                           higher the commitment is expected to be. If a critical mass of votes is acquired (e.g.                            50%+1), the ontology is considered to satisfactorily fit the requirements.   It is well known that direct acquisition of requirements from domain experts is not                            very realistic as they are very expensive and not really willing to do the work falling                                 out of their core activity. So, in this project we are focused on the indirect collection                                 of the stakeholders’ votes by extracting these from high quality and reasonably high                           impact documents authored by the stakeholders.   An important feature to be ensured for knowledge extraction from text collections is                          that the dataset needs to be statistically representative - to cover the opinions of the                              domain knowledge stakeholders satisfactorily fully. OntoElect suggests a method to                    measure the terminological completeness of the document collection by analyzing the                      saturation of terminological footprints of the incremental slices of the document                       collection - as e.g. reported in [3]. The full texts of the documents from the                              retrospective collection are grouped in datasets in the order of their timestamps. The                           first dataset contains the first portion of documents. The second dataset contains the                           first dataset plus the second portion of documents. Finally, the last dataset contains all                             the documents from the collection. At the next step of the OntoElect workflow the                             bags of multi-word terms are extracted from all the datasets using TerMine software                           [4] together with their ​significance (C-value) scores reflecting how often a term was                          met in the dataset. The workflow, presented below in Section 4 also suggests to use an                              alternative way to extract the bags of multi-word terms [5] for comparing the quality                             of term extraction. Further the bags of terms of adjacent datasets (1​st and 2​nd​, 2​nd and                                 3​d​, ...) are compared and the ​termhood difference (​thd ​ ) value is computed for each                              consecutive pair of datasets. Terminological ​saturation is assessed by comparing the                      overall ​thd to individual ​term significance threshold. A dataset for which stable                         4 saturation is observed is further considered as ​complete (and statistically                     representative) for knowledge extraction.   4  Experimental Settings and Workflow  The objective of the presented experimental research project is to check if the                           OntoElect approach to assess the representativeness of a subset within a document                       collection, based on measuring terminological saturation, is valid. The setting of the                        experiments should also consider several parameters which may influence the                     measurements and, therefore the results of measuring saturation. These parameters are                      taken into account while answering the following research questions:  Q1​: Which would be the proper direction to check saturation: chronological,                       counter-chronological, bi-directional, stochastic selection? Which direction is the                 most appropriate to cope with potential terminological drift in time?  Q2​: Would frequently cited documents form a minimal representative subset of                      documents? Do the most frequently cited documents indeed provide the biggest                       terminological contribution to the document collection?  Q3​: Would the size of a dataset increment influence saturation measurements? Is                        there an optimal size of a data chunk for the purpose?  Q4​: Which of the term extraction methods (UPM [5] or Manchester TerMine [4])                           yield more adequate and quality termhoods?   Q5​: Is the method for assessing completeness based on saturation measurements                       valid? Does it indeed provide a correct indication of statistical representativeness?   The answers to the outlined research questions are sought based on conducting                         experiments in real world industrial settings. For that the document collection has                         been formed in cooperation with Springer Nature. Based on the expert advice of the                             partner, fifteen Springer journals have been selected that are broadly relevant to the                           5 domain of Knowledge Management .   6 The chosen collection of journal papers appears to be well suited to attack the                             outlined research questions. Indeed, It is formed of the journals scoping into different                        subfields of Computer Science at broad. The journals in the selection are however                           mutually complementary in terms of providing terminology related to Knowledge                     4 ​A term in a termhood is ​significant if its score puts it in the upper part of the scored list. The                                        upper part forms the prevailing sentiment of the domain knowledge stakeholders - the majority                            vote - as it accumulates 50%+1 stakeholder votes in the terms of the sum of the normalized                                 scores of the respective terms.   5 ​The list of the selected journals is available at: ​https://github.com/bwtgroup/SSRTDC-Paper  Catalogues/blob/master/ListOfJournals.xls  6 ​Knowledge Management has been chosen as a target domain because: (i) the methodology                            developed in the presented experimental study is for knowledge engineering and management;                        (ii) the partners in the presented project possess extensive expertise in Knowledge Management                         and therefore could be used as subject experts; (iii) there is a substantially big collection of                               high-quality full text documents broadly relevant to this domain available at Springer Nature.     Management. So there seems to be a balance between the broadness of the overall                            scope and the focus on the target domain. This balance needs to be checked                             experimentally by verifying if it contains a saturated terminological footprint on the                      domain. Furthermore, individual journal collections chronologically start at very                   different times and contain quite different numbers of volumes, issues and papers. So,                          these internal disbalances may really help reveal the complications like terminological                       temporal drift and different terminological contributions caused by varying data                     volumes coming from different journals.    The experimental workflow is based on the OntoElect workflow described in Section                       3 and is outlined in Fig. 1. This workflow could be generically applied (using                             Configure Experiment step) to perform all the series described below.         Fig. 1​. Planned experimental workflow    Different kinds of experiments, using this workflow, are planned to be conducted in                          the presented study.   The first series of experiments is targeted at checking which direction of choosing                         papers for the datasets yields better saturated termhoods and assesses terminological                       temporal drift. In this series the experimental workflow is applied to the datasets                          which are formed: (i) chronologically; (ii) counter-chronologically; (iii)                 bi-directionally, i.e. including data increments containing the documents from both                     ends of the temporal span in turns (e.g. first issue, than last issue, than second issue,                                etc.); and (iv) stochastically, i.e. including documents picked from the data collection                           uniformly randomly. Saturation measures and saturated termhoods will be compared                     across these different choices. This series will allow answering ​Q1​.   The second kind of experiment will base on the most appropriate selection direction                           choice determined in the first series and investigate the terminological impact of the                           frequently cited documents in the collection. For that, the impact of each document                           will be computed based on its citation frequency. The documents with impact equal to                            n will be replicated ​n times in the corresponding dataset. The experimental workflow                          will be repeated for these “impact” datasets and the results will be compared to the                               first series using “flat” datasets. The comparison will be done in terms of saturation                            measures and terminological contribution peaks. This experiment may allow to                     answer ​Q2 and extract the “decisive minority vote” subset of terms for Knowledge                           Management, contributed by the high-impact papers, as e.g. been done in [3] for Time                           Representation domain.   To answer ​Q3​, the third series will focus on finding out what might be the optimal                                 size of an increment to form experimental datasets. For this series, the datasets will be                              formed following the best selection direction discovered in the first series. The size of                            the increments will however be varying. Saturation measurements will be compared                       for different data increment sizes and the optimal value will be discovered if such an                              optimum does exist.   The fourth series is planned for experimental cross-evaluation of the available                       alternative software tools for multi-word term extraction from texts. Based on the                         datasets with the increments of optimal size determined in the series No 3, term                             extraction will be done separately using the UPM toolset and TerMine. The results                           will be compared in terms of saturation measures for flat datasets and decisive                           minority subsets of terms extracted from the impact datasets (series No 2). This may                             allow to answer ​Q4​.   Perhaps, ​Q5 is the most difficult question to answer and it still requires some thinking                            for offering a convincing method to assess the adequacy and validity of the                           experimental method investigated in the presented project. One possible way is to do                         that based on the cross-evaluation with another method for ontology learning, e.g. [6].                           Another possible way is to select a much smaller subset of a document collection, e.g.                            only the papers with high terminological impact discovered in the series No 2. The                            termhood extracted from this “decisive minority vote” subset could be manually                       checked by human experts.   5  Early Results  The project has been started in November 2016 and is in its initial phase. Since it has                                been started the following steps have been accomplished: (i) the document collection                         has been chosen; (ii) the catalogue of the papers in the document collection has been                               created; (iii) the full texts of the papers have been downloaded and converted to PDF.   Overall the document collection contains more than 9 000 papers. The composition of                          the document collection is diagrammatically shown in Fig. 2. So, performing even                         those initial steps could not be done manually due to the volume and incurred manual                                effort. It has been therefore decided to develop some software instruments which help                         automate these routine steps.   For creating the catalogue of the papers which will further be used for generating                             datasets, a tailored parser has been developed. The parser receives a Springer journal                         7 web page URL as its input and stores the list of all the papers of this journal in the                                      specified .csv file . The information about a paper contains all its reference                        8 information, the abstract, and the no of citations acquired from Google Scholar.   For downloading the full texts of the papers another software module has been                           developed​7​. It receives a .csv list of papers to be downloaded and generates a script to                              download the full texts of the papers based on their DOI information taken from the                               catalogue. The papers in PDF are stored in a folder specified as a parameter.   One more software module has been developed​7 for batch conversions of paper full                           texts in PDF to plain text. It gets a path to the directory where PDF articles are stored,                                 as a parameter. It produces the outputs for each input file in plain text format.      Fig. 2. Distribution of papers in the journals of the document collection. Y-axis shows the                               years of publication, X-axis corresponds to the journals. The numbers in the bars are: no of                                 volumes, no of issues, no of papers for the corresponding journal.     Our next step is generating the incremental datasets using the plain texts of the papers                             in the directions and with increment sizes as specified in Section 4 for the first and                               7 All the developed instrumental software modules are available at:   https://github.com/bwtgroup/SSRTDC-Springer-article-parser  https://github.com/bwtgroup/SSRTDC-Collections-Springer-PDF-Downloader  https://github.com/bwtgroup/SSRTDC-PDF2TXT  8 ​The catalogues of the acquired journal papers in .XLSX format are available at:                             https://github.com/bwtgroup/SSRTDC-PaperCatalogues/​. The data has been collected on               December 3-4, 2016.   "
