
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Methodology}
\label{ch:hypothesis}

\graphicspath{{hypothesis/figures/}}

% -------------------------------------------------------------
% -- Research Objectives 
% -------------------------------------------------------------

The work presented in this thesis aims to facilitate the exploration of huge collections of multilingual documents through thematic associations inferred from their content. It assumes that, given a multilingual corpus, similar documents will contain the same topics. Each of the challenges arising from this objective defines a working dimension and guides the research carried out in this thesis.

% Hacer referencia a secciones de los capitulos 1 y 2 cuando est√©n terminados
The first dimension focuses on \textbf{scalability}, in order to create the text processing flows that are required to create or apply learning models. The workload required to process a corpus varies according to the number of documents, the length of texts and the kind of knowledge (annotations) that need to be inferred from the text. If the design of the workflow is scalable, there is no need to modify the processing logic when working with larger collections of documents, since adding a reasonable amount of computational resources is enough to perform it. These resources can be machines (i.e horizontal scaling) or processing units (e.g CPU, RAM) in an existing machine (i.e vertical scaling). 

The second dimension covers the \textbf{representativeness} of the text annotations when projected into spaces where they are manipulated. The idea behind these spaces is to represent documents as points (or vectors in a vector space) that are close together when the texts are semantically similar, and far apart when they are semantically distant. The ability of these spaces to create meaningful representations is also studied in this work.

In the third dimension, we explore the usage of data structures that efficiently \textbf{cluster} texts from their representations based on probabilistic topics. Divisions of space into semantically-related regions are convenient to allow browsing large document collections. The \textit{representativeness} covered in the previous dimension enables the interpretation of the relations and regions obtained.

And finally, the fourth dimension handles the \textbf{multilingualism} of collections that contain documents in several languages. On a multilingual space, documents are described and related across languages.

This chapter introduces our main hypothesis (section \ref{sec:research-hypothesis}), and the associated research challenges (section \ref{sec:research-challenges}), and presents the research methodology (section \ref{sec:research-methodology}).

\section{Research Hypotheses}\label{sec:research-hypothesis}

We define our main hypothesis as follows:

\textbf{Hypothesis 1} \textit{Large multilingual document collections can be automatically analyzed to discover appropriate thematic relations that facilitate a semantically-enabled text browsing}.

Our hypothesis can be divided into four different sub-parts, which are related to the aforementioned scalability, representativeness, clustering, and multilinguality dimensions. First, by \textit{distributing across different computation nodes both natural language processing tasks and representational models we can efficiently process huge collections of documents (\textbf{H1.1})}.

% topics no se ha utilizado hasta ahora?
Second, it is possible to \textit{semantically relate documents by comparing their most relevant topics (\textbf{H1.2})}. Furthermore, for this purpose we hypothesize that the use of \textit{topic hierarchies (\textbf{H1.2.1}) and similarity metrics based on relevance levels (\textbf{H1.2.2}) help quantifying the semantic distance between texts}. Third, by \textit{dividing the representational space into regions based on topics and relevance levels we can search for related documents without having to calculate all pairwise comparisons and without discarding the notion of topics for further processing (\textbf{H1.3})}.

And finally, \textit{by abstracting the topic representations into concept-based descriptions across languages we can relate documents in various languages without having to translate them (\textbf{H1.5})}.

A summary of the hypotheses and how they tackle our research dimensions can be found in Table~\ref{table:hypotheses}.

\begin{table}[!htbp]
\centering%
\begin{tabularx}{\linewidth}{bs}
\toprule
\heading{Hypothesis} & \heading{Research Dimension } \\
\midrule
\midrule
H1: Large multilingual document collections can be automatically analyzed to be semantically-browsed through thematic relations & D1: Scalability, D2: Representativeness, D3: Clustering, D4: Multilingualism \\
\midrule
H1.1: it is possible to efficiently annotate documents on a large scale by distributing across different computation nodes natural language processing tasks and representation models & D1: Scalability\\
\midrule
H1.2: it is possible to semantically relate texts from their most relevant topics & D2: Representativeness\\
\midrule
H1.3: it is possible to find relevant documents with similar topic distributions without calculating all pairwise comparisons and without discarding the notion of topics from their representation & D3: Clustering\\
\midrule
H1.4: it is possible to relate documents in different languages without having to translate them, by using language agnostic concepts from their main topics & D4: Multilingualism\\
\bottomrule
\end{tabularx}
\caption{Hypotheses and research dimensions.}
\label{table:hypotheses}
\end{table}


\section{Research Challenges}\label{sec:research-challenges}

Several research challenges emerge from these hypotheses. First, in order to facilitate reusing existing topic models by processing systems with different architectures and technological stacks, we need to define \textit{topic-model programming interfaces}. Second, in order to describe and thematically relate documents, we must address how to produce \textit{explainable topic-based associations}. Third, by working with huge collections of documents described by topics, we need to handle \textit{large-scale comparisons of topic distributions}. Finally, in order to explore multilingual document collections from shared topic-based representational spaces, we have to provide \textit{automatic cross-lingual topic alignment}. Each of these research challenges are described below and covered throughout this thesis.

\subsection{Scalable Creation and Inference of Topics}

Although some initiatives to facilitate the reuse of machine-learning models exist in the literature as discussed in section \ref{sec:topic-reuse}, there are still some restrictions that limit the use of topic models. Technical dependencies or closed data formats are the main reasons that prevent or make reproducibility of these models difficult by imposing conditions to work with them. It is common to provide the model only through its functionality, rather than its properties, which prevents it from being used for a different purpose than when it was created. \textit{Reuse of topic models is limited by incompatibility problems \textbf{(RCInterface1)}}.

The properties and functions offered by a topic model, instead of being uniform, vary according to the method used to build it. This limits the ability to reproduce the work done in this area and reuse topic models without losing information or knowledge. \textit{There is no standard to specify the attributes and operations that a topic model can provide \textbf{(RCInterface2)}}. Sometimes topics are described by the top ten or five most relevant words, and occasionally these word lists are not accompanied by weights, making a density-based analysis impossible. These differences in presenting the models can sometimes limit their reusability if they cannot infer new topic distributions even when the learning algorithm allows for it.


\subsection{Explainable Topic-based Relations}

In order to facilitate the exploration of document collections, vector space models are often used to semantically relate texts based on their word distributions. As described in Section \ref{sec:related-work}, these models first create a dictionary with the words used in the collection, and then represent  documents by vectors whose dimensions correspond to each word in the dictionary. In large collections, these models need to be adapted to make operations on vectors more manageable. As a result, a new abstraction method based on emerged topics that reduces the dimensions of vectors. Topics are described by word distributions over the entire vocabulary and documents by vectors containing topic distributions. Despite the extensive use of these representation models, \textit{there is no common criteria for identifying the most representative topics in a document \textbf{(RCExplainable1)}}. 

In addition, since similarity metrics over this representation space are based on accumulating the difference in topic densities, \textit{it is difficult to explain the distance between topic distributions \textbf{(RCExplainable2)}}. And, unless a minimum distance threshold can be defined or a set of n-top topics agreed, \textit{there is no common criterion for determining whether two documents are related \textbf{(RCExplainable3)}}.  


\subsection{Large-scale Comparisons of Topic Distributions}

There are many scenarios where we need to find related documents (e.g. a researcher doing literature review, or an R\&D  manager analyzing project proposals). Experts can benefit from discovering those connections to achieve these goals, but brute-force pairwise comparisons are not computationally adequate when the size of the corpus is too large. Some algorithms in the literature divide the search space into regions containing potentially similar documents, which are later processed separately from the rest in order to reduce the number of pairs compared. However, \textit{there are no mechanisms that efficiently partition the topic-based search space without compromising the ability for thematic exploration \textbf{(RCComparison1)}}.

In addition, documents from the same region should be compared and \textit{there are no similarity metrics that compare partial distributions of topics \textbf{(RCComparison2)}}.


\subsection{Unsupervised Cross-lingual Topic Alignment}

% actualizar referencias a secciones del sota
With the ongoing growth in the number of texts in different languages, we need annotation methods that enable browsing multilingual corpora. As discussed in section \ref{ch:soa}, multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, \textit{there are no approaches that abstract the representation of probabilistic topics in language-independent spaces without translating texts or aligning documents \textbf{(RCCrossLingual1)}}. Existing approaches require parallel or comparable training data to create a language-independent space. 

A summary of the challenges covered in this work and how they map to the hypotheses is presented in table  \ref{table:challenges}.

\begin{table}[!htbp]
\small
\centering%
\begin{tabularx}{\linewidth}{bb}
\toprule
\heading{Research Challenge} & \heading{Hypotheses} \\
\midrule
\midrule
RCInterface1: reuse of topic models is limited by incompatibility problems & H1.1: documents can be efficiently annotated on a large scale by distributing across different computation nodes both natural language processing tasks and representation models\\
\midrule
RCInterface2: there is no standard that unifies the representation of topic models & H1.1: documents can be efficiently annotated on a large scale by distributing across different computation nodes both natural language processing tasks and representation models\\
\midrule
RCExplainable1: there is no common criteria for identifying the most representative topics in a document & H1.2: texts can be semantically related from their most relevant topics, H1.3: documents with similar topic distributions can be found without calculate all pairwise comparisons and without losing the ability to explore them through their topics \\
\midrule
RCExplainable2: it is difficult to understand the distance between topic distributions & H1.2: texts can be semantically related from their most relevant topics\\
\midrule
RCExplainable3: there is no common criterion for determining whether documents are related & H1.2: texts can be semantically related from their most relevant topics\\
\midrule
RCComparison1: there are no mechanisms that efficiently partition the topic-based search space without compromising the ability for thematic exploration & H1.3: documents with similar topic distributions can be found without calculate all pairwise comparisons\\
\midrule
RCComparison2: there are no similarity metrics that compare partial distributions of topics & H1.3: documents with similar topic distributions can be found without calculate all pairwise comparisons\\
\midrule
RCCrossLingual1: there are no approaches to abstract probabilistic topics in language-independent spaces without translating texts or aligning documents  & H1.4: documents in different languages can be related without having to translate them using language agnostic concepts from their main topics\\
\bottomrule
\end{tabularx}
\caption{Open Research Challenges and Hypotheses.}
\label{table:challenges}
\end{table}

\section{Research Methodology}\label{sec:research-methodology}

The research presented in this thesis is based on four dimensions or research areas as discussed in section \ref{sec:research-challenges}. Each one is motivated by different research problems that we need to solve in order to achieve our ultimate goal of making it easier to explore large multilingual document collections through their topics. Once a dimension is tackled, the next one is considered, and so on. This iterative and incremental methodology allows  refining the research results by evaluating them with more experiments and addressing increasingly complex research problems.

Figure \ref{fig:dimensions} shows the dimensions on which the research of this thesis has been built. The top of the pyramid is only reached once the lower dimensions are dealt with successfully. They are presented as a chain of four steps. The first step describes the motivation to perform a given task coming from real-world problems that we had to deal with, and is represented by a brown arrow. In the context of this task, the research problem arises and is framed by a pink arrow. For each of them a solution is proposed and evaluated according to a specific criterion. The proposed solution is represented by a green arrow and the evaluation with a blue arrow. Once a proposal has been validated, the next dimension of the pyramid is achievable and all the previous research problems are added to the new research problem as conditions to be taken into account.

Technical objectives (i.e., develop a new resource) or research objectives (i.e., discover the solution to a problem) guide the solution proposal before moving on to the next dimension. They are presented below, organized by the research problem associated with each dimension.


\subsection{Scalable Creation and Inference of Topics}

This first dimension arose when we had to analyze a huge collection of documents describing research and innovation projects to discover which research areas are being addressed, measure their presence in the collection, and characterize them so their presence can be inferred in unseen documents. Such a high volume of data made difficult to process it manually, so we needed to automate the required processing to draw insights from it. Probabilistic topics allow describing research areas, so we defined a \textit{distributed text-processing model for creating large probabilistic topic models (\textbf{RO1})} and a \textit{Web service template to distribute them (\textbf{RO2})}. In this way, the models themselves could be easily integrated into scalable text processing pipelines. As a result, we created a \textit{scalable platform for topic modeling (\textbf{TO1})}, and produced a \textit{model-as-a-service repository with pre-trained topic models (\textbf{T02})}. The efficiency of this solution was validated by processing a corpus of 100,000 documents collected from the CORDIS dataset\footnote{\url{https://data.europa.eu/euodp/es/data/dataset/cordisH2020projects}}, which contains descriptions of projects funded by the European Union under a framework programme since 1990 \citep{Badenes-Olmedo2017}. 

The main contributions under this dimension are described in Chapter \ref{ch:scalability} as follows:
\begin{itemize}
\item a software architecture to process big volumes of textual documents in a distributed and decoupled manner;
\item the definition of a model-as-a-service template for probabilistic topic models;
\item an implementation of the architecture, librAIry, following those design principles.
\end{itemize} 


\subsection{Explainable Topic-based Relations}

In the second dimension we needed to browse scientific papers through their content-based relations. The problem of massively annotating documents with topic distributions came up. We had to \textit{create annotations based on topic models in a way that was computationally affordable and enabled a semantic-aware exploration of the knowledge inside them (\textbf{RO3})}. Once documents were annotated, a \textit{metric that compares documents and facilitates their interpretation from topic annotations (\textbf{RO4})} was required. As a result, we integrated \textit{the annotation method into the topic model service (\textbf{TO3})} and implemented a text comparison metric based on partial representations of topics. These proposals were validated by classifying 500,000 scientific articles from the Open Research Corpus\footnote{\url{https://allenai.org/data/open-research-corpus}} in domains such as Computer Science, Neuroscience and Biomedicine \citep{Badenes-Olmedo2017b, Badenes-Olmedo2017c, Badenes-Olmedo2019b}. 

The main contributions under this dimension are described in Chapter \ref{ch:explainability} as follows:
\begin{itemize}
\item a clustering algorithm based on probabilistic topic distributions;
\item a hash function to transform topic distributions into topic hierarchies;
\item a similarity metric based on topic sets.
\end{itemize} 

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.24]{dimensions.png}
\caption{Research dimensions of the thesis. The first ones must be overcome before reaching higher dimensions. }
\label{fig:dimensions}
\end{figure}

\subsection{Large-scale Comparisons of Topic Distributions}

This dimension covered the search for similar documents based on their most relevant topics. Thanks to having dealt with the above two dimensions, large collections of documents could be annotated with topic hierarchies and text distances could be measured from their annotations. Now, the aim was to find similar documents without losing the exploratory capacity offered by topics. Similarity comparisons were too costly to be performed in such huge collections of data and required more efficient approaches than having to calculate all pairwise similarities. We applied \textit{techniques based on approximate nearest-neighbors to organize documents in regions with similar topic hierarchies (\textbf{RO5})}. As a result, we developed \textit{a system to automatically find similar documents (\textbf{TO4})}. It was validated on a collection of one million texts retrieved from the United States patents corpus\footnote{\url{https://www.uspto.gov/ip-policy/economic-research/research-datasets}}. The relations between patents derived from their manual categorization were compared with those automatically obtained from their topic distributions \citep{Badenes-Olmedo2020, Badenes-Olmedo2019b}. 

The main contributions under this dimension are described in Chapter~\ref{ch:comparisons} as follows:
\begin{itemize}
\item a data structure to partition the search space and organize documents described by topic hierarchies;
\item a corpus browser that leverages these representations to automatically relate documents.
\end{itemize} 


\subsection{Unsupervised Cross-lingual Topic Alignment}

Finally, a new dimension on top of the previous ones emerged to relate texts coming from different languages. In particular, since document relations were based on their topics, this dimension was focused on aligning topics without supervision from models trained with texts in different languages. Since each language defined its own vocabulary, the topics were model-specific and could not be directly compared. We abstracted the \textit{topic representations to create a single space out of the particularities of the language (\textbf{RO6})}. This approach was validated on the English, Spanish, French, Italian and Portuguese editions of the JCR-Acquis\footnote{\url{https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis}} corpora and revealed promising results on classifying and sorting documents by similar content across languages \citep{Badenes-Olmedo2019, Badenes-Olmedo2019b}. 

The main contributions under this dimension are described in Chapter~\ref{ch:multilinguality}, as follows: 
\begin{itemize}
\item an algorithm to represent probabilistic topics using concept sets;
\item a repository of aligned topic models from the English, Spanish, French, Italian and Portuguese editions of the JRC-Acquis corpus.
\end{itemize}

Table \ref{table:objectives} summarizes the research objectives (ROs), technical objectives (TOs) and connects them with the research challenges (RCs) from Table \ref{table:challenges}.


\begin{table}[!htbp]
\centering%
\small
\begin{tabularx}{\linewidth}{bs}
\toprule
\heading{Research Objective} & \heading{Research Challenge}\\
\midrule
\midrule
RO1: Define a distributed text-processing model for creating large probabilistic topic models  & RCInterface1 \\
\midrule
RO2: Define a template to package probabilistic topic models as web services & RCInterface2\\
\midrule
RO3: Define annotations based on topics that enable a semantic-aware exploration of the knowledge inside a corpus & RCExplainable1\\
\midrule
R04: Define a metric based on topic annotations that compares documents and facilitates their interpretation & RCExplainable2, RCExplainable3\\
\midrule
RO5: Define nearest-neighbor techniques to organize documents in regions with similar topic hierarchies & RCComparison1, RCComparison2\\
\midrule
R06: Define a transformation of the topic-based annotations to create a unique representational space out of the particularities from each language & RCCrossLingual1\\
\midrule
TO1: Create a scalable platform for topic modeling & RCInterface1, RCInterface2\\
\midrule
T02: Create a repository of Topic-based web services & RCInterface2\\
\midrule
T03: Integrate the annotation method based on topic hierarchies into the topic model service & RCExplainable2, RCComparison2\\
\midrule
T04: Create a system capable of finding similar document automatically & RCExplainable2, RCExplainable3, RCComparison1, RCCrossLingual1\\
\bottomrule
\end{tabularx}
\caption{Research and technical objectives and their related challenges.}
\label{table:objectives}
\end{table}
