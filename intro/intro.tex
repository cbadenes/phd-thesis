
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Introduction}\label{ch:introduction}

\graphicspath{{introduction/figures/}}

% -------------------------------------------------------------
% -- Introduction
% -------------------------------------------------------------

% Motivation: 
% - Presentar el problema general y las soluciones propuestas por la tesis (enfoque top-down), en vez del enfoque técnico actual (bootom-up)
% - Buscar referencias que hablen del número de docs que se generan cada día, o cosas similares
% - Hablar de knowledge-intensive task, o de knowledge-workers, o similar

%%% <-- present the motivation

% huge data (https://www.nature.com/articles/nj7612-457a)
% large-scale
% Around 8 TB of textual data are consumed everyday by electronic media. 
Huge amounts of data, in the form of textual documents, are daily produced in digital format. Every second in the Internet more than two thousand blog entries are published, nine thousand tweets are written, and more than two million emails are sent\footnote{https://www.internetlivestats.com/one-second}. The number of scientific publications per year has increased by 8-9\%~ in the last decade\citep{Ware2018STM}. More than one million papers, about two per minute,  were submitted to the PubMed database, the leading database of references and abstracts on life sciences and biomedical topics, in the last year. Statistics on judicial activity is similar. More than 168,000 procedural documents and 3,000 judicial notices were published in the Official Journal of the European Union in 2019 \footnote{\url{https://curia.europa.eu/jcms/upload/docs/application/pdf/2020-05/ra_pan_2019_interieur_en_final.pdf}}. Unlike the academic domain where articles are mostly published in English, legal documents are usually available in multiple languages. The Court of Justice of the European Union had to translated over 1 million texts into its 24 official languages, with 552 possible language combinations, in just one year. These numbers make it virtually impossible for an expert in academic or legal domain to stay abreast by only reading a few articles or judgments nowadays. Navigate the growing torrent of textual data and reviewing its content is not only necessary, but has become a second job that they must reconcile with their daily tasks. 

Some initiatives based on document retrieval are underway in order to facilitate text review in big collections. Major digital publishers specialized in scientific\footnote{\url{https://www.nature.com}}, technical\footnote{\url{https://www.elsevier.com}}, and medical\footnote{\url{https://pubmed.ncbi.nlm.nih.gov}} content provide search engines to make it easier to browse their collections of scientific articles. Given a few keywords, a list of papers that mention them is retrieved and offered for reading. Legal documents are not covered by publishers, but by public administrations that offer it by specialized search services. The spanish\footnote{\url{https://www.oepm.es}}, american\footnote{\url{https://www.uspto.gov/}} and european\footnote{\url{https://www.epo.org}} patent and intellectual property registration offices, for example, allow exploring their patent collections by search engines guided by keywords and/or categories. This type of documents are manually categorized by their authors according to the International Patent Classification (IPC) system, where approximately 70,000 different codes are available for different technical areas. This label-based browsing\footnote{\url{https://patents.google.com}} has been also adopted by some academic search engines\footnote{\url{https://academic.microsoft.com}} to organize papers by research areas, or even by evaluation tasks to browse state-of-the-art methods\footnote{\url{https://paperswithcode.com}}. In natural language processing domain, for example, research papers are organized into 256 tasks such as 'knowledge representation', 'question-answer', 'machine translation' and so on. However, while there are initiatives to normalize the research areas, the use of standard labels to categorize research papers is stills insufficient and text processing is necessary to tag articles. One of the main reasons that limit its widespread use is the difficulty in correctly assigning existing labels to a nobel and innovative research method.

Along with searches based on keywords and categories, the third aspect aimed at facilitating the exploration of documentary corpus is the provision of related texts. A documentary exploration does not stop when a relevant article is found, but starts from its content shaping the area of interest. Most academic\footnote{\url{https://www.semanticscholar.org}} and legal\footnote{\url{https://patents.google.com}} search engines provide a list of related documents for each text and offer a navigation based on them. The relationship can be of reference, when documents are cited by others, or of content, when documents share a common thematic area. The chains of articles derived from that related content can lead to more complex structures when cross-relations are assumed. A document can be related to another that, in turn, is related to a third one that can be also related to the first article. This content-based exploration helps to browse a specific thematic area by offering, for example, a visual overview of an academic field using graphs of articles with similar content\footnote{\url{https://www.connectedpapers.com}}.


While these initiatives are valuable efforts to address access to huge amounts of documents, they are still insufficient to examine the content offered by their texts. New external conditions have appeared in the last years that impose new challenges when explore document collections by browsing their texts:
\begin{itemize}
\item Complexity: The huge number of documents has forced a reconsideration of the way to compare them in order to be able to deal with millions in a collection. The time required to compute each comparison should be reduced as much as possible.
\item Efficiency: The algorithms, besides being accurate enough, must be also efficient in order to be applied on a large scale. Brute-force techniques cannot be applied to compare all items in a huge corpus.
\item Transparency: The associations between documents must be explained in such a way that the relationship itself provides knowledge about the content of the texts. It is not enough that one text is related to another, it is necessary to explain why it is so. 
\item Multilinguality: In addition, the increasing availability of texts written in different languages also makes it necessary to address comparison in multilingual collections. In these collections, external translation systems cannot been considered, since they would increase processing costs and potentially introduce a bias in the relationships obtained. 
\end{itemize}

%such as SemanticScholar\footnote{\url{https://www.semanticscholar.org}}, GoogleScholar\footnote{\url{https://scholar.google.com}}, CORE\footnote{\url{https://core.ac.uk}} or BASE\footnote{\url{https://www.base-search.net}} have extended the original information of an article (i.e title, authors, abstract) with metrics that help the reader to filter the most relevant papers more quickly. They may include basic citation-based scores such as the total number of times an article has been cited, or more elaborate ones such as measuring the importance of an article based on the citations of the articles that cite it. This appreciation of the importance of a document has not been transferred to patent search engines. GooglePatents\footnote{\url{https://patents.google.com}} shows a list of key events for the patent application in order to get an idea of its impact but does not provide a numerical measure of its relevance. However, some categories that describe the type of a patent are provided to guide searches and relate documents. This idea is also present in academic searches. Microsoft Academic\footnote{\url{https://academic.microsoft.com}} provide a list of labels for each article that can be used as search criteria to group documents, and PapersWithCode\footnote{\url{https://paperswithcode.com}} defines domains and evaluation tasks to browse the state-of-the-art methods. In the natural language processing domain, for example, it organizes research papers into 256 tasks such as 'knowledge representation', 'question-answer', 'machine translation' and so on. This approach allows retrieving documents from a given area without the need to know their keywords.
    

This is aimed at facilitating the exploration of large document collections that contain texts written in different languages. We address the problem of comparing them on a large scale and enables a semantic-aware exploration of their content. Our proposal automatically discovers thematic associations between texts and organizes the collection of documents so that it can be efficiently and transparently browsed through related content regardless of their language.




%%%% <-- introduce the solution proposed by this thesis


%Huge amounts of data are produced or captured daily in any imaginable domain offering the possibility of extracting knowledge from them. Much of data are produced them are presented in the form of textual documents such as news articles in digital media or scientific papers in electronic journals.

% Experts who want to find relevant information in this plethora of documents need to browse through them by reviewing their content and filtering out those related to their interests. Understanding the data is essential to distinguish the valuable texts from others.
% ultima linea no clara "Understanding..". Explicar mejor


%\cite{Griffiths2007} argued the knowledge in a text arose from the relations established between their words through the concepts they evoked. Similarly \cite{Kenter2015} considered the knowledge in a collection of documents derives from the relations between them. The knowledge based on associations at different levels, i.e. words and documents, can then help to explore textual data guided by a particular interest.

% P1: Text Scalability -> Text Processing + Representational Models

%Texts usually contain noisy, non-relevant information. Keeping only what can bring value for the involved agents (general consumers, experts, companies, investors...) becomes a challenge. A necessary first step before using documents for knowledge-intensive tasks is to process them following different annotation techniques (entities, keywords, etc) to leverage their content. Multiple algorithms have been proposed to analyze texts for automatically producing such annotations, from minimal units such as terms and entities, to more general descriptors such as summaries or topics. This is a widely known problem in Artificial Intelligence, particularly in the Information Retrieval (IR) and Natural Language Processing (NLP) fields. However, existing solutions fall under strong technological constraints due to their atomic design. In order to reuse their implementations or the models created, it is usually necessary to use the technology on which they are based. \textit{Processing interoperability} has not been a priority issue in the design of these annotation algorithms. The reuse of the raw data created for a specific task is one of the main objectives imposed, without putting too much interest in how the algorithm (i.e its implementation) could be integrated into a specific software environment without the need to reimplement it. In this thesis we propose \textit{reusable text annotation models and scalable document processing pipelines to integrate them}. 
% Explicar mejor la frase "However, existing solutions fall under strong technological constraints due to their atomic design."
% No mencionar las limitaciones tan pronto "Processing...". Presentar los problemas y motivaciones en este punto, y llevar estas limitaciones para el final del capítulo (o incluso en el sota)



% P2: Representative Models -> Topic Models

% Explicar PORQUE se utilizan vectores para representar los textos (hay que centrarse en los problemas que se quieren resolver: navegar por grandes corpus)
%Once documents have been processed, texts are usually converted into numeric vectors to operate with them. The definition and number of dimensions for each vector are key aspects when vector space models (VSM) create their representations. In a common and simple approach, term frequencies guide the creation of a space where each word in the vocabulary is represented by a separate and orthogonal dimension. Term Frequency-Inversed Document Frequency (TFIDF) relativizes the relevance of each term with respect to the entire corpus. The loss of semantic information and the high-number of dimensions are the main drawbacks of this approach that lead to the emergence of other techniques. Among them, text embedding proposes transforming texts into low-dimensional vectors by prediction methods based on (i) word sequences or (ii) bag of words. The first approach assumes words with similar meanings tend to occur in similar contexts. It considers word order relevant and is based on Neural Models that learn word vectors from pairs of target and context words, where context words are taken as words observed to surround a target word. Document vectors are usually created by averaging the word vectors they contain or by considering them as target and context items. The second approach does not consider the order of the words to be relevant, but their frequency is. It assumes words with similar meanings will occur in similar documents. Topic models are an example of this approach where each document is expressed by the times that it contains each word, and common distributions of words in the collection emerge as topics. Document vectors are learned from the corpus as a probability distribution of how relevant that document is to a given number of topics (and thus a lower-dimensional space). Topic-based representations bring a lot of potential when applied over different IR tasks, as evidenced by recent works in different domains such as scholarly  \citep{Gatti2015}, health \citep{Lu2016,TapiNzali2017}, legal \citep{ONeill2017,Greene2016}, news \citep{He2017} and social networks \citep{Cheng2014a}. This thesis proposes a \textit{thematic and low-dimensional feature space suitable for document similarity tasks, especially on big real-world data sets, where documents are described by their most relevant topics}. 
% It is suitable for document similarity tasks, especially on big real-world data sets, because of topic distributions are continuous and not as sparse as discrete-term feature vectors.

% P3: Large Comparison

%However, document similarity comparisons are too costly to be performed in huge collections of data and require more efficient approaches than having to calculate all pairwise similarities. Using a naive approach creating a similarity matrix with all document comparisons takes $O(n^2)$ time (where $n$ is the number of documents), so obtaining all possible pairs of similarities in a large collection of documents (e.g. a corpus of 32 million patents) can be unfeasible because of the exponential cost of comparing every pair of elements. Many different approaches can be used to reduce this complexity. For instance, computation can be approximated by nearest neighbors (ANN) search problem. ANN search is an optimization problem that finds nearest neighbors of a given query in a metric space of $n$ points. Due to the low storage cost and fast retrieval speed, hashing is one of the most popular solutions for ANN search \citep{Zhen2016}. This technique transforms data points from the original feature space into a binary-code space, so that similar data points have larger probability of collision (i.e. having the same hash code). This type of formulation for the document similarity comparison problem has proven to yield good results in the metric space \citep{Krstovski2011} due to the fact that ANN search has been designed to handle distance metrics (e.g. cosine, Euclidean, Manhattan). But distance metrics between topic distributions should be information-theoretically motivated metrics ( e.g. Hellinger, Kullback-Leibler divergence, Jensen-Shannon divergence) since they compare density functions. The simplex spaces where topics are represented have also been  approximated by hash recently \citep{Mao2017}, but sacrificing the exploratory capabilities of topics to support document similarity. The notion of topics is lost and therefore the ability to make thematic explorations of documents. Moreover, metrics in the simplex space are difficult to interpret and the ability to explain the similarity score on the basis of the topics involved in the exploration can be helpful. This thesis proposes a \textit{hash function-based approach that allows efficiently searching for related documents while maintaining topic-based annotation, giving the reasons why two documents are related}. 


% P4: Multilinguality

%When the information extraction task is also cross-language, document retrieval must be independent of the language of the user's query. At execution time, the query in the source language is typically translated into a target language with the help of a dictionary or a machine-translation system. But for many languages we may not have access to translation dictionaries or a full translation system, or they can be expensive to apply in an online search system. In such situations it is useful to rely on smaller annotation units derived from the text so the full content does not need to be translated, for instance by finding correspondences with regard to the topics discussed. In this thesis we propose \textit{to automatically learn cross-lingual topics to browse multi-lingual document collections}.


% Approach - Very brief summary

%In this work we facilitate the exploration of large document collections with texts written in different languages. We address the problem of programmatically generating annotations for each of the items inside big collections of textual documents, in a way that is computationally affordable and enables a semantic-aware exploration of the knowledge inside it. Our proposal automatically discovers thematic associations between texts and organizes the collection so that it can be browsed through related content.


\section{Contributions}

% introducir antes las contribuciones, un poco de bla bla
The work presented in this thesis makes the following contributions:

\begin{itemize}
\item \textbf{Large-scale Topic-based Text-processing Pipeline}: We define a scalable text processing pipeline following web standards and software best practices for the creation and exploitation of probabilistic topic models.
\item \textbf{Topic Model-as-a-Service}: We propose a format to distribute and reuse probabilistic topic models.
\item \textbf{Hierarchical Thematic Annotations}: We present a method to annotate texts by topic hierarchies automatically inferred from their content.
\item \textbf{Massive Document Comparisons}: We leverage multi-level topic annotations to efficiently index and retrieve related documents while allowing the exploration of the collection by the themes inferred from its texts.
\item \textbf{Cross-lingual Document Relations}: We introduce a technique to transform probabilistic topics from different languages into a single and shared representation space where texts can be thematically related regardless of the language used.
\end{itemize}

\section{Thesis Structure}

The thesis is structured as follows:

\textit{Chapter \ref{ch:soa}} analyses the state of the art and describes the main concepts handled throughout the thesis. \textit{Chapter \ref{ch:hypothesis}} presents the research problems and hypotheses that guide our work, and details the methodology that has been followed. \textit{Chapter \ref{ch:scalability}} describes the software architecture proposed to analyze huge document collections and the format suggested to distribute and reuse topic models on which the work presented in this thesis is built. \textit{Chapter \ref{ch:explainability}} details the text annotation algorithm from probabilistic topics. \textit{Chapter \ref{ch:comparisons}} shows how to efficiently store and search documents from large collections when they are annotated with topic hierarchies.  \textit{Chapter \ref{ch:multilinguality}} explains the method to relate texts written in different languages from their main topics without the need for translation. This approach is evaluated in \textit{Chapter \ref{ch:evaluation}}, where the results are explained in detail. \textit{Chapter \ref{ch:experiments}} provides information on real-world projects where contributions from this thesis have been used. Finally, \textit{Chapter \ref{ch:conclusion}} describes conclusions and future lines of work.


\section{Publications}

The following publications support the research work presented in this thesis:

% relacionar los papers a los capítulos, en vez de orden cronológico.

\begin{itemize}
\item \textit{Chapter \ref{ch:scalability}}:
\begin{itemize}
\item \textbf{Carlos Badenes-Olmedo}, José Luis Redondo-Garcia, and Oscar Corcho. Distributing Text Mining tasks with librAIry. Proceedings of the 17th ACM Symposium on Document Engineering (DocEng). Association for Computing Machinery, Valletta, Malta. 2017.
\item Victoria Kosa, Alyona Chugunenko, Eugene Yuschenko, \textbf{Carlos Badenes-Olmedo}, Vadim Ermolayev, and Aliaksandr Birukou. Semantic saturation in retrospective text document collections. Information and Communication Technologies in Education, Research, and Industrial Applications (ICTERI) PhD Symposium, vol. 1851, pages 1-8. CEUR-WS. 2017
\item Victoria Kosa, David Chaves-Fraga, Dmitriy Naumenko, Eugene Yuschenko, \textbf{Carlos Badenes-Olmedo}, Vadim Ermolayev, Aliaksandr Birukou, Nick Bassiliades, Hans-Georg Fill, Vitaliy Yakovyna, Heinrich C. Mayr, Mykola Nikitchenko, Grygoriy Zholtkevych, and Aleksander Spivakovsky. Cross-Evaluation of Automated Term Extraction Tools by Measuring Terminological Saturation. Information and Communication Technologies in Education, Research, and Industrial Applications, pages 135-163. Springer International Publishing. 2018
\end{itemize}
\item \textit{Chapter \ref{ch:explainability}}:
\begin{itemize}
\item \textbf{Carlos Badenes-Olmedo}, José Luis Redondo-García, and Oscar Corcho. Efficient Clustering from Distributions over Topics. Proceedings of the 9th International Conference on Knowledge Capture (K-CAP), Article 17, 1–8. Association for Computing Machinery, Austin, TX, USA. 2017.
\item José Manuel Gómez-Pérez, Ronald Denaux, Daniel Vila and \textbf{Carlos Badenes-Olmedo}. Hybrid Techniques for Knowledge-based NLP - Knowledge Graphs meet Machine Learning and all their Friends. Proceedings of Workshops and Tutorials of the 9th International Conference on Knowledge Capture (K-CAP), 69–70. CEUR-WS, Austin, TX, USA. 2017
\item \textbf{Carlos Badenes-Olmedo}, Jose Luis Redondo-Garcia, and Oscar Corcho. An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts. Proceedings of the 1st Workshop on Enabling Open Semantic Science (SemSci) co-located with 16th International Semantic Web Conference (ISWC 2017), 15-22. Vienna, Austria. 2017.
\end{itemize}
\item \textit{Chapter \ref{ch:comparisons}}:
\begin{itemize}
\item \textbf{Carlos Badenes-Olmedo}, José Luis Redondo-García, and Oscar Corcho. Large-scale Semantic Exploration of Scientific Literature Using Topic-based Hashing Algorithms. Semantic Web, vol. Pre-press, no. Pre-press, pp. 1-16. 2020
%\item Borja Lozano, \textbf{Carlos Badenes-Olmedo} and Oscar Corcho. Hierarchical representations of topics to uncover the underlying knowledge of semantically related texts. Proceedings of the 22nd International Conference on Knowledge Engineering and Knowledge Management (EKAW), (under revision). 2020
%\item \textbf{Carlos Badenes-Olmedo}, David Chaves-Fraga, Maria Poveda-Villalon, Ana Iglesias-Molina, Pablo Calleja, Socorro Bernardos, Patricia Martín-Chozas, Alba Fernández-Izquierdo, Elvira Amador-Dominguez, Paola Espinoza-Arias, Luis Pozo, Edna Ruckhaus, Esteban Gonzalez-Guardia, Raquel Cedazo, Beatriz Lopez-Centeno, and Oscar Corcho. Drugs4Covid: Making drug information available from scientific publications. Proceedings of the 19th International Semantic Web Conference (ISWC), (under revision). 2020
\end{itemize}
\item \textit{Chapter \ref{ch:multilinguality}}:
\begin{itemize}
\item \textbf{Carlos Badenes-Olmedo}, José Luis Redondo-García, and Oscar Corcho. Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies. Proceedings of the 10th International Conference on Knowledge Capture (K-CAP). Association for Computing Machinery, 147–153. Marina Del Rey, CA, USA. 2019
\item \textbf{Carlos Badenes-Olmedo}, José Luis Redondo-García, and Oscar Corcho. Legal document retrieval across languages: topic hierarchies based on synsets. arXiv e-prints, arXiv:1911.12637. 2019
\item Ahmet Soylu, Oscar Corcho, Brian Elvesaeter, \textbf{Carlos Badenes-Olmedo}, Francisco Yedro, Matej Kovacic, Matej Posinkovic, Ian Makgill, Chris Taggart, Elena Simperl, Till C. Lech, and Dumitru Roman. Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph. Proceedings of the 19th International Semantic Web Conference (ISWC), (under revision). 2020
\end{itemize}
\end{itemize}


%\item \textbf{Carlos Badenes-Olmedo}, Ronald Denaux, Martine De Vos, Daniel Garijo, José Manuel Gómez-Pérez, Agnieszka Lawrynowicz, Pasquale Lisena, Raul Palma, Raphaël Troncy, and Daniel Vila. K-CAP2017 Satellites: Workshops and Tutorials. Proceedings of the 9th International Conference on Knowledge Capture (K-CAP), Article 1e, 1–6. Association for Computing Machinery, Austin, TX, USA. 2017

%\item Beatriz López-Centeno, \textbf{Carlos Badenes-Olmedo}, Ángel Mataix-Sanjuan, Katie McAllister, José M Bellón, Pascual Balsalobre,  Juana Benedí, Saye Khoo, María J Calvo-Alcántara, and Juan Berenguer. Polypharmacy in HIV-infected and non-HIV-infected individuals in the region of Madrid (Spain): A population-based study. Proceedings of the 10th GeSIDA Conference, Spain. 2018.

%\item Beatriz López-Centeno, \textbf{Carlos Badenes-Olmedo}, Ángel Mataix-Sanjuan, Katie McAllister, José M Bellón, Sara Gibbons, Pascual Balsalobre, Leire Pérez-Latorre, Juana Benedí, Catia Marzolini, Ainhoa Aranguren-Oyarzábal, Saye Khoo, María J Calvo-Alcántara, Juan Berenguer. Polypharmacy and drug-drug interactions in HIV-infected subjects in the region of Madrid, Spain: a population-based study. Clinical infectious diseases : an official publication of the Infectious Diseases Society of America, ciz811. 2019



