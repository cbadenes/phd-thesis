{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKbr98mC/rIJqrZiMTRdEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/phd-thesis/blob/master/notebooks/soa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKXrGgKqX7u"
      },
      "source": [
        "This notebook supports the state-of-the-art content of the thesis: *Semantically-enabled Browsing of Large Multilingual Document Collections, Badenes-Olmedo, C. 2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXRIfz2kq9ee"
      },
      "source": [
        "# 2.- Techniques for Document Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvHN9zQapir8"
      },
      "source": [
        "The analysis of human-readable documents is a well-known problem in Artificial Intelligence (AI) in general, and in the Information Retrieval (IR) and Natural Language Processing (NLP) fields in particular. As an academic field of study, information retrieval might be defined as finding documents of an unstructured nature, usually text, that satisfies an information need from within large collections (Manning et al., 2008). As defined in this way, hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email. Information retrieval is fast becoming the dominant form of information access, surpassing traditional database searching where identifiers are needed to have results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFdYC8fo2Nl"
      },
      "source": [
        "There are two major categories of IR technology and research: semantic and statistical. Semantic approaches attempt to implement some degree of syntactic and semantic analysis. They try to reproduce to some degree the understanding of the natural language text that a human user would provide. In statistical approaches, the documents that are retrieved or that are highly ranked are those that match the query most closely in terms of some statistical measure. The work presented in this thesis follows this second approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw0G4p1geCAx"
      },
      "source": [
        "## 2.1.- Load Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQhyR8L0rGQX"
      },
      "source": [
        "An illustrative example may help to better understand IR techniques, so the publications listed in Section 1.1 are used as a sample collection for applying each of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0ngppsheGCe"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#increase the max column length\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "corpus_df = pd.read_csv('https://www.dropbox.com/s/pag5jseq2e9wcvb/corpus.csv?raw=1',usecols=['title','text'])\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKk9U8-hFBV"
      },
      "source": [
        "## 2.2. Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9JzJsJsIWf"
      },
      "source": [
        "Documents must be pre-processed to transform their texts into terms. These terms are the population that is counted and measured statistically. Most commonly, the terms are words (or combination of adjacent words or characters) that occur in a given query or collection of documents and often require pre-processing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd1VTpW4iru1"
      },
      "source": [
        "### 2.2.1: Methods to transform texts into terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_nSrGfgsnod"
      },
      "source": [
        "Words are reduced to a common base form by using a heuristic process that removes affixes, stemming, or by returning its dictionary form, lemma (Porter, 1997). The objective is to eliminate the variation that arises from the occurrence of different grammatical forms of the same word, e.g., ”program”, ”programming”, ”programs”, and ”programmed” should all be recognized as forms of the same word, ”program”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elq65_4Qsvjd"
      },
      "source": [
        "Another common form of pre-processing is the elimination of common words that have little power to discriminate relevant from non-relevant documents,e.g., ”the”, ”a”, ”it”. Hence, IR engines are usually provided with a stop-list of such noise words. Note that both stemming/lemma and stopwords are language-dependent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWhN4MhStpvQ"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = nlp(text)\n",
        "  return tokens\n",
        "\n",
        "def is_valid(token):\n",
        "  return len(token.text) > 1 and not token.is_stop\n",
        "\n",
        "def lemma(token):\n",
        "  return token.lemma_\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens = []\n",
        "  for token in tokenize(text):\n",
        "    if is_valid(token): \n",
        "      tokens.append(lemma(token))\n",
        "  return tokens\n",
        "\n",
        "print(\"methods created succesfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBhQ2lRUf6g"
      },
      "source": [
        "The following sentence taken from one of the documents can be used to see each of the steps: *”Probabilistic Topic Models reduce that feature space by annotating documents with thematic information”*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SKih16gUj5w"
      },
      "source": [
        "tokens = preprocess(\"Probabilistic Topic Models reduce that feature space by annotating documents with thematic information\")\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngevbE-2U2Gy"
      },
      "source": [
        "At this step ’annotating’ was transformed to ’annotate’ and ’documents’ was reduced to ’document’. However, ’Models’ remains unchanged. The reason is that since it starts with a capital letter, it is considered a proper noun. Finally, those words that appear in a stop-word list are removed (e.g. ’that’, ’by’ and ’with’). Each text is transformed into a normalized list of terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y562UNjCkYG9"
      },
      "source": [
        "### 2.2.2. Count words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhLgTKXSkak7"
      },
      "source": [
        "def count_words(text):\n",
        "  return len(text.split(\" \"))\n",
        "\n",
        "corpus_df['#words'] = corpus_df['text'].apply(count_words)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDhLJ7_QivyH"
      },
      "source": [
        "### 2.2.3. Tokenize Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrnhZZipjBjc"
      },
      "source": [
        "corpus_df['tokens'] = corpus_df['text'].apply(preprocess)\n",
        "\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4qWo8kujLZo"
      },
      "source": [
        "### 2.2.4. Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd0QX-rCjOZs"
      },
      "source": [
        "def count_tokens(tokens):\n",
        "  return len(tokens)\n",
        "\n",
        "corpus_df['#tokens'] = corpus_df['tokens'].apply(count_tokens)\n",
        "\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL1zTSPXktuE"
      },
      "source": [
        "### 2.2.5 Some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ddmT6Gkzvr"
      },
      "source": [
        "unique_tokens = []\n",
        "\n",
        "for pos in range(len(corpus_df.index)):\n",
        "  num_words = corpus_df['#words'][pos]\n",
        "  num_tokens = corpus_df['#tokens'][pos]\n",
        "  num_unique_tokens = len(set(corpus_df['tokens'][pos]))\n",
        "  unique_tokens.append(num_unique_tokens)  \n",
        "\n",
        "corpus_df['#uni_tokens']=unique_tokens\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjqffNPFmnyJ"
      },
      "source": [
        "## 2.3. Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajYqcsxZs2_E"
      },
      "source": [
        "Once all terms have been pre-processed, numerical weights are assigned to each them. The same term may have a different weight in each distinct document in which it occurs. The weight is usually a measure of how effective the given term is likely to be in distinguishing the given document from other documents in the given collection, and is often normalized to be a fraction between zero and one. Statistical approaches fall into the following categories: boolean, vector space and probabilistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lPx4akLzb3a"
      },
      "source": [
        "all_tokens = []\n",
        "for tokens in corpus_df['tokens']:\n",
        "  all_tokens.extend(tokens)\n",
        "\n",
        "vocabulary = list(set(all_tokens))\n",
        "print(\"Vocabulary size:\",len(vocabulary),\" unique words(tokens)\")\n",
        "print(\"Vocabulary words:\",vocabulary[1:10],\"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxmE0Zx93U"
      },
      "source": [
        "To encode our documents, we’ll create a vectorize function that creates a dictionary whose keys are the tokens in the document and whose values will depend on the approach we use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObcQrTJlMvl"
      },
      "source": [
        "\n",
        "The `defaultdic` object allows us to specify what the dictionary will return for a key that hasn’t been assigned to it yet. By setting `defaultdict(int)` we are specifying that a 0 should be returned, thus creating a simple counting dictionary. We can map this function to every item in the corpus creating an iterable of vectorized documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p397ElSvkItE"
      },
      "source": [
        "### 2.3.1. Boolean Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEmOhEO_0Ahv"
      },
      "source": [
        "The Boolean representation sets true or false for each vocabulary word depending on whether or not it appears in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9EkhRpqy8Yr"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def boolean_vectorize(tokens):\n",
        "    features = defaultdict(bool)\n",
        "    for token in tokens:\n",
        "        features[token] = True\n",
        "    return features\n",
        "\n",
        "corpus_df['boolean'] = corpus_df['tokens'].apply(boolean_vectorize)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cacVm6aF0POJ"
      },
      "source": [
        "In the boolean approach, the query is formulated as a boolean combination of terms. A conventional boolean query uses the classical operators AND, OR, and NOT. The query ”t1 AND t2” is satisfied by a given document D1 if and only if D1 contains both terms t1 and t2. Similarly, the query ”t1 OR t2” is satisfied by D1 if and only if it contains t1 or t2 or both. The query ”t1 AND NOT t2” satisfies D1 if and only if it contains t1 and does not contain t2. More complex boolean queries can be built up out of these operators and evaluated according to the classical rules of boolean algebra. Such a boolean query is either true or false. Correspondingly, a document either satisfies such a query, i.e. is relevant, or does not satisfy it, i.e. is non-relevant. **No ranking is possible**, which is a significant limitation for this approach (Harmon, 1996)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrUgr3a53PLt"
      },
      "source": [
        "For example, we can search for documents about topic hierarchies and multilinguality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhpXox-o0zaM"
      },
      "source": [
        "def relevant(doc):\n",
        "  #return doc['HIV']\n",
        "  return doc['multilingual'] and doc['topic'] and doc['hierarchy']\n",
        "  #return doc['multilingual'] and doc['procurement']\n",
        "\n",
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['boolean']:\n",
        "  if relevant(vector):\n",
        "    result.append(corpus_df['title'][pos])\n",
        "  pos+=1 \n",
        "\n",
        "for paper in result:\n",
        "  print(\"-\",paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYdnZkrr45D1"
      },
      "source": [
        "### 2.3.2 Vector space models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9FsGqw847uK"
      },
      "source": [
        "Vector space models (VSM) (Salton and McGill, 1983) were proposed to represent texts as vectors where each entry corresponds to a different term and the number at that entry corresponds to how many times that term is present in the text. The objective was twofold: on the one hand, making document collections manageable since we move from having lots of terms for each text to only one vector per document with a defined dimension; on the other hand, having representations based on metric spaces where calculations can be made, for example comparisons by measuring vector distances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LgdiFxdLew"
      },
      "source": [
        "#### 2.3.2.1 Term-Frequency (TF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUisxAH65FrB"
      },
      "source": [
        "The definition and number of dimensions for each vector are key aspects in a VSM. Based on the use of this type of model, traditional document retrieval tasks over collections of textual documents highly rely on individual features like term frequencies (TF) (Hearst and Hall, 1999). A representational space is created where each term in the vocabulary is projected by a separate and orthogonal dimension.\n",
        "\n",
        "Vectors are created with the frequency of each word as it appears in the document. In this encoding scheme, each document is represented as the multiset of the tokens that compose it and the value for each word position in the vector is its count. This representation can either be a straight count (integer) encoding or a normalized encoding where each word is weighted by the total number of words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGMOd8vgkM6Y"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def tf_vectorize(tokens):\n",
        "    features = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        features[token] += 1\n",
        "    return features\n",
        "\n",
        "corpus_df['tf'] = corpus_df['tokens'].apply(tf_vectorize)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onn58OYn24Ag"
      },
      "source": [
        "The relevant results can now be sorted according to the frequency of the terms they contain. But **all terms in a document are treated as equally descriptive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx5Iv4X83Ori"
      },
      "source": [
        "def relevant(doc):\n",
        "  # multiple scores for OR queries, the max value should be returned\n",
        "  score = 0\n",
        "  score += doc['multilingual']\n",
        "  score += doc['topic'] \n",
        "  score += doc['hierarchy']\n",
        "  return score\n",
        "\n",
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['tf']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(vector)})  \n",
        "  pos+=1 \n",
        "\n",
        "def sort_by_score(element):\n",
        "  return element['score']\n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2eYbhPgdQyO"
      },
      "source": [
        "#### 2.3.2.2 Term-Frequency Inverse-Document-Frequency (TF/IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8VATDc-dZr9"
      },
      "source": [
        "To overcome this limitation, Term-Frequency Inverse-Document Frequency (TF-IDF) (Lee, 1995) relativizes the relevance of each term with respect to the entire corpus. TF-IDF calculates the importance of a term for a document, based on the number of times the term appears in the document itself (term frequency - TF) and the number of documents in the corpus, which contain the term (document frequency - DF)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDInLJMgdf3p"
      },
      "source": [
        "# https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
        "from collections import defaultdict\n",
        "from nltk.text import TextCollection\n",
        "\n",
        "texts  = TextCollection(corpus_df['tokens'])\n",
        "\n",
        "vectors = []\n",
        "\n",
        "for doc in corpus_df['tokens']:\n",
        "  features = defaultdict(int)\n",
        "  for term in doc:\n",
        "    features[term]=texts.tf_idf(term, doc)\n",
        "  vectors.append(features)\n",
        "\n",
        "corpus_df['tf_idf'] = vectors\n",
        "corpus_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUT1MiuR5wr4"
      },
      "source": [
        "Relevance now depends not only on the document, but also on the corpus. Those documents that contain the key terms in a different proportion to the rest of the documents will be the most relevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gSOCAVg6KO2"
      },
      "source": [
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['tf_idf']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(vector)})  \n",
        "  pos+=1 \n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8REaBnOO7ayR"
      },
      "source": [
        "However the **absence of semantic information, and the high-number of dimensions** are the main drawbacks of these approaches that lead to the emergence of other techniques. New ways of characterizing documents appeared based on the automatic generation of models discovering the main themes covered in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3iy0fjH73ys"
      },
      "source": [
        "#### 2.3.2.3 Text embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7bKKfcv7lSR"
      },
      "source": [
        " Among them, text embedding proposes transforming texts into low-dimensional vectors by pre- diction methods based on (i) word sequences or (ii) bag-of-words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P-apJdx8DQn"
      },
      "source": [
        "##### 2.3.2.3.1 Word sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lj6pi648HGb"
      },
      "source": [
        "This approach assumes words with similar meanings tend to occur in similar contexts. It considers that word order is relevant, and is based on Neural Models (NM) that learn word vectors from pairs of target and context words. Context words are taken as words observed to surround a target word. \n",
        "\n",
        "Document vectors are usually created by taking the word vectors they contain or by considering them as target and context items. Skip-gram with negative sampling (Word2Vec) (Mikolov et al., 2013) and Global Vectors (GloVe) (Pennington et al., 2014) are indeed the most popular methods to learn word embeddings due to its training efficiency and robustness (Levy et al., 2015).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGunwvB-8wrK"
      },
      "source": [
        "The Doc2Vec algorithm is an extension of Word2Vec. It proposes a paragraph vector, i.e. an unsupervised algorithm that learns fixed-length feature representations from variable length documents. It takes into consideration the ordering of words within a narrow context, similar to an n-gram model. The combined result generalizes and has a lower dimensionality but still is of a fixed length so it can be used in common machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VanWTtt8TmV"
      },
      "source": [
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_df['tokens'])]\n",
        "model = Doc2Vec(documents, vector_size=5, min_count=0, window=2, workers=4)\n",
        "\n",
        "docvecs = []\n",
        "for pos in range(len(corpus_df['tokens'])):\n",
        "  docvecs.append(model.docvecs[pos])\n",
        "\n",
        "corpus_df['d2v'] = docvecs\n",
        "corpus_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1Fg7JbAjqO"
      },
      "source": [
        "Now, a text or a reference document can be used in the query by  measuring its similarity to the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5OwhdOBGM3"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "query_paper = 4\n",
        "query_vector = corpus_df['d2v'][query_paper]\n",
        "\n",
        "def relevant(vector):\n",
        "  distance = spatial.distance.cosine(query_vector, vector)\n",
        "  similarity = 1 - distance\n",
        "  return similarity\n",
        "\n",
        "pos = 0\n",
        "result = []\n",
        "for v1 in corpus_df['d2v']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(v1)})    \n",
        "  pos+=1\n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "print(corpus_df['title'][query_paper],\":\")\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYO-DpTj8UTk"
      },
      "source": [
        "##### 2.3.2.3.2 Bag-of-words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3WVlqAj8YHq"
      },
      "source": [
        "This approach does not consider the order of the words to be relevant, but their frequency. It assumes words with similar meanings will occur in similar documents, although a recent proposal uses an embeddings based approach to model the topics (Dieng et al., 2020). Topic models (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 2001) are the main methods based on this approach. This second approach is used in our work since we are not only interested in representing words and documents, but we also seek structures that allows considering knowledge about the collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwx62ZSm_iN"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Create Dictionary\n",
        "dictionary = gensim.corpora.Dictionary(corpus_df['tokens'])\n",
        "\n",
        "# Create bag-of-words\n",
        "bows = [dictionary.doc2bow(text) for text in corpus_df['tokens']]\n",
        "\n",
        "print(\"->\",corpus_df['title'][0],\":\")\n",
        "for word in bows[0][200:210]:\n",
        "  id = word[0]\n",
        "  freq = word[1]\n",
        "  print(dictionary[id],freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnpL0grsEBFK"
      },
      "source": [
        "### 2.3.3 Probabilistic Topic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnROb8LdEKHv"
      },
      "source": [
        "Probabilistic Topic Models (PTM) (Blei et al., 2003; Hofmann, 2001) are statistical methods based on bag-of-words that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, or how they change over time. \n",
        "\n",
        "PTM do not require any prior annotations or labeling of the documents. The topics emerge, as hidden structures, from the analy- sis of the original texts. These structures are topic distributions, per-document topic distributions or per-document per-word topic assignments.\n",
        "\n",
        "In turn, a topic is a distribution over terms that is biased around those words associated to a single theme. This interpretable hidden structure annotates each document in the collection and these annotations can be used to perform deeper analysis about relationships between documents.\n",
        "\n",
        "Topic-based representations bring a lot of potential when applied over different IR tasks, as evidenced by recent works in different domains such as scholarly (Gatti et al., 2015), health (Hsin-Min et al., 2016; Nzali et al., 2017), legal (Greene and Cross, 2016; O’Neill et al., 2017), news (He et al., 2017) and social networks (Cheng et al.,2014). \n",
        "\n",
        "Topic modeling provides an algorithmic solution to organize and annotate large collections of textual documents according to their topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791k5zdJExA6"
      },
      "source": [
        "#### 2.3.3.1 LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWD8-A0NE0go"
      },
      "source": [
        "The simplest generative topic model proposed in the state of the art is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Along with Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 2001) are part of the field known as topic modeling. They are well-known latent variable models for high dimensional data, such as the bag-of-words representation for textual data or any other count-based data representation. They try to capture the intuition that documents can exhibit multiple themes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqD61-w6E66U"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=bows,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=2, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "#doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPAWt_z5E7IF"
      },
      "source": [
        "Each document exhibits each topic in different proportion, and each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over topics. All the documents in a collection share the same set of topics, but each document exhibits these topics in a different proportion. Texts are described as a vector of counts with W components, where W is the number of words in the vocabulary. Each document in the corpus is modeled as a mixture over K topics, and each topic k is a distribution over the vocabulary of W words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-6VfGcH5dm"
      },
      "source": [
        "corpus_doc_lda = lda_model[bows]\n",
        "\n",
        "topic_vectors = []\n",
        "\n",
        "for doc_lda in corpus_doc_lda:\n",
        "  topic_vectors.append(doc_lda[0])\n",
        "\n",
        "corpus_df['topics'] = topic_vectors\n",
        "corpus_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6PYFVgnHdUH"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(bows))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_df['tokens'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIPr9j43m2Pw"
      },
      "source": [
        "# References\n",
        "\n",
        "* Manning, C. D., Raghavan, P., and Schutze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.\n",
        "* Porter, M. F. (1997). An Algorithm for Suffix Stripping, page 313–316. Morgan Kauf- mann Publishers Inc.\n",
        "* Harmon, D. K. (1996). Overview of the Third Text Retrieval Conference (TREC-3). DIANE Publishing Company.\n",
        "* Salton, G. and McGill, M. J. (1983). Introduction to Modern Information Retrieval. McGraw-Hill, Inc\n",
        "* Hearst, M. a. and Hall, S. (1999). Untangling Text Data Mining. In the 37th Annual Meeting of the Association for Computational Linguistics, pages 1–13\n",
        "* Lee, J. H. (1995). Combining multiple evidence from different properties of weighting schemes. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’95, page 180–188. Association for Computing Machinery.\n",
        "* Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. Proceedings of the 26th International Conference on Neural Information Processing Systems, 2:3111– 3119.\n",
        "* Pennington, J., Socher, R., and Manning, C. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP), pages 1532–1543. Association for Computational Linguistics.\n",
        "* Dieng, A. B., Ruiz, F., and Blei, D. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8:439–453\n",
        "* Blei, D., Ng, A., and Jordan, M. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(4-5):993–1022\n",
        "* Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.\n",
        "* Hofmann, T. (2001). Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42(1-2):177–196.\n",
        "* Gatti, C., Brooks, J., and Nurre, S. (2015). A Historical Analysis of the Field of OR/MS using Topic Models. CoRR, abs/1510.0.\n",
        "* Hsin-Min, L., Chih-Ping, W., and Fei-Yuan, C. (2016). Modeling healthcare data using multiple-channel latent Dirichlet allocation. Journal of Biomedical Informatics, 60:210–223.\n",
        "* Nzali, T., Donald, M., Bringay, S., Lavergne, C., Mollevi, C., and Opitz, T. (2017). What Patients Can Tell Us: Topic Analysis for Social Media on Breast Cancer. JMIR medical informatics, 5(3):e23.\n",
        "* Greene, D. and Cross, J. (2016). Exploring the political agenda of the european par- liament using a dynamic topic modeling approach. Political Analysis, 25(1):77–94.\n",
        "* O’Neill, J., Robin, C., O’Brien, L., and Buitelaar, P. (2017). An analysis of topic modelling for legislative texts. CEUR Workshop Proceedings, 2143.\n",
        "* He, J., Li, L., and Wu, X. (2017). A self-adaptive sliding window based topic model for non-uniform texts. In Proceedings - IEEE International Conference on Data Mining, ICDM, volume 2017-Novem, pages 147–156.\n",
        "* Cheng, X., Yan, X., Lan, Y., and Guo, J. (2014). BTM : Topic Modeling over Short Texts. IEEE Transactions on Knowledge and Data Engineering, 26(12):2928–2941. \n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}