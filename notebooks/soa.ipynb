{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq8Y+rDSaP/agUpe8HyXDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/phd-thesis/blob/master/notebooks/soa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKXrGgKqX7u"
      },
      "source": [
        "This notebook supports the state-of-the-art content of the thesis: *Semantically-enabled Browsing of Large Multilingual Document Collections, Badenes-Olmedo, C. 2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXRIfz2kq9ee"
      },
      "source": [
        "# 2.- Techniques for Document Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvHN9zQapir8"
      },
      "source": [
        "The analysis of human-readable documents is a well-known problem in Artificial Intelligence (AI) in general, and in the Information Retrieval (IR) and Natural Language Processing (NLP) fields in particular. As an academic field of study, information retrieval might be defined as finding documents of an unstructured nature, usually text, that satisfies an information need from within large collections (Manning et al., 2008). As defined in this way, hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email. Information retrieval is fast becoming the dominant form of information access, surpassing traditional database searching where identifiers are needed to have results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFdYC8fo2Nl"
      },
      "source": [
        "There are two major categories of IR technology and research: semantic and statistical. Semantic approaches attempt to implement some degree of syntactic and semantic analysis. They try to reproduce to some degree the understanding of the natural language text that a human user would provide. In statistical approaches, the documents that are retrieved or that are highly ranked are those that match the query most closely in terms of some statistical measure. The work presented in this thesis follows this second approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw0G4p1geCAx"
      },
      "source": [
        "## 2.1.- Load Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQhyR8L0rGQX"
      },
      "source": [
        "An illustrative example may help to better understand IR techniques, so the publications listed in Section 1.1 are used as a sample collection for applying each of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "V0ngppsheGCe",
        "outputId": "e92fe165-68cc-4832-ee34-9b6eb447ebd1"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#increase the max column length\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "corpus_df = pd.read_csv('https://www.dropbox.com/s/pag5jseq2e9wcvb/corpus.csv?raw=1',usecols=['title','text'])\n",
        "corpus_df"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Distributing Text Mining tasks with librAIry</td>\n",
              "      <td>Distributing Text Mining tasks with librAIry Carlos Badenes-Olmedo cbadenes@f.upm.es Universidad Polit´ecnica de Madrid Ontology Engineering Group Boadilla del Monte, Spain Jos´e Luis Redondo-Garc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms</td>\n",
              "      <td>Semantic Web 0 (0) 1 1 IOS Press Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms Editor(s): Tomi Kauppinen, Aalto University, Finland; Daniel Garijo,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts</td>\n",
              "      <td>An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts Carlos Badenes-Olmedo1, Jos´e Luis Redondo-Garc´ıa2, and Oscar Corcho1 1 Universi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Efficient Clustering from Distributions over Topics</td>\n",
              "      <td>Efficient Clustering from Distributions over Topics Carlos Badenes-Olmedo cbadenes@￿.upm.es Ontology Engineering Group Universidad Polit´ecnica de Madrid Boadilla del Monte, Spain Jos´e Luis Redon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Legal Documents Retrieval Across Languages: Topic Hierarchies based on synsets</td>\n",
              "      <td>Cross-lingual annotations of legislative texts enable us to explore major themes covered in multi- lingual legal data and are a key facilitator of semantic similarity when searching for similar do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies</td>\n",
              "      <td>Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies Carlos Badenes-Olmedo cbadenes@fi.upm.es Ontology Engineering Group, Universidad Politécnica de Madrid Boad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Potentially inappropriate medications in older adults living with HIV</td>\n",
              "      <td>Potentially inappropriate medications in older adults living with HIV B L�opez-Centeno,1,* C Badenes-Olmedo,2 A Mataix-Sanjuan,1 JM Bell�on,3 L P�erez-Latorre,3 JC L�opez,3 J Bened�ı,4,* S Khoo,5 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Semantic Saturation in Retrospective Text Document  Collections</td>\n",
              "      <td>Semantic Saturation in Retrospective Text Document  Collections      Victoria Kosa​1​, Eugene Yuschenko​2​, Carlos Badenes​3​, Vadim Ermolayev​1​,   and Aliaksandr Birukou​4    1​ Department of Co...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                   title                                                                                                                                                                                                     text\n",
              "0                                       Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...\n",
              "1   Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...\n",
              "2                                            Drugs4Covid: Making drug information available from scientific publications  Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...\n",
              "3                                                                           Distributing Text Mining tasks with librAIry  Distributing Text Mining tasks with librAIry Carlos Badenes-Olmedo cbadenes@f.upm.es Universidad Polit´ecnica de Madrid Ontology Engineering Group Boadilla del Monte, Spain Jos´e Luis Redondo-Garc...\n",
              "4                         Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms  Semantic Web 0 (0) 1 1 IOS Press Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms Editor(s): Tomi Kauppinen, Aalto University, Finland; Daniel Garijo,...\n",
              "5     An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts  An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts Carlos Badenes-Olmedo1, Jos´e Luis Redondo-Garc´ıa2, and Oscar Corcho1 1 Universi...\n",
              "6                                                                    Efficient Clustering from Distributions over Topics  Efficient Clustering from Distributions over Topics Carlos Badenes-Olmedo cbadenes@￿.upm.es Ontology Engineering Group Universidad Polit´ecnica de Madrid Boadilla del Monte, Spain Jos´e Luis Redon...\n",
              "7                                         Legal Documents Retrieval Across Languages: Topic Hierarchies based on synsets  Cross-lingual annotations of legislative texts enable us to explore major themes covered in multi- lingual legal data and are a key facilitator of semantic similarity when searching for similar do...\n",
              "8                               Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies  Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies Carlos Badenes-Olmedo cbadenes@fi.upm.es Ontology Engineering Group, Universidad Politécnica de Madrid Boad...\n",
              "9                                                  Potentially inappropriate medications in older adults living with HIV  Potentially inappropriate medications in older adults living with HIV B L�opez-Centeno,1,* C Badenes-Olmedo,2 A Mataix-Sanjuan,1 JM Bell�on,3 L P�erez-Latorre,3 JC L�opez,3 J Bened�ı,4,* S Khoo,5 ...\n",
              "10                                                       Semantic Saturation in Retrospective Text Document  Collections  Semantic Saturation in Retrospective Text Document  Collections      Victoria Kosa​1​, Eugene Yuschenko​2​, Carlos Badenes​3​, Vadim Ermolayev​1​,   and Aliaksandr Birukou​4    1​ Department of Co..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKk9U8-hFBV"
      },
      "source": [
        "## 2.2. Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9JzJsJsIWf"
      },
      "source": [
        "Documents must be pre-processed to transform their texts into terms. These terms are the population that is counted and measured statistically. Most commonly, the terms are words (or combination of adjacent words or characters) that occur in a given query or collection of documents and often require pre-processing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd1VTpW4iru1"
      },
      "source": [
        "### 2.2.1: Methods to transform texts into terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_nSrGfgsnod"
      },
      "source": [
        "Words are reduced to a common base form by using a heuristic process that removes affixes, stemming, or by returning its dictionary form, lemma (Porter, 1997). The objective is to eliminate the variation that arises from the occurrence of different grammatical forms of the same word, e.g., ”program”, ”programming”, ”programs”, and ”programmed” should all be recognized as forms of the same word, ”program”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elq65_4Qsvjd"
      },
      "source": [
        "Another common form of pre-processing is the elimination of common words that have little power to discriminate relevant from non-relevant documents,e.g., ”the”, ”a”, ”it”. Hence, IR engines are usually provided with a stop-list of such noise words. Note that both stemming/lemma and stopwords are language-dependent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWhN4MhStpvQ",
        "outputId": "4b330eed-499c-4c5d-91fc-1ccec59c7c9e"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = nlp(text)\n",
        "  return tokens\n",
        "\n",
        "def is_valid(token):\n",
        "  return len(token.text) > 1 and not token.is_digit and not token.is_stop\n",
        "\n",
        "def lemma(token):\n",
        "  return token.lemma_\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens = []\n",
        "  for token in tokenize(text):\n",
        "    if is_valid(token): \n",
        "      tokens.append(lemma(token))\n",
        "  return tokens\n",
        "\n",
        "print(\"methods created succesfully\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "methods created succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBhQ2lRUf6g"
      },
      "source": [
        "The following sentence taken from one of the documents can be used to see each of the steps: *”Probabilistic Topic Models reduce that feature space by annotating documents with thematic information”*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SKih16gUj5w",
        "outputId": "455d8a8c-44ce-472e-c73a-e493c8a246d9"
      },
      "source": [
        "tokens = preprocess(\"Probabilistic Topic Models reduce that feature space by annotating documents with thematic information\")\n",
        "print(tokens)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Probabilistic', 'Topic', 'Models', 'reduce', 'feature', 'space', 'annotate', 'document', 'thematic', 'information']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngevbE-2U2Gy"
      },
      "source": [
        "At this step ’annotating’ was transformed to ’annotate’ and ’documents’ was reduced to ’document’. However, ’Models’ remains unchanged. The reason is that since it starts with a capital letter, it is considered a proper noun. Finally, those words that appear in a stop-word list are removed (e.g. ’that’, ’by’ and ’with’). Each text is transformed into a normalized list of terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y562UNjCkYG9"
      },
      "source": [
        "### 2.2.2. Count words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "GhLgTKXSkak7",
        "outputId": "bd7e67c2-61da-41e8-a768-c550664843c6"
      },
      "source": [
        "def count_words(text):\n",
        "  return len(text.split(\" \"))\n",
        "\n",
        "corpus_df['#words'] = corpus_df['text'].apply(count_words)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ... #words\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...  12954\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...   5827\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...   5417\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDhLJ7_QivyH"
      },
      "source": [
        "### 2.2.3. Tokenize Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "YrnhZZipjBjc",
        "outputId": "3172a64c-d5ee-4ea7-cc16-01958b69bd80"
      },
      "source": [
        "corpus_df['tokens'] = corpus_df['text'].apply(preprocess)\n",
        "\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ...                                                                                                                                                                                                   tokens\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...  [synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...  [enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...  [Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4qWo8kujLZo"
      },
      "source": [
        "### 2.2.4. Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "yd0QX-rCjOZs",
        "outputId": "15823c4a-99cf-45e3-8ea0-aa3fe54377f7"
      },
      "source": [
        "def count_tokens(tokens):\n",
        "  return len(tokens)\n",
        "\n",
        "corpus_df['#tokens'] = corpus_df['tokens'].apply(count_tokens)\n",
        "\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#tokens</th>\n",
              "      <th>#uni_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "      <td>6342</td>\n",
              "      <td>1618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "      <td>3406</td>\n",
              "      <td>1290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "      <td>3260</td>\n",
              "      <td>1367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ... #uni_tokens\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...        1618\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...        1290\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...        1367\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL1zTSPXktuE"
      },
      "source": [
        "### 2.2.5 Some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "k8ddmT6Gkzvr",
        "outputId": "12ff0b8e-9be1-411c-a94e-8ba935aa852a"
      },
      "source": [
        "unique_tokens = []\n",
        "\n",
        "for pos in range(len(corpus_df.index)):\n",
        "  num_words = corpus_df['#words'][pos]\n",
        "  num_tokens = corpus_df['#tokens'][pos]\n",
        "  num_unique_tokens = len(set(corpus_df['tokens'][pos]))\n",
        "  unique_tokens.append(num_unique_tokens)  \n",
        "\n",
        "corpus_df['#uni_tokens']=unique_tokens\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#tokens</th>\n",
              "      <th>#uni_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "      <td>6342</td>\n",
              "      <td>1618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "      <td>3406</td>\n",
              "      <td>1290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "      <td>3260</td>\n",
              "      <td>1367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ... #uni_tokens\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...        1618\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...        1290\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...        1367\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjqffNPFmnyJ"
      },
      "source": [
        "## 2.3. Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajYqcsxZs2_E"
      },
      "source": [
        "Once all terms have been pre-processed, numerical weights are assigned to each them. The same term may have a different weight in each distinct document in which it occurs. The weight is usually a measure of how effective the given term is likely to be in distinguishing the given document from other documents in the given collection, and is often normalized to be a fraction between zero and one. Statistical approaches fall into the following categories: boolean, vector space and probabilistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lPx4akLzb3a",
        "outputId": "a88feb58-1334-443e-a4dd-cc9c9b138d55"
      },
      "source": [
        "all_tokens = []\n",
        "for tokens in corpus_df['tokens']:\n",
        "  all_tokens.extend(tokens)\n",
        "\n",
        "vocabulary = list(set(all_tokens))\n",
        "print(\"Vocabulary size:\",len(vocabulary),\" unique words(tokens)\")\n",
        "print(\"Vocabulary words:\",vocabulary[1:10],\"...\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 6182  unique words(tokens)\n",
            "Vocabulary words: ['rmlmapper', 'comunicação', 'Building', 'instru-', 'A10BB01', 'execute', 'heterogeneity', 'face', 'absolute'] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxmE0Zx93U"
      },
      "source": [
        "To encode our documents, we’ll create a vectorize function that creates a dictionary whose keys are the tokens in the document and whose values will depend on the approach we use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObcQrTJlMvl"
      },
      "source": [
        "\n",
        "The `defaultdic` object allows us to specify what the dictionary will return for a key that hasn’t been assigned to it yet. By setting `defaultdict(int)` we are specifying that a 0 should be returned, thus creating a simple counting dictionary. We can map this function to every item in the corpus creating an iterable of vectorized documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p397ElSvkItE"
      },
      "source": [
        "### 2.3.1. Boolean Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEmOhEO_0Ahv"
      },
      "source": [
        "The Boolean representation sets true or false for each vocabulary word depending on whether or not it appears in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "G9EkhRpqy8Yr",
        "outputId": "3f237143-5cbe-4a74-e5e3-a7147f9b65bf"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def boolean_vectorize(tokens):\n",
        "    features = defaultdict(bool)\n",
        "    for token in tokens:\n",
        "        features[token] = True\n",
        "    return features\n",
        "\n",
        "corpus_df['boolean'] = corpus_df['tokens'].apply(boolean_vectorize)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#tokens</th>\n",
              "      <th>#uni_tokens</th>\n",
              "      <th>boolean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "      <td>6342</td>\n",
              "      <td>1618</td>\n",
              "      <td>{'synopsis': True, 'Refinements': True, 'Extensions': True, 'compare': True, 'publication': True, 'Conference': True, 'Proceedings': True, 'submission': True, 'refined': True, 'extended': True, 'p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "      <td>3406</td>\n",
              "      <td>1290</td>\n",
              "      <td>{'enhance': True, 'Public': True, 'Procurement': True, 'European': True, 'Union': True, 'Constructing': True, 'exploit': True, 'Integrated': True, 'Knowledge': True, 'Graph': True, 'Ahmet': True, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "      <td>3260</td>\n",
              "      <td>1367</td>\n",
              "      <td>{'Drugs4Covid': True, 'make': True, 'drug': True, 'information': True, 'available': True, 'scientific': True, 'publication': True, 'Carlos': True, 'Badenes': True, 'Olmedo1': True, 'David': True, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ...                                                                                                                                                                                                  boolean\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...  {'synopsis': True, 'Refinements': True, 'Extensions': True, 'compare': True, 'publication': True, 'Conference': True, 'Proceedings': True, 'submission': True, 'refined': True, 'extended': True, 'p...\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...  {'enhance': True, 'Public': True, 'Procurement': True, 'European': True, 'Union': True, 'Constructing': True, 'exploit': True, 'Integrated': True, 'Knowledge': True, 'Graph': True, 'Ahmet': True, ...\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...  {'Drugs4Covid': True, 'make': True, 'drug': True, 'information': True, 'available': True, 'scientific': True, 'publication': True, 'Carlos': True, 'Badenes': True, 'Olmedo1': True, 'David': True, ...\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cacVm6aF0POJ"
      },
      "source": [
        "In the boolean approach, the query is formulated as a boolean combination of terms. A conventional boolean query uses the classical operators AND, OR, and NOT. The query ”t1 AND t2” is satisfied by a given document D1 if and only if D1 contains both terms t1 and t2. Similarly, the query ”t1 OR t2” is satisfied by D1 if and only if it contains t1 or t2 or both. The query ”t1 AND NOT t2” satisfies D1 if and only if it contains t1 and does not contain t2. More complex boolean queries can be built up out of these operators and evaluated according to the classical rules of boolean algebra. Such a boolean query is either true or false. Correspondingly, a document either satisfies such a query, i.e. is relevant, or does not satisfy it, i.e. is non-relevant. **No ranking is possible**, which is a significant limitation for this approach (Harmon, 1996)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrUgr3a53PLt"
      },
      "source": [
        "For example, we can search for documents about topic hierarchies or multilinguality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhpXox-o0zaM",
        "outputId": "6f4a1cac-b5da-4866-eece-d67d702ddc05"
      },
      "source": [
        "def relevant(doc):\n",
        "  #return doc['HIV']\n",
        "  return doc['multilingual'] or (doc['topic'] and doc['hierarchy'])\n",
        "  #return doc['multilingual'] and doc['procurement']\n",
        "\n",
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['boolean']:\n",
        "  if relevant(vector):\n",
        "    result.append(corpus_df['title'][pos])\n",
        "  pos+=1 \n",
        "\n",
        "for paper in result:\n",
        "  print(\"-\",paper)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph\n",
            "- Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms\n",
            "- Efficient Clustering from Distributions over Topics\n",
            "- Legal Documents Retrieval Across Languages: Topic Hierarchies based on synsets\n",
            "- Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYdnZkrr45D1"
      },
      "source": [
        "### 2.3.2 Vector space models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9FsGqw847uK"
      },
      "source": [
        "Vector space models (VSM) (Salton and McGill, 1983) were proposed to represent texts as vectors where each entry corresponds to a different term and the number at that entry corresponds to how many times that term is present in the text. The objective was twofold: on the one hand, making document collections manageable since we move from having lots of terms for each text to only one vector per document with a defined dimension; on the other hand, having representations based on metric spaces where calculations can be made, for example comparisons by measuring vector distances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LgdiFxdLew"
      },
      "source": [
        "#### 2.3.2.1 Term-Frequency (TF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUisxAH65FrB"
      },
      "source": [
        "The definition and number of dimensions for each vector are key aspects in a VSM. Based on the use of this type of model, traditional document retrieval tasks over collections of textual documents highly rely on individual features like term frequencies (TF) (Hearst and Hall, 1999). A representational space is created where each term in the vocabulary is projected by a separate and orthogonal dimension.\n",
        "\n",
        "Vectors are created with the frequency of each word as it appears in the document. In this encoding scheme, each document is represented as the multiset of the tokens that compose it and the value for each word position in the vector is its count. This representation can either be a straight count (integer) encoding or a normalized encoding where each word is weighted by the total number of words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BGMOd8vgkM6Y",
        "outputId": "9c845beb-a3b8-4b1b-9290-12d6444806cc"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def tf_vectorize(tokens):\n",
        "    features = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        features[token] += 1\n",
        "    return features\n",
        "\n",
        "corpus_df['tf'] = corpus_df['tokens'].apply(tf_vectorize)\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#tokens</th>\n",
              "      <th>#uni_tokens</th>\n",
              "      <th>boolean</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "      <td>6342</td>\n",
              "      <td>1618</td>\n",
              "      <td>{'synopsis': True, 'Refinements': True, 'Extensions': True, 'compare': True, 'publication': True, 'Conference': True, 'Proceedings': True, 'submission': True, 'refined': True, 'extended': True, 'p...</td>\n",
              "      <td>{'synopsis': 1, 'Refinements': 1, 'Extensions': 1, 'compare': 21, 'publication': 3, 'Conference': 1, 'Proceedings': 1, 'submission': 2, 'refined': 2, 'extended': 1, 'paper': 44, 'base': 43, 'ICTER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "      <td>3406</td>\n",
              "      <td>1290</td>\n",
              "      <td>{'enhance': True, 'Public': True, 'Procurement': True, 'European': True, 'Union': True, 'Constructing': True, 'exploit': True, 'Integrated': True, 'Knowledge': True, 'Graph': True, 'Ahmet': True, ...</td>\n",
              "      <td>{'enhance': 9, 'Public': 9, 'Procurement': 9, 'European': 5, 'Union': 3, 'Constructing': 1, 'exploit': 1, 'Integrated': 8, 'Knowledge': 11, 'Graph': 11, 'Ahmet': 1, 'Soylu1': 1, 'Oscar': 1, 'Corch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "      <td>3260</td>\n",
              "      <td>1367</td>\n",
              "      <td>{'Drugs4Covid': True, 'make': True, 'drug': True, 'information': True, 'available': True, 'scientific': True, 'publication': True, 'Carlos': True, 'Badenes': True, 'Olmedo1': True, 'David': True, ...</td>\n",
              "      <td>{'Drugs4Covid': 24, 'make': 2, 'drug': 71, 'information': 11, 'available': 12, 'scientific': 14, 'publication': 9, 'Carlos': 1, 'Badenes': 10, 'Olmedo1': 1, 'David': 1, 'Chaves': 1, 'Fraga1': 1, '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ...                                                                                                                                                                                                       tf\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...  {'synopsis': 1, 'Refinements': 1, 'Extensions': 1, 'compare': 21, 'publication': 3, 'Conference': 1, 'Proceedings': 1, 'submission': 2, 'refined': 2, 'extended': 1, 'paper': 44, 'base': 43, 'ICTER...\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...  {'enhance': 9, 'Public': 9, 'Procurement': 9, 'European': 5, 'Union': 3, 'Constructing': 1, 'exploit': 1, 'Integrated': 8, 'Knowledge': 11, 'Graph': 11, 'Ahmet': 1, 'Soylu1': 1, 'Oscar': 1, 'Corch...\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...  {'Drugs4Covid': 24, 'make': 2, 'drug': 71, 'information': 11, 'available': 12, 'scientific': 14, 'publication': 9, 'Carlos': 1, 'Badenes': 10, 'Olmedo1': 1, 'David': 1, 'Chaves': 1, 'Fraga1': 1, '...\n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onn58OYn24Ag"
      },
      "source": [
        "The relevant results can now be sorted according to the frequency of the terms they contain. But **all terms in a document are treated as equally descriptive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx5Iv4X83Ori",
        "outputId": "0f8379d2-aaf2-4c4c-f2cf-f215e9a20def"
      },
      "source": [
        "def relevant(doc):\n",
        "  # multiple scores for OR queries, the max value should be returned\n",
        "  q1_score = 0\n",
        "  q2_score = 0\n",
        "  if doc['multilingual'] > 0:\n",
        "    q1_score = doc['multilingual']\n",
        "  if doc['topic'] > 0 and doc['hierarchy'] > 0:\n",
        "    q2_score = doc['topic'] + doc['hierarchy']\n",
        "  return max(q1_score, q2_score)\n",
        "\n",
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['tf']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(vector)})  \n",
        "  pos+=1 \n",
        "\n",
        "def sort_by_score(element):\n",
        "  return element['score']\n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'title': 'Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms', 'score': 197}\n",
            "{'title': 'Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies', 'score': 128}\n",
            "{'title': 'Efficient Clustering from Distributions over Topics', 'score': 85}\n",
            "{'title': 'Legal Documents Retrieval Across Languages: Topic Hierarchies based on synsets', 'score': 33}\n",
            "{'title': 'Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph', 'score': 13}\n",
            "{'title': 'Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation', 'score': 0}\n",
            "{'title': 'Drugs4Covid: Making drug information available from scientific publications', 'score': 0}\n",
            "{'title': 'Distributing Text Mining tasks with librAIry', 'score': 0}\n",
            "{'title': 'An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts', 'score': 0}\n",
            "{'title': 'Potentially inappropriate medications in older adults living with HIV', 'score': 0}\n",
            "{'title': 'Semantic Saturation in Retrospective Text Document  Collections', 'score': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2eYbhPgdQyO"
      },
      "source": [
        "#### 2.3.2.2 Term-Frequency Inverse-Document-Frequency (TF/IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8VATDc-dZr9"
      },
      "source": [
        "To overcome this limitation, Term-Frequency Inverse-Document Frequency (TF-IDF) (Lee, 1995) relativizes the relevance of each term with respect to the entire corpus. TF-IDF calculates the importance of a term for a document, based on the number of times the term appears in the document itself (term frequency - TF) and the number of documents in the corpus, which contain the term (document frequency - DF)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tDInLJMgdf3p",
        "outputId": "8a2d24b7-f563-4420-a502-11946b7d6c98"
      },
      "source": [
        "# https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
        "from collections import defaultdict\n",
        "from nltk.text import TextCollection\n",
        "\n",
        "texts  = TextCollection(corpus_df['tokens'])\n",
        "\n",
        "vectors = []\n",
        "\n",
        "for doc in corpus_df['tokens']:\n",
        "  features = defaultdict(int)\n",
        "  for term in doc:\n",
        "    features[term]=texts.tf_idf(term, doc)\n",
        "  vectors.append(features)\n",
        "\n",
        "corpus_df['tf_idf'] = vectors\n",
        "corpus_df.head(3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>#words</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#tokens</th>\n",
              "      <th>#uni_tokens</th>\n",
              "      <th>boolean</th>\n",
              "      <th>tf</th>\n",
              "      <th>tf_idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation</td>\n",
              "      <td>Synopsis of the Refinements and Extensions  Compared to the Publication in the Conference Proceedings  This submission is a refined and extended paper based on the ICTERI 2017 PhD  Symposium paper...</td>\n",
              "      <td>12954</td>\n",
              "      <td>[synopsis, Refinements, Extensions, compare, publication, Conference, Proceedings, submission, refined, extended, paper, base, ICTERI, phd, symposium, paper, Kosa, et, al, fact, submission, totall...</td>\n",
              "      <td>6342</td>\n",
              "      <td>1618</td>\n",
              "      <td>{'synopsis': True, 'Refinements': True, 'Extensions': True, 'compare': True, 'publication': True, 'Conference': True, 'Proceedings': True, 'submission': True, 'refined': True, 'extended': True, 'p...</td>\n",
              "      <td>{'synopsis': 1, 'Refinements': 1, 'Extensions': 1, 'compare': 21, 'publication': 3, 'Conference': 1, 'Proceedings': 1, 'submission': 2, 'refined': 2, 'extended': 1, 'paper': 44, 'base': 43, 'ICTER...</td>\n",
              "      <td>{'synopsis': 0.0003780976462942874, 'Refinements': 0.0003780976462942874, 'Extensions': 0.0003780976462942874, 'compare': 0.0003155966218686256, 'publication': 0.0002867245996075286, 'Conference':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph</td>\n",
              "      <td>Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph Ahmet Soylu1, Oscar Corcho2, Brian Elvesæter1, Carlos Badenes-Olmedo2, Francisc...</td>\n",
              "      <td>5827</td>\n",
              "      <td>[enhance, Public, Procurement, European, Union, Constructing, exploit, Integrated, Knowledge, Graph, Ahmet, Soylu1, Oscar, Corcho2, Brian, Elvesæter1, Carlos, Badenes, Olmedo2, Francisco, Yedro2, ...</td>\n",
              "      <td>3406</td>\n",
              "      <td>1290</td>\n",
              "      <td>{'enhance': True, 'Public': True, 'Procurement': True, 'European': True, 'Union': True, 'Constructing': True, 'exploit': True, 'Integrated': True, 'Knowledge': True, 'Graph': True, 'Ahmet': True, ...</td>\n",
              "      <td>{'enhance': 9, 'Public': 9, 'Procurement': 9, 'European': 5, 'Union': 3, 'Constructing': 1, 'exploit': 1, 'Integrated': 8, 'Knowledge': 11, 'Graph': 11, 'Ahmet': 1, 'Soylu1': 1, 'Oscar': 1, 'Corch...</td>\n",
              "      <td>{'enhance': 0.004504619151540172, 'Public': 0.006336188330941086, 'Procurement': 0.006336188330941086, 'European': 0.0014850277622995888, 'Union': 0.0011444066213713397, 'Constructing': 0.00070402...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications</td>\n",
              "      <td>Drugs4Covid: Making drug information available from scientific publications Carlos Badenes-Olmedo1, David Chaves-Fraga1, Mar´ıa Poveda-Villal´on1, Ana Iglesias-Molina1, Pablo Calleja1, Socorro Ber...</td>\n",
              "      <td>5417</td>\n",
              "      <td>[Drugs4Covid, make, drug, information, available, scientific, publication, Carlos, Badenes, Olmedo1, David, Chaves, Fraga1, Mar´ıa, Poveda, Villal´on1, Ana, Iglesias, Molina1, Pablo, Calleja1, Soc...</td>\n",
              "      <td>3260</td>\n",
              "      <td>1367</td>\n",
              "      <td>{'Drugs4Covid': True, 'make': True, 'drug': True, 'information': True, 'available': True, 'scientific': True, 'publication': True, 'Carlos': True, 'Badenes': True, 'Olmedo1': True, 'David': True, ...</td>\n",
              "      <td>{'Drugs4Covid': 24, 'make': 2, 'drug': 71, 'information': 11, 'available': 12, 'scientific': 14, 'publication': 9, 'Carlos': 1, 'Badenes': 10, 'Olmedo1': 1, 'David': 1, 'Chaves': 1, 'Fraga1': 1, '...</td>\n",
              "      <td>{'Drugs4Covid': 0.017653216732257945, 'make': 0.00019537038718928502, 'drug': 0.03712794924813748, 'information': 0.0, 'available': 0.00035083501768463167, 'scientific': 0.004344298393711264, 'pub...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                  title  ...                                                                                                                                                                                                   tf_idf\n",
              "0                                      Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation  ...  {'synopsis': 0.0003780976462942874, 'Refinements': 0.0003780976462942874, 'Extensions': 0.0003780976462942874, 'compare': 0.0003155966218686256, 'publication': 0.0002867245996075286, 'Conference':...\n",
              "1  Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph  ...  {'enhance': 0.004504619151540172, 'Public': 0.006336188330941086, 'Procurement': 0.006336188330941086, 'European': 0.0014850277622995888, 'Union': 0.0011444066213713397, 'Constructing': 0.00070402...\n",
              "2                                           Drugs4Covid: Making drug information available from scientific publications  ...  {'Drugs4Covid': 0.017653216732257945, 'make': 0.00019537038718928502, 'drug': 0.03712794924813748, 'information': 0.0, 'available': 0.00035083501768463167, 'scientific': 0.004344298393711264, 'pub...\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUT1MiuR5wr4"
      },
      "source": [
        "Relevance now depends not only on the document, but also on the corpus. Those documents that contain the key terms in a different proportion to the rest of the documents will be the most relevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gSOCAVg6KO2",
        "outputId": "6f1215da-22df-4526-ef2b-f9bf30ebd377"
      },
      "source": [
        "result = []\n",
        "pos = 0\n",
        "for vector in corpus_df['tf_idf']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(vector)})  \n",
        "  pos+=1 \n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'title': 'Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies', 'score': 0.01144612295909046}\n",
            "{'title': 'Legal Documents Retrieval Across Languages: Topic Hierarchies based on synsets', 'score': 0.011401880063353832}\n",
            "{'title': 'Large-Scale Semantic Exploration of Scientific Literature using Topic-based Hashing Algorithms', 'score': 0.009676444259207134}\n",
            "{'title': 'Efficient Clustering from Distributions over Topics', 'score': 0.005909174741856991}\n",
            "{'title': 'Enhancing Public Procurement in the European Union through Constructing and Exploiting an Integrated Knowledge Graph', 'score': 0.0009384925736670831}\n",
            "{'title': 'Cross-Evaluation of Term Extraction Tools by Measuring Terminological Saturation', 'score': 0}\n",
            "{'title': 'Drugs4Covid: Making drug information available from scientific publications', 'score': 0}\n",
            "{'title': 'Distributing Text Mining tasks with librAIry', 'score': 0}\n",
            "{'title': 'An initial Analysis of Topic-based Similarity among Scientific Documents based on their Rhetorical Discourse Parts', 'score': 0}\n",
            "{'title': 'Potentially inappropriate medications in older adults living with HIV', 'score': 0}\n",
            "{'title': 'Semantic Saturation in Retrospective Text Document  Collections', 'score': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8REaBnOO7ayR"
      },
      "source": [
        "However the **absence of semantic information, and the high-number of dimensions** are the main drawbacks of these approaches that lead to the emergence of other techniques. New ways of characterizing documents appeared based on the automatic generation of models discovering the main themes covered in the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3iy0fjH73ys"
      },
      "source": [
        "#### 2.3.2.3 Text embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7bKKfcv7lSR"
      },
      "source": [
        " Among them, text embedding proposes transforming texts into low-dimensional vectors by pre- diction methods based on (i) word sequences or (ii) bag-of-words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P-apJdx8DQn"
      },
      "source": [
        "##### 2.3.2.3.1 Word sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lj6pi648HGb"
      },
      "source": [
        "This approach assumes words with similar meanings tend to occur in similar contexts. It considers that word order is relevant, and is based on Neural Models (NM) that learn word vectors from pairs of target and context words. Context words are taken as words observed to surround a target word. \n",
        "\n",
        "Document vectors are usually created by taking the word vectors they contain or by considering them as target and context items. Skip-gram with negative sampling (Word2Vec) (Mikolov et al., 2013) and Global Vectors (GloVe) (Pennington et al., 2014) are indeed the most popular methods to learn word embeddings due to its training efficiency and robustness (Levy et al., 2015).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGunwvB-8wrK"
      },
      "source": [
        "The Doc2Vec algorithm is an extension of Word2Vec. It proposes a paragraph vector, i.e. an unsupervised algorithm that learns fixed-length feature representations from variable length documents. It takes into consideration the ordering of words within a narrow context, similar to an n-gram model. The combined result generalizes and has a lower dimensionality but still is of a fixed length so it can be used in common machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VanWTtt8TmV"
      },
      "source": [
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_df['tokens'])]\n",
        "model = Doc2Vec(documents, vector_size=5, min_count=0, window=2, workers=4)\n",
        "\n",
        "docvecs = []\n",
        "for pos in range(len(corpus_df['tokens'])):\n",
        "  docvecs.append(model.docvecs[pos])\n",
        "\n",
        "corpus_df['d2v'] = docvecs\n",
        "corpus_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1Fg7JbAjqO"
      },
      "source": [
        "Now, a text or a reference document can be used in the query by  measuring its similarity to the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5OwhdOBGM3"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "query_paper = 4\n",
        "query_vector = corpus_df['d2v'][query_paper]\n",
        "\n",
        "def relevant(vector):\n",
        "  distance = spatial.distance.cosine(query_vector, vector)\n",
        "  similarity = 1 - distance\n",
        "  return similarity\n",
        "\n",
        "pos = 0\n",
        "result = []\n",
        "for v1 in corpus_df['d2v']:\n",
        "  result.append({ 'title': corpus_df['title'][pos],\n",
        "                 'score' : relevant(v1)})    \n",
        "  pos+=1\n",
        "\n",
        "result.sort(reverse=True, key=sort_by_score)\n",
        "\n",
        "print(corpus_df['title'][query_paper],\":\")\n",
        "for paper in result:\n",
        "  print(paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYO-DpTj8UTk"
      },
      "source": [
        "##### 2.3.2.3.2 Bag-of-words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3WVlqAj8YHq"
      },
      "source": [
        "This approach does not consider the order of the words to be relevant, but their frequency. It assumes words with similar meanings will occur in similar documents, although a recent proposal uses an embeddings based approach to model the topics (Dieng et al., 2020). Topic models (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 2001) are the main methods based on this approach. This second approach is used in our work since we are not only interested in representing words and documents, but we also seek structures that allows considering knowledge about the collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiwx62ZSm_iN"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Create Dictionary\n",
        "dictionary = gensim.corpora.Dictionary(corpus_df['tokens'])\n",
        "\n",
        "# Create bag-of-words\n",
        "bows = [dictionary.doc2bow(text) for text in corpus_df['tokens']]\n",
        "\n",
        "print(\"->\",corpus_df['title'][0],\":\")\n",
        "for word in bows[0][200:210]:\n",
        "  id = word[0]\n",
        "  freq = word[1]\n",
        "  print(dictionary[id],freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnpL0grsEBFK"
      },
      "source": [
        "### 2.3.3 Probabilistic Topic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnROb8LdEKHv"
      },
      "source": [
        "Probabilistic Topic Models (PTM) (Blei et al., 2003; Hofmann, 2001) are statistical methods based on bag-of-words that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, or how they change over time. \n",
        "\n",
        "PTM do not require any prior annotations or labeling of the documents. The topics emerge, as hidden structures, from the analy- sis of the original texts. These structures are topic distributions, per-document topic distributions or per-document per-word topic assignments.\n",
        "\n",
        "In turn, a topic is a distribution over terms that is biased around those words associated to a single theme. This interpretable hidden structure annotates each document in the collection and these annotations can be used to perform deeper analysis about relationships between documents.\n",
        "\n",
        "Topic-based representations bring a lot of potential when applied over different IR tasks, as evidenced by recent works in different domains such as scholarly (Gatti et al., 2015), health (Hsin-Min et al., 2016; Nzali et al., 2017), legal (Greene and Cross, 2016; O’Neill et al., 2017), news (He et al., 2017) and social networks (Cheng et al.,2014). \n",
        "\n",
        "Topic modeling provides an algorithmic solution to organize and annotate large collections of textual documents according to their topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791k5zdJExA6"
      },
      "source": [
        "#### 2.3.3.1 LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWD8-A0NE0go"
      },
      "source": [
        "The simplest generative topic model proposed in the state of the art is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Along with Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 2001) are part of the field known as topic modeling. They are well-known latent variable models for high dimensional data, such as the bag-of-words representation for textual data or any other count-based data representation. They try to capture the intuition that documents can exhibit multiple themes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqD61-w6E66U"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=bows,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=2, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "#doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPAWt_z5E7IF"
      },
      "source": [
        "Each document exhibits each topic in different proportion, and each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over topics. All the documents in a collection share the same set of topics, but each document exhibits these topics in a different proportion. Texts are described as a vector of counts with W components, where W is the number of words in the vocabulary. Each document in the corpus is modeled as a mixture over K topics, and each topic k is a distribution over the vocabulary of W words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_-6VfGcH5dm"
      },
      "source": [
        "corpus_doc_lda = lda_model[bows]\n",
        "\n",
        "topic_vectors = []\n",
        "\n",
        "for doc_lda in corpus_doc_lda:\n",
        "  topic_vectors.append(doc_lda[0])\n",
        "\n",
        "corpus_df['topics'] = topic_vectors\n",
        "corpus_df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6PYFVgnHdUH"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(bows))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_df['tokens'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIPr9j43m2Pw"
      },
      "source": [
        "# References\n",
        "\n",
        "* Manning, C. D., Raghavan, P., and Schutze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.\n",
        "* Porter, M. F. (1997). An Algorithm for Suffix Stripping, page 313–316. Morgan Kauf- mann Publishers Inc.\n",
        "* Harmon, D. K. (1996). Overview of the Third Text Retrieval Conference (TREC-3). DIANE Publishing Company.\n",
        "* Salton, G. and McGill, M. J. (1983). Introduction to Modern Information Retrieval. McGraw-Hill, Inc\n",
        "* Hearst, M. a. and Hall, S. (1999). Untangling Text Data Mining. In the 37th Annual Meeting of the Association for Computational Linguistics, pages 1–13\n",
        "* Lee, J. H. (1995). Combining multiple evidence from different properties of weighting schemes. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’95, page 180–188. Association for Computing Machinery.\n",
        "* Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. Proceedings of the 26th International Conference on Neural Information Processing Systems, 2:3111– 3119.\n",
        "* Pennington, J., Socher, R., and Manning, C. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP), pages 1532–1543. Association for Computational Linguistics.\n",
        "* Dieng, A. B., Ruiz, F., and Blei, D. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8:439–453\n",
        "* Blei, D., Ng, A., and Jordan, M. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(4-5):993–1022\n",
        "* Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391–407.\n",
        "* Hofmann, T. (2001). Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42(1-2):177–196.\n",
        "* Gatti, C., Brooks, J., and Nurre, S. (2015). A Historical Analysis of the Field of OR/MS using Topic Models. CoRR, abs/1510.0.\n",
        "* Hsin-Min, L., Chih-Ping, W., and Fei-Yuan, C. (2016). Modeling healthcare data using multiple-channel latent Dirichlet allocation. Journal of Biomedical Informatics, 60:210–223.\n",
        "* Nzali, T., Donald, M., Bringay, S., Lavergne, C., Mollevi, C., and Opitz, T. (2017). What Patients Can Tell Us: Topic Analysis for Social Media on Breast Cancer. JMIR medical informatics, 5(3):e23.\n",
        "* Greene, D. and Cross, J. (2016). Exploring the political agenda of the european par- liament using a dynamic topic modeling approach. Political Analysis, 25(1):77–94.\n",
        "* O’Neill, J., Robin, C., O’Brien, L., and Buitelaar, P. (2017). An analysis of topic modelling for legislative texts. CEUR Workshop Proceedings, 2143.\n",
        "* He, J., Li, L., and Wu, X. (2017). A self-adaptive sliding window based topic model for non-uniform texts. In Proceedings - IEEE International Conference on Data Mining, ICDM, volume 2017-Novem, pages 147–156.\n",
        "* Cheng, X., Yan, X., Lan, Y., and Guo, J. (2014). BTM : Topic Modeling over Short Texts. IEEE Transactions on Knowledge and Data Engineering, 26(12):2928–2941. \n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}