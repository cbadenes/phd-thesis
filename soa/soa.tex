
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Related Work}\label{ch:soa}

\graphicspath{{soa/figures/}}

% -------------------------------------------------------------
% -- Related Work
% -------------------------------------------------------------

Texts usually contain noisy, non-relevant information and keeping only what can bring value for the involved agents (general consumers, experts, companies, investors...) becomes a challenge. In order to facilitate the exploration of large and multilingual document collections we need to process texts in a way that is computationally affordable and enables a semantic-aware exploration of the knowledge inside it. Let's look at the existing techniques and methods involved in tasks related to this objective. 


\section{Text Annotations}

A necessary first step before using documents for knowledge-intensive tasks is to process them following different techniques to leverage their content. Recent studies \cite{Westergaard2017}\cite{Sciences2016} have shown that mining full-text articles give consistently better results than only using sections or summaries. Given the size limitations and concise nature of summaries, they often omit descriptions or results that are considered to be less relevant but still are important for some IR tasks\cite{Divoli2012}.  Since this behavior is present in many other domains, our interest is focused on processing full texts, not summaries or parts of texts, so we have to take it into account during the whole process.

The annotation of human-readable documents is a well-known problem in the Artificial Intelligence (AI) domain in general and Information Retrieval (IR) and Natural Language Processing (NLP) fields in particular. There already exist a broad set of tools and frameworks able to analyze text for automatically producing such annotations, at very different levels of granularity: from minimal units such as terms and entities, to descriptors at the level of the entire collection such as  summaries or topics. spaCy\footnote{\url{https://spacy.io}} , NLTK, Stanford CoreNLP~\cite{Manning2014TheToolkit} and IXA pipes~\cite{Agerri2014} are well-known frameworks for text annotation using Part-of-Speech (PoS) tagging or Named Entity Recognition (NER). Mallet\footnote{\url{http://mallet.cs.umass.edu}},  SparkLDA\footnote{\url{https://spark.apache.org/mllib/}} or Gensim are also widely used libraries to perform more advanced tasks such as topic modeling or clustering over document collections. 

Although they are all widely used resources, their design has not paid special attention to facilitating their interoperability. The main reason that limits the reuse and expansion possibilities of NLP-based tools is their strong technological dependence. Mallet has been designed as a Java library, so that spaCy cannot be integrated and used for text processing because its technology is based on Python. This example can even be extended to the models that Mallet generates, since they cannot be used from other tools either as they are distributed in a proprietary format. To the best of our knowledge, the efforts that have been made go in the direction of creating ecosystems that integrate resources\footnote{\url{https://onnx.ai/}}, rather than creating open environments that follow standards to be (re)used freely. We are focused on the transversal problem of making those standalone tools coexisting under a same solution in an open environment. In this thesis we propose \textit{reusable text annotation models and scalable document processing pipelines to integrate them}.  
 

\section{Topic Distributions}\label{ch:soa_topics}

Once texts have been processed, vector space models (VSM) are created from their content. The objective is twofold, on the one hand to make a huge collection manageable since we move from having lots of words for each text to only one vector per document, and on the other hand to have representations based on metric spaces where calculations can be made, for example comparisons by measuring vector distances. The definition and number of dimensions for each vector are key aspects in a VSM. Traditional retrieval tasks over large collections of textual documents highly rely on individual features like term frequencies (TF)\cite{Hearst1999}. A representational space is created where each word in the vocabulary is projected by a separate and orthogonal dimension. Term Frequency-Inversed Document Frequency (TF-IDF) relativizes the relevance of each term with respect to the entire corpus. The loss of semantic information and the high-number of dimensions are the main drawbacks of these approaches that lead to the emergence of other techniques. New ways of characterizing documents based on the automatic generation of models surfacing the main subjects covered in the corpus are developed during recent years. Among them, text embedding proposes transforming texts into low-dimensional vectors by prediction methods based on (i) word sequences or (ii) bag-of-words. The first approach assumes words with similar meanings tend to occur in similar contexts. It considers word order relevant and is based on Neural Models (NM) that learn word vectors from pairs of target and context words, where context words are taken as words observed to surround a target word. Document vectors are usually created by averaging the word vectors they contain or by considering them as target and context items. The second approach does not consider the order of the words to be relevant, but their frequency is. It assumes words with similar meanings will occur in similar documents. This second approach is used in our work since we are not only interested in representing words and documents, but we also seek  structures that can provide knowledge about the collection as a whole.

Probabilistic Topic Models (PTM)\cite{Blei2010} are statistical methods based on bag-of-words that analyze the words of the original texts to discover the themes that run through them, how those themes are connected to each other, or how they change over time. PTM do not require any prior annotations or labeling of the documents. The topics emerge, as hidden structures, from the analysis of the original texts. These structures are topics distributions, per-resource topic distributions or per-resource per-word topic assignments. In turn, a topic is a distribution over terms that is biased around those words associated to a single theme. This interpretable hidden structure annotates each resource in the collection and these annotations can be used to perform deeper analysis about relationships between resources. Topic-based representations bring a lot of potential when applied over different IR tasks, as evidenced by recent works in different domains such as scholarly  \citep{Gatti2015}, health \citep{Lu2016,TapiNzali2017}, legal \citep{ONeill2017,Greene2016}, news \citep{He2017} and social networks \citep{Cheng2014a}. Topic modeling provides us an algorithmic solution to organize and annotate large collections of textual documents according to their topics.

The simplest generative topic model is \textit{Latent Dirichlet Allocation} (LDA) \cite{Blei2003}. This and other models such as \textit{Latent Semantic Analysis} (LSA) \cite{Deerwester1990} or \textit{Probabilistic Latent Semantic Analysis} (pLSA) \cite{Hofmann2001} are part of the field known as topic modeling. They are well-known latent variable models for high dimensional data, such as the bag-of-words representation for textual data or any other count-based data representation. They try to capture the intuition that documents can exhibit multiple themes. Each document exhibits each topic in different proportion, and each word in each document is drawn from one of the topics, where the selected topic is chosen from the per-document distribution over topics. All the documents in a collection share the same set of topics, but each document exhibits these topics in a different proportion. 

Documents are represented as a vector of counts with $W$ components, where $W$ is the number of words in the vocabulary. Each document in the corpus is modeled as a mixture over $K$ topics, and each topic $k$ is a distribution over the vocabulary of $W$ words. Formally, a \textit{topic} is a multinomial distribution over words of a fixed vocabulary representing some concept. Depending on the function used to describe that distribution there are different algorithms to create topic models. While LSA and pLSA propose a singular value decomposition, LDA, influenced by the generative Bayesian framework to avoid some of the over-fitting issues that were observed with pLSA, suggests the use of a Dirichlet function. It is a continuous multivariate probability distribution parameterized by a vector of positive reals whose elements sum to 1.  It is \textit{continuous} because the relative likelihood for a random variable to take on a given value is described by a probability density function, and is \textit{multivariate} because it has a list of variables with unknown values. In fact, the Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution and is responsible for, unlike LSA and pLSA, LDA can infer topic distributions in texts that have not been used during training.

It is not a restrictive clustering model, where each document is assigned to one cluster, but allows documents to exhibit multiple topics. Since LDA is unsupervised, the topics covered in a set of documents are discovered from the own corpus. The mixed-membership assumptions lead to sharper estimates of word co-occurrence patterns, key to this thesis that proposes a \textit{thematic and low-dimensional feature space suitable for big real-world data sets, where documents are only described by their most relevant topics}.


\section{Distance Measures}

In a \textit{Topic Model} the feature vector is a topic distribution expressed as vector of probabilities. Taking into account this premise, the similarity between two topic-based resources will be based on the distance between their topic distributions, which can be also seen as two probability mass functions. A commonly used metric is the \textit{Kullback-Liebler} (KL) divergence. However, it presents two major problems: (1) when a topic distribution is zero, KL divergence is not defined and (2) it is not symmetric, which does not fit well with semantic similarity measures that are usually symmetric \cite{Rus2013}.

\textit{Jensen-Shannon} (JS) divergence \cite{Rao1982}\cite{Lin1991} solves these problems considering the average of the distributions as below \cite{Celikyilmaz2010}:

\begin{equation}
JS(p,q) = \sum\limits_{i=1}^K p_{i}*\log \frac{2*p_{i}}{p_{i}+q_{i}}  +  \sum\limits_{i=1}^K q_{i}*\log \frac{2*q_{i}}{q_{i}+p_{i}}
\label{eq:jsdivergence}
\end{equation}
where  $K$ is the number of topics and $p,q$ are the topics distributions

It can be transformed into a similarity measure as follows \cite{Dagan1998} :

\begin{equation}
sim_{JS}(D_i , D_j) = 10^{- JS(p,q)}
\label{eq:simjs}
\end{equation}
where  $D_i,D_j$ are the documents and $p,q$ the topic distributions of each of them.


\textit{Hellinger} (He) distance is also symmetric and is used along with JS divergence in various fields where a comparison between two probability distributions is required \cite{Blei2007a} \cite{Hall2008} \cite{Boyd-Graber2010}:

\begin{equation}
	He(p, q) = \frac{1}{\sqrt{2}}\cdot\sqrt{\sum\limits_{i=1}^K (\sqrt{p_i} - \sqrt{q_i})^2)}
	\label{eq:hedistance}
\end{equation}

It can be transformed into a similarity measure by subtracting it from 1 \cite{Rus2013} such that a zero distance means max. similarity score and vice versa:

\begin{equation}
	sim_{He}(D_i, D_j) = 1 - He(p,q)
	\label{eq:simhe}
\end{equation}

However, all these metrics are not well-defined distance metrics, that is, they do not satisfy triangle inequality \cite{Charikar2002}. This inequality considers $d(x, z) <= d(x, y) + d(y, z)$ for a metric $d$ \cite{Griffiths2007}. It places strong constraints on distance measures and on the locations of points in a space given a set of distances. As a metric axiom the triangle inequality must be satisfied in order to take advantage of the inferences that can be deduced from it. Thus, if similarity is assumed to be a monotonically decreasing function of distance, this inequality avoids the calculation of all pairs of similarities by considering that if $x$ is similar to $y$ and $y$ is similar to $z$, then $x$ must be similar to $z$. 

\text{S2JSD} was introduced by \cite{Endres2003} to satisfy the triangle inequality. It is the square root of two times the $JS$ divergence:

%S2JSD formula
\begin{equation}
    S2JSD(P,Q) = \sqrt{2*JS(P,Q)}
\label{eq:s2jsd}
\end{equation}

Making sense out of the similarity score is not easy. As shown in figure \ref{fig:topic_distances}, given a set of pairs of documents, their similarity scores vary according to the number of topics. So the distances between those pairs fluctuate from being more to less distant when changing the number of topics.
\begin{figure}
\begin{center}
\begin{adjustbox}{minipage=0.45\linewidth,frame=0pt 3pt}
\includegraphics[width=\linewidth]{KL_100_2k.png}
\centering (a)
\end{adjustbox}
\hfill
\begin{adjustbox}{minipage=0.45\linewidth,frame=0pt 3pt}
\includegraphics[width=\linewidth]{JSD_100_2k.png}
\centering (b)
\end{adjustbox}
\hfill
\begin{adjustbox}{minipage=0.45\linewidth,frame=0pt 3pt}
\includegraphics[width=\linewidth]{He_100_2k.png}
\centering (c)
\end{adjustbox}
\hfill
\begin{adjustbox}{minipage=0.45\linewidth,frame=0pt 3pt}
\includegraphics[width=\linewidth]{S2JSD_100_2k.png}
\centering (d)
\end{adjustbox}
\end{center}
\caption{Distance values between 10 pair of documents from topic models with 100-to-2000 dimensions. The Kullback-Liebler(a), Jensen-Shannon Divergence(b), Hellinger(c) and S2JSD(c) metrics are considered.}
\label{fig:topic_distances}
\end{figure}

Distances based on topic distributions between documents generally increase as the number of dimensions of the space increases. This is due to the fact that as the number of topics describing the model increases, the more specific the topics will be. Topics shared by a pair of documents can be broken down into more specific topics that are not shared by those documents. Similarity between pairs of documents is then dependent on the model used to represent them when considering this type of metrics. 


Each topic is drawn from a Dirichlet distribution with parameter $\beta$, while each document's mixture is sampled from a Dirichlet distribution with parameter $\alpha$. These two priors, $\alpha$ and $\beta$, are also known as hyper-parameters of a topic model and they set the probability that a document or a word, respectively, contains more than one topic. We know that absolute distances between documents vary when we tune those hyper-parameters differently, but we also see that "relative distances" also change. Imagine that we have two documents, A and B, and one topic model, M1. The distance from the topic distribution of A to B is less than from A to C. However, in a second topic model, M2, trained with the same documents as M1 but with different hyper-parameters, the distance from the topic distribution of A to C is less than to B (cross-lines in fig \ref{fig:topic_distances}). This behaviour highlights the difficulty of establishing absolute similarity thresholds and the complexity to measure distances taking into account all dimensions. Distance thresholds should be model-dependent rather than general and metrics flexible enough to handle dimensional changes. 

In addition, document similarity comparisons are too costly to be performed in huge collections of data and require more efficient approaches than having to calculate all pairwise similarities. Using a naive approach creating a similarity matrix with all document comparisons takes $O(n^2)$ time (where $n$ is the number of documents), so obtaining all possible pairs of similarities in a large collection of documents (e.g. a corpus of 32 million patents) can be unfeasible because of the exponential cost of comparing every pair of elements. Many different approaches have been proposed to reduce this complexity. For instance, computation can be approximated by a nearest neighbors (ANN) search problem. ANN search is an optimization problem that finds nearest neighbors of a given query in a metric space of $n$ points. Due to the low storage cost and fast retrieval speed, hashing is one of the most popular solutions for ANN search \citep{Zhen2016}. 

This technique transforms data points from the original feature space into a binary-code space, so that similar data points have larger probability of collision (i.e. having the same hash code). This type of formulation for the document similarity comparison problem has proven to yield good results in the metric space \citep{Krstovski2011} due to the fact that ANN search has been designed to handle distance metrics (e.g. cosine, Euclidean, Manhattan). But distance metrics between topic distributions should be information-theoretically motivated metrics ( e.g. Hellinger, Kullback-Leibler divergence, Jensen-Shannon divergence) since they compare density functions. 


These challenges can be tackled by hashing methods based on clusters of topics to measure similarity, instead of directly using their weights. Hashing methods transform the data points from the original feature space into a binary-code Hamming space, where the similarities in the original space are preserved. They can learn hash functions (data-dependent) or use projections (data-independent) from the training data \cite{Wang2016}. Data-independent methods unlike data-dependent ones do not need to be re-calculated when data changes, i.e. adding or removing documents to the collection. Taking large-scale scenarios into account (e.g. Document clustering, Content-based Recommendation, Duplicate Detection), this is a key feature along with the ability to infer hash codes individually (for each document) rather than on a set of documents. Data-independent hashing methods depend on two key elements: (1) data type and (2) distance metric. For vector-type data, as introduced in section \ref{sec:similarity}, based on $l_p$ distance with $p \epsilon [0,2)$ lots of hashing methods have been proposed, such as p-stable Locality-Sensitive Hashing (LSH) \cite{Datar2004}, Leech lattice LSH \cite{Andoni2006}, Spherical LSH \cite{Terasawa2007}, and Beyond LSH \cite{Andoni2014}. Based on the $\theta$ distance many methods have been developed such as Kernel LSH \cite{Kulis2012} and Hyperplane hashing \cite{Vijayanarasimhan2014}. But only few methods handle density metrics in a simplex space, where topic distributions are projected. A first approach transformed the $He$ divergence into an Euclidean distance so that existing ANN techniques, such as LSH and k-d tree, could be applied \cite{Krstovski2013a}. But this solution does not consider the special attributions of probability distributions, such as Non-negative and Sum-equal-one. Recently, a hashing schema \cite{Mao2017} has been proposed taking into account the symmetry, non-negativity and triangle inequality features of the S2JSD metric for probability distributions. For set-type data, Jaccard Coefficient is the main metric used. Some examples are K-min Sketch \cite{Li2012}, Min-max hash \cite{Ji2013}, B-bit minwise hashing \cite{Li2010b} and Sim-min-hash \cite{Zhao2013}.

All of them have demonstrated efficiency in the search for similar documents, but none of them allows the search for documents (1) by thematic areas or (2) by similarity levels, nor they offer (3) an explanation about the similarity obtained beyond the vectors used to calculate it. Binary-hash codes drop a very precious information: the topic relevance. This thesis proposes a \textit{hash function-based approach that allows efficiently searching for related documents while maintaining topic-based annotation, giving the reasons why two documents are related}.


\section{Multilingual Topic Alignment}


When the IR task is also cross-language, document retrieval must be independent of the language of the user's query. At execution time, the query in the source language is typically translated into a target language with the help of a dictionary or a machine-translation system. But for many languages we may not have access to translation dictionaries or a full translation system, or they can be expensive to apply in an online search system. In such situations it is useful to rely on smaller annotation units derived from the text so the full content does not need to be translated, for instance by finding correspondences with regard to the topics discussed.

They are mainly based on Latent Dirichlet Allocation (LDA) \cite{Blei2003}, adding supervised associations between languages by using \textit{parallel} corpus, with sentence-aligned documents (e.g. Europarl\footnote{https://ec.europa.eu/jrc/en/language-technologies/dcep} corpora), or \textit{comparable} corpus, with theme-aligned documents (e.g. Wikipedia\footnote{https://www.wikipedia.org/} articles), in multiple languages. These requirements restrict the kind of corpora that can be used for training since large parallel corpora are rare in most of the use cases, especially for languages with fewer resources. Wikipedia, for example, contains texts in 304 languages but 255 of them have less than 3\% of articles\footnote{https://meta.wikimedia.org/wiki/List\_of\_Wikipedias}. Therefore, the requirement of parallel/comparable corpora for multilingual topic models limits their usage in many situations. In addition, these models rely on associations between documents prior to training. So in order to incorporate new languages or update the existing associations, the model must be re-trained with documents from all languages, making it difficult to scale to large corpora \cite{Hao2018} \cite{Moritz2017}. 

Multilingual probabilistic topic models (MuPTM) \cite{Vulic2015} have recently emerged as a group of language-independent generative machine learning models that can be used on large-volume theme-aligned multilingual text. They are based on LDA, adding supervised associations between languages by using \textit{parallel} corpus, with sentence-aligned documents (e.g. Europarl\footnote{https://ec.europa.eu/jrc/en/language-technologies/dcep} corpora), or \textit{comparable} corpus, with theme-aligned documents (e.g. Wikipedia\footnote{https://www.wikipedia.org/} articles), in multiple languages. Once a MuPTM has been generated, documents can be represented by data points in a single feature space based on topics to detect similarities among them exploiting inference results and using distance metrics. Due to its generic language-independent nature and the power of inference on unseen documents, MuPTM's have found many interesting applications in many different cross-lingual tasks. They have been used on cross-lingual event clustering \cite{DeSmet2009}, document classification \cite{10.1007/978-3-642-20841-6_45} \cite{Ni:2011:CLT:1935826.1935887},  semantic similarity of words \cite{Mimno:2009:PTM:1699571.1699627}  \cite{Vulic:2012:DHC:2380816.2380872}, information retrieval \cite{10.1007/978-3-642-36973-5_9} \cite{ganguly-etal-2012-cross}, document matching \cite{Platt:2010:TDR:1870658.1870683} \cite{zhu-etal-2013-building}, and others. 

However, the requirement of parallel/comparable corpora limits their usage in many situations. There are not many document collections that can be used for training since large parallel corpora are rare in most of the use cases, especially for languages with fewer resources. Moreover, in order to incorporate new languages or update the existing associations, these models must be re-trained with documents from all languages at the same time, making it difficult to scale to large corpora \cite{Hao2018} \cite{Moritz2017}. We take MuPTM a step further, to make them cross-lingual through representations based on topic hierarchies. Documents from multi-language corpora are described by expressions of multi-lingual concepts and can then be efficiently browsed and related without the need for translation or parallel or comparable corpora. In this thesis we propose to \textit{automatically learn cross-lingual topics to browse multi-lingual document collections}. 
